{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization, InputLayer, Input, add, dot, multiply, concatenate, Reshape\n",
    "#from tensorflow.python.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "data = pd.read_csv('PrOCTOR_sample_data_all.csv', header=0)\n",
    "data1 = data.fillna(data.mean()['MolecularWeight':'Salivary Gland'])\n",
    "data1[\"target\"] = np.where(data1.iloc[:, 1] == \"passed\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# x, y variable\n",
    "X = data1.iloc[:, 2:-1]\n",
    "y = data1['target']\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "X = scaler.fit_transform(X)\n",
    "y = np.array(y)\n",
    "y_1 = y.reshape(y.shape[0],-1)\n",
    "#y = scaler.fit_transform(X)\n",
    "des=X[:,:13]\n",
    "body=X[:,13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(828, 13)\n",
      "(828, 34)\n",
      "(828,)\n",
      "(828, 1)\n"
     ]
    }
   ],
   "source": [
    "print(des.shape)\n",
    "print(body.shape)\n",
    "print(y.shape)\n",
    "print(y_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, des_col = des.shape\n",
    "_, body_col = body.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_change(predict):\n",
    "    y_predict = []\n",
    "    for i in range(len(predict)):\n",
    "        y_predict = np.append(y_predict, predict[i])\n",
    "    \n",
    "    y_predict = np.array(y_predict)\n",
    "    y_predict = y_predict.reshape(y_predict.shape[0],-1)\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, f1_score, confusion_matrix, accuracy_score, matthews_corrcoef, accuracy_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "import math\n",
    "\n",
    "class Evaluation:\n",
    "    def __init__(self, pred, y):\n",
    "        self.y_pred = pred\n",
    "        self.y = y\n",
    "        \n",
    "    def matrix(self):\n",
    "        y_1 = self.y.reshape(self.y.shape[0],-1)\n",
    "        y_pred = np.array(self.y_pred)\n",
    "        y_pred = y_pred.reshape(y_pred.shape[0], -1)\n",
    "        \n",
    "        y_classify = []\n",
    "        for i in range(len(self.y_pred)):\n",
    "            if self.y_pred[i] >= 0.5:\n",
    "                a = 1.\n",
    "                y_classify.append(a)\n",
    "            else:\n",
    "                a = 0.\n",
    "                y_classify.append(a)\n",
    "        \n",
    "        fpr,tpr,threshold = roc_curve(y_1 , y_pred, pos_label = 1)\n",
    "        precision, recall, threshold = precision_recall_curve(y_1, y_pred, pos_label = 1)\n",
    "        \n",
    "        roc_auc = auc(fpr,tpr)\n",
    "        auprc = auc(recall, precision)\n",
    "        mean_precision = np.mean(precision)\n",
    "        mean_recall = np.mean(recall)\n",
    "        F1 = 2 * (mean_precision * mean_recall) / (mean_precision + mean_recall)\n",
    "        # binary \n",
    "        accuracy = accuracy_score(y_1, y_classify)\n",
    "        mcc = matthews_corrcoef(y_1, y_classify) \n",
    "        g_mean = geometric_mean_score(y_1, y_classify)\n",
    "        confusion = confusion_matrix(y_1, y_classify)\n",
    "        print(confusion.ravel())\n",
    "        tn, fp, fn, tp = confusion.ravel()\n",
    "        tpr = tp / (tp + fn)\n",
    "        tnr = tn / (tn + fp)\n",
    "        ppv = tp / (tp + fp)\n",
    "        fnr = fn / (fn + tp)\n",
    "        fpr = fp / (fp + tn)\n",
    "\n",
    "        confu_precision = ppv \n",
    "        confu_recall = tpr # sensitivity\n",
    "        confu_f1 = 2 * ((ppv * tpr) / (ppv + tpr))\n",
    "        confu_accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        confu_mcc = ((tp * tn)-(fp-fn))/ math.sqrt((tp + fp)*(tp + fn)*(tn + fp)*(tn + fn))\n",
    "        confu_g_mean = math.sqrt(tpr * tnr)\n",
    "        Optimized_precision = (confu_accuracy - abs(tnr-tpr)) / (tnr + tpr)\n",
    "\n",
    "        print('공통 \\nAUC :',roc_auc) # pb\n",
    "        print(\"AUPRC :\", auprc) # pb\n",
    "        print(\"Optimized precision :\", Optimized_precision)\n",
    "\n",
    "        print(\"\\nfunction 사용\\nAccuracy :\", accuracy) #pb\n",
    "        print(\"Precision(pb) :\",mean_precision )\n",
    "        print(\"Recall(pb) :\", mean_recall) # pb\n",
    "        print(\"F1 score(pb) :\", F1) #pb\n",
    "\n",
    "        print(\"MCC :\", mcc)\n",
    "        print(\"G-mean :\", g_mean)\n",
    "\n",
    "\n",
    "        print(\"\\nConfusion_matrix 사용 \\n\", confusion)\n",
    "        print(\"Accuracy :\", confu_accuracy)\n",
    "        print(\"Precision :\", confu_precision) \n",
    "        print(\"Recall :\", confu_recall) \n",
    "        print(\"F1 score :\", confu_f1) \n",
    "\n",
    "        print(\"MCC :\", confu_mcc)\n",
    "        print(\"G-mean :\", confu_g_mean)\n",
    "        \n",
    "        return roc_auc, auprc, Optimized_precision, accuracy, mean_precision, mean_recall, F1, mcc, g_mean, confu_accuracy, confu_precision, confu_recall, confu_f1, confu_mcc, confu_g_mean;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut, KFold, cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 모델설정\n",
    "sm = SMOTE(random_state=202004)\n",
    "loo = LeaveOneOut()\n",
    "kfold = KFold(n_splits = 5, shuffle = True, random_state = 111)\n",
    "kfold.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_x = dict()\n",
    "dict_y = dict()\n",
    "\n",
    "class train_model:\n",
    "    def __init__(self, x, y, pd_des, pd_body):\n",
    "        self.X = x\n",
    "        self.y = y\n",
    "        self.pd_des = pd_des\n",
    "        self.pd_body = pd_body\n",
    "\n",
    "        \n",
    "    def train_based(self):\n",
    "        for train_index, test_index in kfold.split(self.X):\n",
    "            print(\"TEST:\", test_index)\n",
    "            #des_train, des_test = des[train_index], des[test_index]\n",
    "            #body_train, body_test = body[train_index], body[test_index]\n",
    "            X_train, X_test = self.X[train_index], self.X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            rf_des = RandomForestClassifier(n_estimators=100, max_features = des_col)\n",
    "            rf_des.fit(X_train, y_train)\n",
    "\n",
    "            #rf_body = RandomForestClassifier(n_estimators=100, max_features = body_col)\n",
    "            #rf_body.fit(body_train, y_train)\n",
    "\n",
    "            pred_y = rf_des.predict_proba(X_test)\n",
    "            #pred_body = rf_body.predict_proba(body_test)\n",
    "\n",
    "            #print(pred_des[:,1].shape) # pass의 확률\n",
    "            pred_y = pred_y[:,1]\n",
    "            #pred_body = pred_body[:,1]\n",
    "            print(pred_y)\n",
    "            #print(pred_body)\n",
    "            for i in range(len(test_index)):\n",
    "                index = test_index[i]\n",
    "                self.pd_des[index] = pred_y[i]\n",
    "                #self.pd_body[index] = pred_body[i]\n",
    "            \n",
    "            #print(self.pd_des)\n",
    "            #print(self.pd_body)\n",
    "        return self.pd_des#, self.pd_body\n",
    "    '''\n",
    "    def train_smote(self):\n",
    "        for train_index, test_index in kfold.split(self.X):\n",
    "            print(\"TEST:\", test_index)\n",
    "            des_train, des_test = des[train_index], des[test_index]\n",
    "            body_train, body_test = body[train_index], body[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            \n",
    "            sm_des_train, sm_y_train = sm.fit_sample(des_train, y_train)\n",
    "            sm_body_train, _ = sm.fit_sample(body_train, y_train)\n",
    "\n",
    "            self.model.compile(optimizer= keras.optimizers.Adam(), loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "            kf_history = self.model.fit(x = [sm_des_train, sm_body_train], y = sm_y_train, epochs=50)\n",
    "\n",
    "            y_pred = self.model.predict([des_test[:], body_test[:]])\n",
    "            \n",
    "            for i in range(len(test_index)):\n",
    "                index = test_index[i]\n",
    "                self.pd_sm_y[index] = y_pred[i]\n",
    "            \n",
    "        return self.pd_sm_y\n",
    "    '''\n",
    "    def train_weight(self):\n",
    "        for train_index, test_index in kfold.split(self.X):\n",
    "            print(\"TEST:\", test_index)\n",
    "            #des_train, des_test = des[train_index], des[test_index]\n",
    "            #body_train, body_test = body[train_index], body[test_index]\n",
    "            X_train, X_test = self.X[train_index], self.X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "                        \n",
    "            neg, pos = np.bincount(y_train)\n",
    "            total = neg + pos\n",
    "\n",
    "            weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "            weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "            class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "            \n",
    "            rf_des = RandomForestClassifier(n_estimators=100, max_features = des_col, class_weight = class_weight)\n",
    "            rf_des.fit(X_train, y_train)\n",
    "\n",
    "            #rf_body = RandomForestClassifier(n_estimators=100, max_features = body_col, class_weight = class_weight)\n",
    "            #rf_body.fit(body_train, y_train)\n",
    "\n",
    "            pred_des = rf_des.predict_proba(X_test)\n",
    "            #pred_body = rf_body.predict_proba(body_test)\n",
    "\n",
    "            pred_des = pred_des[:,1]\n",
    "            #pred_body = pred_body[:,1]\n",
    "            print(pred_des)\n",
    "            #print(pred_body)\n",
    "            for i in range(len(test_index)):\n",
    "                index = test_index[i]\n",
    "                self.pd_des[index] = pred_des[i]\n",
    "                #self.pd_body[index] = pred_body[i]\n",
    "            \n",
    "            #print(self.pd_des)\n",
    "            #print(self.pd_body)\n",
    "        return self.pd_des#, self.pd_body\n",
    "\n",
    "    def train_sm_weight(self):\n",
    "        for train_index, test_index in kfold.split(self.X):\n",
    "            print(\"TEST:\", test_index)\n",
    "            #des_train, des_test = des[train_index], des[test_index]\n",
    "            #body_train, body_test = body[train_index], body[test_index]\n",
    "            X_train, X_test = self.X[train_index], self.X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "                        \n",
    "            neg, pos = np.bincount(y_train)\n",
    "            total = neg + pos\n",
    "\n",
    "            weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "            weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "            class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "            \n",
    "            sm_X_train, sm_y_train = sm.fit_resample(X_train, y_train)\n",
    "            #sm_body_train, _ = sm.fit_sample(body_train, y_train)\n",
    "            \n",
    "            rf_des = RandomForestClassifier(n_estimators=100, max_features = des_col, class_weight = class_weight)\n",
    "            rf_des.fit(sm_X_train, sm_y_train)\n",
    "\n",
    "            #rf_body = RandomForestClassifier(n_estimators=100, max_features = body_col, class_weight = class_weight)\n",
    "            #rf_body.fit(sm_body_train, sm_y_train)\n",
    "\n",
    "            pred_des = rf_des.predict_proba(X_test)\n",
    "            #pred_body = rf_body.predict_proba(body_test)\n",
    "\n",
    "            pred_des = pred_des[:,1]\n",
    "            #pred_body = pred_body[:,1]\n",
    "            print(pred_des)\n",
    "            #print(pred_body)\n",
    "            for i in range(len(test_index)):\n",
    "                index = test_index[i]\n",
    "                self.pd_des[index] = pred_des[i]\n",
    "                #self.pd_body[index] = pred_body[i]\n",
    "\n",
    "        return self.pd_des#, self.pd_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: [  1  11  12  34  36  49  56  71  93 112 119 125 126 140 145 152 157 183\n",
      " 190 194 198 205 209 210 211 214 233 237 257 264 272 298 303 323 328 340\n",
      " 354 368 373 380 399 403 409 412 422 432 434 447 456 461 464 468 474 476\n",
      " 491 496 506 518 520 523 528 532 538 547 553 556 589 611 624 629 645 658\n",
      " 665 700 702 708 735 757 766 774 799 800 825]\n",
      "[0.99 0.78 0.92 0.97 0.94 0.94 0.99 0.92 0.91 0.91 0.81 0.98 1.   0.82\n",
      " 0.96 0.98 0.97 1.   0.82 1.   0.95 0.93 0.88 1.   1.   0.99 0.84 0.9\n",
      " 1.   0.68 0.84 0.95 0.94 1.   0.84 0.75 0.94 0.98 0.92 0.98 0.96 0.91\n",
      " 0.92 0.94 1.   0.75 0.7  0.96 1.   0.89 0.96 0.99 0.93 0.95 0.97 0.93\n",
      " 0.95 0.99 0.86 0.91 0.83 0.92 0.92 0.95 0.86 0.99 0.61 0.98 0.91 0.76\n",
      " 1.   0.99 0.98 0.77 1.   0.93 0.93 0.84 0.97 1.   1.   1.   0.93]\n",
      "TEST: [  5  39  46  52  68 102 104 122 162 164 166 167 223 228 230 234 249 258\n",
      " 277 280 281 291 326 331 332 338 342 366 374 375 381 383 389 401 413 421\n",
      " 423 431 443 444 460 465 471 481 493 498 507 516 526 537 551 572 575 581\n",
      " 586 604 606 613 632 651 652 653 664 666 673 694 701 706 707 709 718 723\n",
      " 744 745 754 767 769 780 783 789 806 816 822]\n",
      "[0.97 0.68 0.86 1.   0.89 0.97 1.   0.99 1.   0.95 1.   0.97 0.94 0.95\n",
      " 0.89 1.   0.92 0.4  0.94 0.99 0.67 0.98 0.98 0.93 0.93 0.96 0.96 0.72\n",
      " 1.   0.59 0.99 0.88 0.86 0.96 0.96 0.94 0.62 0.83 0.92 0.97 0.83 0.98\n",
      " 1.   1.   0.86 0.94 0.81 0.83 0.93 0.49 0.83 0.95 0.76 0.8  0.85 1.\n",
      " 0.98 0.94 0.96 1.   0.72 1.   0.99 0.98 0.84 0.93 0.94 1.   0.98 0.99\n",
      " 0.98 0.94 0.96 0.21 0.93 0.95 0.82 1.   1.   0.94 0.59 0.37 0.87]\n",
      "TEST: [ 13  16  22  23  24  43  59  72  87  94  96 100 132 134 146 150 158 175\n",
      " 180 189 192 195 240 241 243 246 262 263 271 279 301 307 309 310 325 349\n",
      " 355 360 367 371 395 405 406 436 452 463 469 473 492 499 510 512 530 535\n",
      " 536 549 561 567 577 585 599 601 602 612 618 623 625 627 630 639 648 660\n",
      " 697 719 725 749 753 758 761 768 786 787 797]\n",
      "[0.83 0.89 0.87 1.   0.81 0.87 0.63 1.   1.   1.   0.69 0.92 0.87 0.82\n",
      " 1.   0.92 0.96 0.75 0.82 0.87 0.72 0.64 0.96 0.86 0.95 0.92 1.   0.78\n",
      " 0.96 0.99 0.28 0.96 0.99 0.99 0.98 0.9  0.81 0.51 0.87 0.75 1.   0.86\n",
      " 0.7  0.94 0.98 0.86 0.61 0.81 0.85 0.89 0.87 0.96 1.   0.86 0.99 0.96\n",
      " 1.   0.98 0.94 0.98 0.88 0.98 0.97 0.59 0.95 0.99 0.99 0.96 0.59 0.95\n",
      " 0.93 0.99 0.99 0.81 0.65 0.99 0.74 1.   0.97 1.   1.   0.75 0.83]\n",
      "TEST: [  0   3  10  20  25  26  32  65  67  73  83  89  97 117 120 121 196 212\n",
      " 215 226 244 248 251 255 261 294 295 314 316 327 335 361 385 386 410 411\n",
      " 415 419 433 440 446 458 462 502 509 514 540 550 582 587 597 600 626 634\n",
      " 644 649 656 663 678 679 684 695 715 717 721 722 729 752 765 785 790 792\n",
      " 793 795 796 801 802 804 809 811 814 817 820]\n",
      "[0.97 1.   0.94 1.   0.94 1.   0.56 0.96 0.91 0.86 1.   0.99 0.95 0.93\n",
      " 0.99 0.6  0.93 1.   0.9  0.98 0.99 1.   0.99 0.97 0.91 1.   0.98 0.65\n",
      " 0.83 0.84 0.98 1.   0.94 0.68 0.77 0.97 0.96 0.92 0.97 0.85 0.9  0.95\n",
      " 0.78 0.91 0.78 0.91 0.6  0.98 0.94 0.68 0.9  0.88 1.   1.   0.81 0.77\n",
      " 0.99 1.   1.   0.95 0.98 1.   1.   0.94 0.98 0.96 0.99 0.99 0.78 0.96\n",
      " 0.95 0.98 0.9  0.98 0.99 0.92 1.   0.97 0.99 0.97 0.82 0.57 1.  ]\n",
      "TEST: [  6  15  18  19  27  30  57  63  76  99 106 107 115 130 143 147 169 182\n",
      " 184 185 186 188 199 204 208 218 221 224 225 227 238 256 269 278 285 306\n",
      " 319 320 321 336 337 339 341 347 351 353 358 402 407 408 414 425 438 439\n",
      " 442 450 459 484 489 495 517 559 564 565 576 584 594 621 655 668 671 674\n",
      " 676 688 691 696 720 734 746 750 763 798 807]\n",
      "[0.98 0.66 0.99 0.93 0.84 1.   0.95 0.81 0.99 0.54 1.   0.93 0.97 0.95\n",
      " 0.99 1.   0.57 0.99 0.6  0.85 0.99 0.81 0.96 0.84 0.98 0.99 0.94 0.97\n",
      " 0.83 0.95 1.   0.99 0.98 0.8  0.99 0.97 0.67 0.87 0.97 0.99 0.89 0.96\n",
      " 0.68 0.94 0.8  0.92 0.99 1.   0.78 0.98 0.91 0.77 0.85 0.91 0.85 0.71\n",
      " 1.   0.91 0.97 0.99 0.89 1.   0.95 0.87 0.86 0.89 0.99 1.   0.97 0.75\n",
      " 0.88 0.93 0.94 0.55 0.93 0.96 0.4  0.79 0.97 0.44 0.93 0.99 1.  ]\n",
      "TEST: [  2   9  53  58  77  85 101 111 131 133 149 153 154 165 172 176 193 201\n",
      " 202 242 250 252 282 300 308 311 312 318 329 330 346 357 359 370 384 388\n",
      " 397 416 420 441 451 467 472 487 497 503 524 527 534 552 554 557 558 573\n",
      " 579 583 593 596 605 614 633 637 642 654 659 661 672 682 687 711 730 736\n",
      " 741 751 755 756 760 777 784 812 813 823 826]\n",
      "[0.59 0.96 0.98 0.99 0.96 0.84 0.85 0.92 1.   0.89 0.98 0.94 0.96 0.95\n",
      " 0.97 0.94 0.97 0.98 0.95 0.99 0.94 0.94 0.98 0.94 0.83 0.97 0.98 0.48\n",
      " 0.82 0.97 0.85 0.8  0.96 0.99 0.82 1.   0.96 0.98 0.73 1.   0.92 1.\n",
      " 0.88 0.99 1.   0.96 0.98 0.86 0.76 0.76 0.72 0.92 0.97 0.98 1.   0.99\n",
      " 1.   0.97 0.99 0.96 0.49 0.89 0.99 0.96 0.97 0.94 1.   0.94 0.84 1.\n",
      " 0.83 0.85 0.99 0.93 0.89 0.88 0.99 0.92 0.86 0.96 0.83 0.82 0.89]\n",
      "TEST: [  4  17  29  38  45  47  48  61  92  95 109 114 116 124 142 151 168 171\n",
      " 179 181 203 206 207 213 216 217 247 253 254 259 267 274 276 286 287 299\n",
      " 305 315 333 352 362 365 369 376 382 392 404 424 427 448 457 480 494 500\n",
      " 504 522 533 541 544 568 569 588 603 608 620 638 640 657 667 670 677 686\n",
      " 690 716 726 737 748 759 776 779 781 803 827]\n",
      "[1.   0.53 0.85 0.8  0.88 0.91 0.96 1.   0.97 1.   1.   0.88 0.53 0.79\n",
      " 0.96 0.79 1.   1.   0.89 0.96 0.98 1.   0.93 0.67 0.94 0.94 0.87 0.96\n",
      " 0.92 0.98 0.99 0.83 0.7  0.91 0.95 0.99 1.   1.   0.67 0.78 0.88 1.\n",
      " 1.   0.91 0.84 0.81 0.95 1.   1.   1.   0.97 0.99 0.68 0.98 0.93 0.97\n",
      " 0.61 0.98 0.89 0.96 1.   0.97 0.98 0.79 0.95 0.73 0.95 0.99 0.95 0.88\n",
      " 0.99 0.71 0.92 1.   0.95 0.73 0.91 0.98 1.   0.93 0.97 0.96 0.86]\n",
      "TEST: [ 14  28  35  41  44  51  55  78  81  88  90 108 127 128 148 155 159 161\n",
      " 163 174 191 220 229 235 245 265 288 292 297 313 343 348 372 387 390 393\n",
      " 394 398 400 417 430 435 445 453 454 478 479 482 501 515 521 529 531 542\n",
      " 545 555 574 578 592 595 607 610 616 619 622 628 635 636 647 650 675 692\n",
      " 693 703 705 710 739 762 772 788 791 815 821]\n",
      "[0.99 0.58 0.77 0.9  0.84 1.   0.87 1.   0.92 0.97 0.99 0.85 0.93 0.87\n",
      " 1.   0.91 0.97 0.82 1.   0.96 0.96 1.   0.98 1.   0.61 0.9  0.97 0.8\n",
      " 0.99 0.9  0.93 0.96 0.93 0.99 0.99 1.   0.93 0.83 0.96 0.89 0.91 0.96\n",
      " 1.   0.73 0.96 0.98 0.97 0.96 1.   0.9  0.86 0.75 0.81 0.87 0.8  0.98\n",
      " 1.   1.   0.87 0.91 1.   0.99 1.   0.97 0.92 0.93 0.95 0.75 0.9  0.92\n",
      " 0.96 0.9  1.   0.98 0.94 1.   0.86 0.99 0.79 0.85 0.97 0.83 0.92]\n",
      "TEST: [  8  31  33  40  42  60  66  70  79  80  82  84  98 103 105 110 113 129\n",
      " 136 138 141 144 156 177 178 197 219 232 239 283 290 293 302 304 317 345\n",
      " 356 363 364 377 378 391 428 437 466 475 486 488 490 505 508 519 539 546\n",
      " 548 560 563 570 580 591 598 631 641 662 680 699 704 713 727 733 764 770\n",
      " 771 773 775 778 782 794 805 810 818 824]\n",
      "[0.95 0.91 0.94 0.87 0.96 0.91 1.   0.87 0.99 0.97 0.75 1.   0.75 0.96\n",
      " 1.   0.82 0.51 0.89 0.74 0.96 0.99 0.76 0.96 0.97 0.9  1.   0.78 0.9\n",
      " 0.99 0.83 0.94 0.89 0.87 0.99 0.85 0.82 0.92 1.   0.98 0.99 0.56 0.92\n",
      " 0.84 0.99 0.95 1.   0.95 0.98 0.96 0.97 0.99 0.99 0.94 0.75 0.92 1.\n",
      " 0.99 0.88 0.41 0.98 0.87 0.92 0.98 0.85 0.96 1.   0.91 0.94 0.93 0.87\n",
      " 1.   0.98 0.99 0.98 0.99 0.92 0.83 1.   0.79 0.99 0.85 0.86]\n",
      "TEST: [  7  21  37  50  54  62  64  69  74  75  86  91 118 123 135 137 139 160\n",
      " 170 173 187 200 222 231 236 260 266 268 270 273 275 284 289 296 322 324\n",
      " 334 344 350 379 396 418 426 429 449 455 470 477 483 485 511 513 525 543\n",
      " 562 566 571 590 609 615 617 643 646 669 681 683 685 689 698 712 714 724\n",
      " 728 731 732 738 740 742 743 747 808 819]\n",
      "[0.87 0.8  0.86 0.73 0.78 0.98 0.94 1.   0.71 0.94 1.   0.74 0.95 0.91\n",
      " 0.81 1.   0.8  0.75 0.96 0.88 0.97 0.93 0.98 0.61 0.87 0.97 0.78 1.\n",
      " 0.99 0.76 0.93 0.89 0.98 1.   0.92 0.88 0.98 0.95 0.83 1.   0.99 0.72\n",
      " 0.99 0.9  0.84 0.94 1.   0.92 0.85 0.9  0.93 0.89 0.97 0.82 0.95 1.\n",
      " 0.98 0.91 1.   1.   0.89 0.67 0.96 0.97 0.59 0.92 1.   0.82 0.88 0.67\n",
      " 0.86 0.99 0.99 0.97 0.79 0.59 0.98 0.7  0.95 0.97 0.4  0.72]\n"
     ]
    }
   ],
   "source": [
    "rf_y, f_sm_y, rf_cw_y, rf_smcw_y = [], [], [], []\n",
    "output_rf = train_model(X, y, dict_x, dict_y)\n",
    "rf_y = output_rf.train_based()\n",
    "final_result = Evaluation(predict_change(rf_y), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n",
      "[1.   0.96 0.62 0.86 0.89 0.95 0.68 0.85 0.86 1.   1.   0.84 0.91 0.81\n",
      " 0.94 1.   0.85 0.81 1.   0.97 1.   0.89 0.93 0.95 0.99 1.   0.93 1.\n",
      " 0.98 0.99 0.82 0.99 0.94 0.87 0.86 1.   1.   0.97 0.9  0.92 0.92 0.73\n",
      " 0.99 0.87 0.85 1.   0.45 0.72 0.77 0.92 0.93 0.63 0.99 0.97 0.97 0.97\n",
      " 0.97 0.76 0.94 0.92 0.88 0.72 0.84 0.91 0.71 0.99 0.86 0.95 0.86 0.97\n",
      " 0.98 0.71 0.79 0.93 0.96 0.94 0.93 0.93 0.93 0.88 1.   0.55 0.72 0.8\n",
      " 0.63 0.97 0.98 0.94 0.98 0.75 0.85 0.96 0.96 0.99 1.   0.88 0.93 1.\n",
      " 0.97 0.93 0.92 0.89 0.9  0.97 0.91 0.97 0.88 0.89 0.95 0.73 0.89 0.47\n",
      " 0.91 0.91 0.77 0.82 0.99 0.92 0.71 0.87 0.92 0.63 1.   0.99 0.95 1.\n",
      " 0.88 0.78 0.96 1.   1.   0.71 1.   0.99 0.99 0.96 0.99 0.8  0.94 0.7\n",
      " 0.96 0.97 1.   0.99 0.91 1.   0.98 0.91 0.86 0.96 0.22 0.85 0.85 0.99\n",
      " 0.92 0.84 0.99 0.97 0.99 0.84 1.   1.   0.49 0.42 0.93 0.92]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.95 1.   0.83 0.83 0.95 1.   0.82 1.   0.98 0.85 1.   0.56 0.98 0.61\n",
      " 0.93 0.84 1.   0.89 1.   1.   0.98 0.98 0.74 0.94 0.85 0.99 0.97 0.7\n",
      " 0.89 0.91 0.97 0.98 0.89 0.79 0.85 0.78 0.68 0.69 0.86 0.97 0.9  0.99\n",
      " 0.91 0.91 0.93 1.   0.94 0.99 1.   1.   0.97 0.99 0.78 1.   1.   1.\n",
      " 0.97 0.23 0.98 0.96 0.96 0.64 0.83 0.96 0.88 0.96 0.91 0.74 0.49 1.\n",
      " 0.95 0.75 0.94 0.79 0.97 0.87 0.71 0.74 0.96 0.99 0.93 0.98 0.98 0.91\n",
      " 0.82 0.99 0.98 0.7  0.89 0.65 0.68 0.77 0.95 0.94 0.65 0.83 0.99 0.92\n",
      " 1.   0.9  0.94 0.76 0.98 0.96 0.99 1.   0.95 0.91 1.   0.68 0.86 0.95\n",
      " 0.79 1.   0.94 1.   0.99 0.99 0.99 0.97 0.98 0.61 0.98 0.99 0.71 0.96\n",
      " 0.71 0.97 0.97 1.   1.   0.95 0.99 1.   0.99 0.99 0.97 0.9  0.97 0.96\n",
      " 0.59 0.94 1.   0.97 0.64 0.99 1.   0.76 1.   0.98 0.97 0.63 0.95 0.98\n",
      " 0.91 0.96 0.99 0.93 0.96 1.   0.92 0.99 0.94 0.85 0.65 1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.45 0.94 0.93 0.7  1.   0.93 0.82 1.   0.98 0.99 0.99 0.71 0.99 0.96\n",
      " 0.7  0.73 0.94 0.99 0.91 0.98 0.93 0.99 1.   0.9  0.99 0.98 1.   0.91\n",
      " 0.99 0.95 0.7  0.96 0.96 0.98 0.49 0.85 1.   0.85 0.85 0.92 0.97 0.95\n",
      " 0.77 0.97 0.98 0.91 0.96 0.71 0.97 1.   0.98 0.89 1.   0.99 0.99 0.74\n",
      " 0.98 1.   0.99 0.93 0.84 0.98 1.   0.61 0.58 0.87 0.94 0.87 0.95 0.99\n",
      " 0.85 0.96 0.71 0.91 0.9  0.81 0.95 0.86 0.94 0.95 1.   0.98 0.99 0.99\n",
      " 1.   0.77 0.96 0.96 0.97 0.58 0.91 0.79 0.88 1.   0.91 0.59 0.92 1.\n",
      " 1.   0.92 0.92 1.   1.   0.99 1.   0.96 0.76 0.97 0.85 0.88 0.86 0.8\n",
      " 0.92 0.88 1.   0.85 0.84 0.98 0.8  1.   1.   0.82 0.97 0.94 0.98 0.98\n",
      " 0.9  1.   0.49 0.79 0.97 0.91 0.94 1.   0.79 0.78 0.81 0.99 0.9  0.93\n",
      " 0.95 0.84 0.55 0.94 0.84 0.98 0.28 0.91 0.8  0.88 0.99 0.94 0.46 0.91\n",
      " 0.87 0.89 0.96 0.89 0.92 0.95 1.   0.97 0.99 0.81 0.84 0.89]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   1.   0.41 0.66 0.91 0.7  0.88 0.94 0.86 0.97 0.81 0.92 1.   0.95\n",
      " 1.   1.   0.9  0.98 1.   1.   1.   0.9  0.99 0.94 0.54 0.87 0.94 0.74\n",
      " 0.97 0.96 0.84 0.87 0.95 0.87 1.   0.99 0.96 0.99 0.95 0.98 0.92 0.98\n",
      " 1.   0.91 0.71 1.   0.87 1.   0.98 1.   0.66 0.96 0.94 0.95 0.97 0.91\n",
      " 0.94 0.87 0.59 0.96 0.9  1.   0.81 1.   0.96 0.99 0.93 1.   0.9  0.91\n",
      " 0.9  0.86 0.91 0.98 0.98 0.91 0.97 0.77 1.   0.99 0.79 1.   0.93 0.79\n",
      " 0.9  0.91 0.98 0.98 1.   0.95 0.9  1.   0.98 0.67 0.99 0.94 0.97 0.99\n",
      " 0.97 0.95 0.69 1.   1.   0.93 0.91 0.83 0.95 0.98 0.59 0.99 0.81 0.97\n",
      " 0.73 0.99 0.99 1.   1.   0.99 0.96 0.85 0.98 0.95 1.   0.66 0.98 1.\n",
      " 0.99 0.95 0.94 0.89 0.92 0.72 0.69 0.84 0.97 0.82 0.99 0.89 0.95 0.92\n",
      " 1.   0.9  0.94 0.92 1.   0.99 0.95 1.   1.   0.94 0.71 0.66 0.96 0.99\n",
      " 1.   0.76 1.   0.96 0.99 0.86 0.96 0.94 0.78 0.98 0.88]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.8  0.94 0.72 0.94 0.96 0.93 0.85 0.95 0.78 0.73 0.85 0.99 0.9  1.\n",
      " 1.   0.95 0.66 0.96 0.99 0.98 0.81 0.98 1.   0.8  0.82 0.87 1.   0.8\n",
      " 0.53 0.94 0.89 0.89 0.9  0.64 1.   0.99 0.76 1.   0.78 0.93 0.77 0.96\n",
      " 0.88 0.95 0.83 0.99 0.97 0.85 0.85 1.   0.64 0.86 0.91 1.   0.94 0.8\n",
      " 1.   0.96 0.74 0.92 0.88 0.83 0.98 0.91 0.87 1.   0.86 0.98 0.73 0.91\n",
      " 0.85 0.96 0.95 0.76 0.9  0.96 0.98 0.96 0.93 0.49 0.99 0.9  0.99 0.72\n",
      " 0.98 0.86 0.93 0.93 0.84 0.94 0.9  1.   1.   0.95 0.93 0.88 0.92 0.99\n",
      " 0.99 0.92 0.98 0.9  0.89 0.91 1.   0.82 0.92 0.88 0.68 0.86 0.95 0.86\n",
      " 0.95 1.   0.84 0.89 0.42 0.91 0.92 0.88 0.99 1.   0.91 0.93 0.95 0.63\n",
      " 0.94 0.93 0.99 0.83 0.7  0.92 0.99 0.74 0.95 0.99 0.89 0.62 0.92 0.86\n",
      " 1.   0.87 1.   1.   0.77 0.86 0.58 0.95 0.72 0.97 0.96 0.99 0.96 1.\n",
      " 0.98 0.98 0.94 0.89 1.   0.66 0.34 0.96 0.82 0.87 0.82]\n",
      "[  8  63   8 749]\n",
      "공통 \n",
      "AUC : 0.661888105382626\n",
      "AUPRC : 0.9503047829573824\n",
      "Optimized precision : 0.03402143431815221\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9142512077294686\n",
      "Precision(pb) : 0.9342529593570803\n",
      "Recall(pb) : 0.8079651646362348\n",
      "F1 score(pb) : 0.8665319637344437\n",
      "MCC : 0.20768205987017752\n",
      "G-mean : 0.3338941332254215\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  8  63]\n",
      " [  8 749]]\n",
      "Accuracy : 0.9142512077294686\n",
      "Precision : 0.9224137931034483\n",
      "Recall : 0.9894319682959049\n",
      "F1 score : 0.9547482472912683\n",
      "MCC : 0.2246735403515386\n",
      "G-mean : 0.33389413322542144\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.   0.98 0.68 0.9  0.92 0.86 0.62 0.82 0.89 1.   0.98 0.89 0.97 0.89\n",
      " 0.94 1.   0.87 0.8  1.   0.95 1.   0.77 0.94 0.98 0.99 0.98 0.94 0.99\n",
      " 0.97 0.99 0.77 0.96 0.91 0.87 0.85 1.   1.   0.97 0.92 0.93 0.93 0.81\n",
      " 0.99 0.9  0.92 1.   0.43 0.78 0.86 0.92 0.95 0.62 0.98 0.93 0.89 0.97\n",
      " 0.96 0.87 0.93 0.89 0.92 0.74 0.87 0.95 0.78 0.96 0.88 0.99 0.89 0.98\n",
      " 0.98 0.76 0.84 0.96 0.97 0.91 0.98 0.9  0.96 0.88 1.   0.53 0.8  0.83\n",
      " 0.56 0.93 0.99 0.92 0.99 0.81 0.83 0.96 0.96 0.97 1.   0.85 0.95 1.\n",
      " 0.96 0.92 0.91 0.95 0.91 0.98 0.84 0.98 0.86 0.84 0.92 0.84 0.88 0.53\n",
      " 0.9  0.9  0.72 0.81 0.97 0.93 0.73 0.89 0.86 0.66 0.99 0.98 0.98 1.\n",
      " 0.9  0.82 0.95 1.   1.   0.68 1.   0.97 0.99 0.95 0.99 0.86 0.93 0.77\n",
      " 0.93 0.97 1.   0.98 0.94 1.   0.99 0.94 0.96 0.96 0.26 0.96 0.79 0.98\n",
      " 0.98 0.78 1.   0.99 1.   0.87 1.   1.   0.62 0.36 0.95 0.93]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.99 1.   0.85 0.83 0.94 1.   0.83 1.   0.99 0.89 0.99 0.47 0.94 0.64\n",
      " 0.96 0.87 1.   0.91 0.99 1.   0.99 0.99 0.71 0.97 0.84 0.96 0.96 0.72\n",
      " 0.94 0.91 0.98 0.95 0.92 0.73 0.87 0.81 0.73 0.64 0.89 1.   0.9  0.96\n",
      " 0.97 0.87 0.97 1.   0.97 0.98 0.97 0.99 0.93 1.   0.77 0.98 1.   1.\n",
      " 0.97 0.27 0.99 0.93 0.98 0.54 0.8  0.97 0.86 0.98 0.9  0.81 0.56 1.\n",
      " 0.99 0.79 0.93 0.75 0.98 0.9  0.76 0.81 0.95 0.98 0.9  0.94 0.99 0.86\n",
      " 0.87 0.97 0.97 0.75 0.95 0.71 0.71 0.79 0.91 0.98 0.72 0.8  1.   0.93\n",
      " 0.97 0.93 0.92 0.69 0.99 0.98 0.97 0.99 0.97 0.89 1.   0.66 0.84 0.94\n",
      " 0.9  1.   0.9  1.   0.99 1.   0.99 0.95 0.99 0.56 0.98 0.99 0.77 0.97\n",
      " 0.8  0.97 0.97 0.99 1.   0.93 0.99 0.98 1.   1.   0.94 0.97 0.98 0.96\n",
      " 0.63 0.99 1.   1.   0.62 1.   0.97 0.81 1.   0.96 0.97 0.57 0.95 1.\n",
      " 0.93 0.97 0.98 0.94 0.97 1.   0.94 1.   0.92 0.77 0.55 1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.54 0.94 0.92 0.82 0.99 0.96 0.75 1.   0.98 0.97 0.98 0.76 1.   0.94\n",
      " 0.81 0.71 0.95 0.99 0.94 0.98 0.96 0.99 0.99 0.93 0.99 1.   0.98 0.94\n",
      " 0.98 0.96 0.68 0.96 0.96 0.97 0.53 0.9  0.98 0.81 0.92 0.91 0.96 0.88\n",
      " 0.79 0.98 0.96 0.87 0.94 0.77 0.97 1.   1.   0.96 0.97 1.   0.99 0.74\n",
      " 0.97 0.99 1.   0.95 0.85 0.97 0.99 0.7  0.65 0.84 0.95 0.88 0.94 0.99\n",
      " 0.83 0.97 0.81 0.8  0.92 0.85 0.98 0.83 0.93 0.94 1.   1.   0.98 0.99\n",
      " 1.   0.78 0.94 0.98 0.98 0.65 0.89 0.74 0.95 1.   0.9  0.54 0.93 1.\n",
      " 1.   0.95 0.93 0.99 1.   0.98 1.   0.98 0.82 0.97 0.9  0.83 0.89 0.75\n",
      " 0.85 0.94 1.   0.91 0.78 0.97 0.91 0.98 0.99 0.8  0.97 0.97 0.98 0.97\n",
      " 0.9  1.   0.49 0.77 0.97 0.96 0.96 0.96 0.88 0.77 0.84 1.   0.9  0.91\n",
      " 0.9  0.83 0.41 0.95 0.88 0.99 0.41 0.81 0.84 0.86 0.99 0.98 0.5  0.97\n",
      " 0.8  0.84 1.   0.87 0.94 0.91 1.   0.97 0.97 0.86 0.82 0.91]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   0.99 0.42 0.67 0.95 0.79 0.92 0.85 0.84 0.97 0.88 0.95 1.   0.9\n",
      " 0.99 1.   0.91 0.98 1.   0.98 1.   0.81 1.   0.94 0.5  0.84 0.91 0.83\n",
      " 0.93 0.97 0.84 0.9  0.89 0.87 1.   1.   0.97 0.9  0.89 0.95 0.94 0.99\n",
      " 1.   0.97 0.74 0.93 0.96 1.   0.96 1.   0.61 0.97 0.94 0.93 0.95 0.94\n",
      " 0.98 0.91 0.71 0.93 0.96 0.99 0.82 1.   0.96 1.   0.93 0.99 0.91 0.95\n",
      " 0.84 0.72 0.89 0.97 1.   0.88 1.   0.79 1.   1.   0.86 0.99 0.89 0.79\n",
      " 0.96 0.85 0.91 1.   0.99 0.94 0.95 0.98 1.   0.73 1.   0.95 0.97 0.97\n",
      " 0.98 0.94 0.71 0.99 1.   0.91 0.89 0.89 0.93 0.99 0.66 0.98 0.84 0.9\n",
      " 0.77 0.98 0.92 0.98 1.   1.   0.96 0.83 0.95 0.95 0.99 0.71 0.97 1.\n",
      " 1.   0.91 0.91 0.91 0.89 0.79 0.77 0.93 0.94 0.85 1.   0.9  0.9  0.97\n",
      " 1.   0.96 0.98 0.88 1.   0.98 0.93 1.   1.   0.96 0.76 0.77 0.95 0.96\n",
      " 1.   0.7  1.   0.94 0.96 0.79 0.97 0.94 0.76 0.98 0.91]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.89 0.93 0.75 0.87 0.95 0.87 0.94 0.91 0.78 0.71 0.93 1.   0.91 1.\n",
      " 0.99 0.81 0.69 0.96 0.99 0.98 0.81 0.97 1.   0.83 0.87 0.9  1.   0.74\n",
      " 0.58 0.93 0.89 0.83 0.83 0.7  1.   0.97 0.75 1.   0.78 0.96 0.75 0.95\n",
      " 0.89 0.96 0.88 0.99 0.96 0.85 0.85 1.   0.64 0.89 0.9  1.   0.96 0.74\n",
      " 1.   0.97 0.75 0.93 0.9  0.81 0.96 0.87 0.84 1.   0.88 0.95 0.84 0.9\n",
      " 0.86 0.94 0.95 0.76 0.8  0.89 1.   0.98 0.95 0.52 1.   0.87 0.95 0.7\n",
      " 1.   0.85 0.89 0.97 0.82 0.93 0.93 0.99 1.   0.96 0.89 0.94 0.96 0.97\n",
      " 0.97 0.93 0.98 0.96 0.88 0.95 0.98 0.78 0.95 0.91 0.72 0.85 0.99 0.89\n",
      " 0.96 1.   0.88 0.93 0.43 0.96 0.92 0.85 0.99 0.97 0.87 0.89 0.96 0.76\n",
      " 0.94 0.96 1.   0.88 0.68 0.9  0.99 0.78 0.9  0.99 0.9  0.72 0.95 0.96\n",
      " 0.97 0.94 1.   0.98 0.81 0.95 0.52 0.97 0.77 0.95 0.96 0.98 0.94 1.\n",
      " 0.95 0.96 0.93 0.84 1.   0.67 0.38 0.95 0.85 0.86 0.87]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6  65   5 752]\n",
      "공통 \n",
      "AUC : 0.6796658418144269\n",
      "AUPRC : 0.9528777598062302\n",
      "Optimized precision : 0.006096100693626181\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9154589371980676\n",
      "Precision(pb) : 0.9336768221038984\n",
      "Recall(pb) : 0.8208388375165125\n",
      "F1 score(pb) : 0.8736293609800961\n",
      "MCC : 0.19051030658901236\n",
      "G-mean : 0.2897393165673664\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  6  65]\n",
      " [  5 752]]\n",
      "Accuracy : 0.9154589371980676\n",
      "Precision : 0.9204406364749081\n",
      "Recall : 0.9933949801849405\n",
      "F1 score : 0.9555273189326556\n",
      "MCC : 0.20256792093008907\n",
      "G-mean : 0.2897393165673664\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n",
      "[1.   0.98 0.61 0.87 0.96 0.98 0.67 0.85 0.9  1.   1.   0.85 0.93 0.81\n",
      " 0.96 1.   0.85 0.89 1.   0.98 0.99 0.89 0.95 0.97 0.98 1.   0.95 1.\n",
      " 0.98 1.   0.81 1.   0.98 0.9  0.82 1.   1.   0.96 0.92 0.93 0.92 0.73\n",
      " 1.   0.88 0.88 1.   0.38 0.77 0.84 0.95 0.97 0.65 0.98 0.97 0.94 0.99\n",
      " 0.99 0.86 0.91 0.95 0.91 0.7  0.87 0.96 0.74 0.94 0.9  1.   0.85 0.96\n",
      " 0.99 0.79 0.8  0.96 0.96 0.91 0.95 0.89 0.98 0.79 1.   0.54 0.73 0.78\n",
      " 0.61 0.96 1.   0.96 0.98 0.77 0.82 0.94 0.99 0.97 1.   0.94 0.98 1.\n",
      " 0.94 0.92 0.9  0.91 0.93 0.96 0.81 0.96 0.9  0.91 0.92 0.79 0.87 0.48\n",
      " 0.9  0.89 0.82 0.88 0.98 0.93 0.68 0.81 0.89 0.68 1.   1.   0.97 1.\n",
      " 0.94 0.84 0.92 1.   1.   0.77 1.   0.97 1.   0.97 0.98 0.84 0.92 0.67\n",
      " 0.95 0.99 0.99 1.   0.92 0.98 1.   0.95 0.93 0.97 0.23 0.89 0.85 0.97\n",
      " 0.97 0.83 0.99 0.96 0.98 0.86 1.   1.   0.6  0.32 0.94 0.93]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.96 1.   0.85 0.78 0.89 1.   0.82 1.   0.99 0.92 0.99 0.58 0.93 0.61\n",
      " 0.93 0.89 1.   0.88 1.   0.99 0.95 0.98 0.73 0.99 0.89 0.98 0.96 0.62\n",
      " 0.95 0.92 1.   0.97 0.96 0.66 0.88 0.89 0.65 0.57 0.87 1.   0.88 0.96\n",
      " 0.99 0.86 0.93 0.99 0.94 0.99 0.98 0.97 0.98 1.   0.76 0.99 0.98 0.96\n",
      " 0.99 0.22 0.99 0.94 1.   0.5  0.88 1.   0.88 0.98 0.92 0.77 0.41 0.99\n",
      " 0.94 0.72 0.88 0.66 0.94 0.92 0.6  0.81 0.98 0.95 0.92 0.93 0.94 0.84\n",
      " 0.85 0.98 0.98 0.75 0.95 0.6  0.61 0.88 0.98 0.94 0.74 0.86 1.   0.94\n",
      " 1.   0.86 0.95 0.76 0.96 0.97 0.97 1.   0.97 0.9  0.99 0.66 0.86 0.94\n",
      " 0.86 1.   0.92 0.99 0.98 1.   0.99 0.99 0.97 0.52 0.94 0.97 0.86 0.99\n",
      " 0.73 0.97 0.96 1.   0.99 0.9  0.99 0.98 1.   1.   0.94 0.93 1.   0.99\n",
      " 0.72 0.97 1.   0.97 0.61 0.98 0.97 0.72 1.   0.98 0.99 0.6  0.86 0.97\n",
      " 0.94 0.99 0.98 0.98 0.95 1.   0.94 0.99 0.96 0.84 0.55 0.99]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.5  0.93 0.97 0.71 1.   0.96 0.82 0.99 0.99 0.98 0.99 0.83 0.99 0.97\n",
      " 0.74 0.74 0.93 1.   0.93 0.97 0.97 0.96 0.99 0.87 1.   0.99 0.99 0.9\n",
      " 0.98 0.99 0.77 0.98 0.98 0.96 0.56 0.88 0.99 0.82 0.88 0.93 0.98 0.91\n",
      " 0.85 0.97 1.   0.91 0.98 0.77 0.95 1.   1.   0.96 0.99 0.98 0.99 0.72\n",
      " 1.   1.   0.96 0.9  0.84 0.98 0.99 0.72 0.59 0.87 0.91 0.75 0.92 0.99\n",
      " 0.89 0.98 0.75 0.88 0.95 0.84 0.97 0.81 0.92 0.93 1.   0.98 0.98 0.99\n",
      " 1.   0.78 0.96 0.97 0.97 0.64 0.85 0.81 0.96 1.   0.85 0.59 0.88 1.\n",
      " 0.98 0.94 0.97 1.   0.99 0.98 1.   0.98 0.77 0.96 0.9  0.79 0.84 0.8\n",
      " 0.88 0.95 1.   0.91 0.8  0.99 0.74 1.   0.99 0.83 0.98 0.96 0.98 0.99\n",
      " 0.91 1.   0.48 0.73 0.95 0.94 0.94 0.97 0.76 0.71 0.91 1.   0.96 0.91\n",
      " 0.92 0.71 0.5  0.92 0.91 0.99 0.31 0.89 0.81 0.9  0.97 0.96 0.52 0.89\n",
      " 0.87 0.86 1.   0.97 0.94 0.93 0.98 0.99 0.98 0.84 0.86 0.96]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   1.   0.46 0.63 0.88 0.81 0.91 0.87 0.97 0.92 0.88 0.95 1.   0.87\n",
      " 0.99 1.   0.95 0.97 0.99 0.99 1.   0.93 0.99 0.95 0.52 0.87 0.97 0.86\n",
      " 0.96 0.98 0.86 0.88 0.87 0.9  0.99 1.   0.97 0.97 0.98 0.97 0.94 1.\n",
      " 0.98 0.98 0.67 0.97 0.93 0.98 0.98 1.   0.66 0.89 0.89 0.92 0.99 0.94\n",
      " 0.96 0.82 0.64 0.93 0.91 1.   0.78 1.   0.97 0.98 0.91 0.98 0.94 0.96\n",
      " 0.89 0.7  0.9  0.96 0.97 0.92 0.98 0.83 0.96 1.   0.82 1.   0.94 0.85\n",
      " 0.89 0.77 0.98 0.99 0.99 0.91 0.91 1.   1.   0.74 0.96 0.95 0.98 1.\n",
      " 0.99 0.94 0.62 0.98 1.   0.89 0.93 0.87 0.96 1.   0.56 0.98 0.75 0.94\n",
      " 0.74 0.99 0.97 1.   1.   0.97 0.96 0.79 0.96 0.93 1.   0.78 0.97 1.\n",
      " 0.99 0.93 0.9  0.95 0.87 0.75 0.76 0.91 0.96 0.87 0.99 0.91 0.96 0.96\n",
      " 0.99 0.88 0.96 0.92 1.   0.97 0.96 1.   1.   0.95 0.67 0.76 0.9  0.98\n",
      " 1.   0.76 1.   0.99 0.96 0.83 0.96 0.99 0.75 0.92 0.93]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.88 0.92 0.69 0.99 0.96 0.9  0.86 0.99 0.87 0.7  0.86 1.   0.94 0.99\n",
      " 1.   0.93 0.66 0.91 0.98 0.94 0.78 0.99 1.   0.78 0.81 0.89 1.   0.88\n",
      " 0.47 0.97 0.9  0.9  0.87 0.67 1.   0.95 0.77 1.   0.77 0.96 0.69 0.96\n",
      " 0.91 0.94 0.86 0.98 0.95 0.84 0.85 1.   0.6  0.86 0.91 1.   1.   0.76\n",
      " 1.   0.99 0.68 0.98 0.86 0.78 0.94 0.91 0.98 1.   0.9  0.98 0.75 0.92\n",
      " 0.87 0.97 0.97 0.75 0.87 0.91 0.97 0.98 0.93 0.43 1.   0.85 0.99 0.65\n",
      " 0.99 0.9  0.89 0.99 0.82 0.94 0.91 1.   1.   0.96 0.93 0.94 0.94 0.99\n",
      " 0.98 0.95 0.99 0.9  0.93 0.91 0.98 0.74 0.91 0.92 0.69 0.9  0.93 0.94\n",
      " 0.97 1.   0.79 0.92 0.34 0.93 0.98 0.89 1.   0.99 0.84 0.94 0.97 0.62\n",
      " 0.94 0.96 0.98 0.88 0.72 0.91 0.98 0.83 0.92 1.   0.92 0.72 0.92 0.83\n",
      " 1.   0.95 1.   0.97 0.83 0.87 0.52 0.96 0.73 0.96 0.95 0.94 0.92 1.\n",
      " 0.96 0.96 0.87 0.85 1.   0.65 0.37 0.95 0.76 0.87 0.81]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8  63   5 752]\n",
      "공통 \n",
      "AUC : 0.6636649487413252\n",
      "AUPRC : 0.9491376505848188\n",
      "Optimized precision : 0.03359230199640227\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9178743961352657\n",
      "Precision(pb) : 0.9326073959671307\n",
      "Recall(pb) : 0.8275753979804311\n",
      "F1 score(pb) : 0.8769577109046378\n",
      "MCC : 0.2389040138144472\n",
      "G-mean : 0.33456214482997443\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  8  63]\n",
      " [  5 752]]\n",
      "Accuracy : 0.9178743961352657\n",
      "Precision : 0.9226993865030675\n",
      "Recall : 0.9933949801849405\n",
      "F1 score : 0.9567430025445293\n",
      "MCC : 0.24967376149911882\n",
      "G-mean : 0.3345621448299744\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n",
      "[0.98 0.98 0.67 0.91 0.99 0.86 0.7  0.9  0.87 1.   1.   0.86 0.92 0.84\n",
      " 0.98 1.   0.94 0.85 1.   0.94 1.   0.8  0.96 1.   0.97 0.98 0.93 1.\n",
      " 0.99 1.   0.84 0.97 0.94 0.87 0.82 1.   0.99 0.97 0.96 0.88 0.95 0.7\n",
      " 0.98 0.92 0.93 1.   0.46 0.73 0.83 0.96 0.95 0.67 1.   0.94 0.94 0.97\n",
      " 0.98 0.85 0.95 0.93 0.91 0.7  0.9  0.9  0.79 0.99 0.93 0.99 0.83 0.97\n",
      " 0.95 0.67 0.84 0.91 0.98 0.9  0.97 0.95 0.97 0.83 1.   0.51 0.76 0.77\n",
      " 0.67 0.96 0.98 0.95 0.97 0.88 0.87 0.98 0.99 0.99 1.   0.9  0.95 1.\n",
      " 0.97 0.95 0.92 0.9  0.96 0.99 0.92 1.   0.89 0.93 0.88 0.77 0.93 0.4\n",
      " 0.95 0.92 0.74 0.83 0.99 0.91 0.82 0.84 0.8  0.65 0.98 1.   0.94 0.99\n",
      " 0.92 0.79 0.89 1.   1.   0.71 1.   0.99 0.99 0.93 1.   0.86 0.89 0.82\n",
      " 0.97 0.96 1.   1.   0.9  1.   1.   0.9  0.93 0.98 0.3  0.89 0.83 0.96\n",
      " 0.93 0.84 1.   0.98 1.   0.9  1.   1.   0.55 0.29 0.95 0.96]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.99 0.99 0.88 0.81 0.94 1.   0.84 1.   0.99 0.93 0.98 0.46 0.89 0.56\n",
      " 0.96 0.9  1.   0.84 1.   1.   0.96 0.99 0.72 0.99 0.86 0.98 0.98 0.68\n",
      " 0.96 0.92 0.97 0.97 0.98 0.72 0.92 0.92 0.73 0.59 0.92 0.99 0.87 0.98\n",
      " 0.95 0.9  0.95 1.   0.92 1.   0.98 0.95 0.96 0.98 0.78 0.97 0.99 0.98\n",
      " 0.98 0.39 1.   0.94 0.99 0.65 0.82 0.98 0.87 0.99 0.88 0.81 0.49 0.99\n",
      " 0.96 0.77 0.97 0.81 0.99 0.9  0.73 0.76 0.97 0.99 0.9  0.98 1.   0.81\n",
      " 0.86 0.95 0.98 0.66 0.86 0.73 0.72 0.81 0.91 0.9  0.7  0.8  0.99 0.95\n",
      " 1.   0.86 0.9  0.75 0.97 0.97 1.   1.   0.91 0.95 1.   0.61 0.8  0.94\n",
      " 0.83 0.99 0.94 1.   0.99 1.   1.   1.   1.   0.55 0.98 0.98 0.73 0.91\n",
      " 0.78 0.98 0.97 1.   1.   0.96 0.99 1.   1.   1.   0.95 0.92 0.98 0.97\n",
      " 0.65 0.98 1.   0.98 0.7  1.   0.97 0.74 1.   0.96 1.   0.56 0.91 0.98\n",
      " 0.89 0.98 0.99 0.95 0.95 1.   0.95 1.   0.95 0.86 0.54 1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.56 0.95 0.97 0.81 0.98 0.95 0.79 1.   1.   0.98 0.99 0.81 0.96 0.94\n",
      " 0.71 0.66 0.92 0.99 0.91 0.99 0.97 0.97 0.99 0.9  1.   0.97 0.99 0.96\n",
      " 1.   0.95 0.67 0.96 0.97 0.97 0.58 0.92 0.99 0.86 0.96 0.96 0.98 0.93\n",
      " 0.86 0.99 0.98 0.94 0.97 0.75 0.94 1.   0.99 0.95 0.98 0.94 0.99 0.82\n",
      " 0.99 0.98 0.98 0.95 0.83 0.99 1.   0.69 0.55 0.85 0.94 0.82 0.92 1.\n",
      " 0.92 0.98 0.78 0.89 0.98 0.78 0.98 0.77 0.98 0.98 0.99 0.93 0.99 0.97\n",
      " 1.   0.81 0.98 0.95 0.96 0.67 0.88 0.86 0.92 1.   0.82 0.64 0.92 1.\n",
      " 1.   0.97 0.95 0.99 0.98 0.98 1.   0.97 0.84 0.99 0.87 0.82 0.88 0.71\n",
      " 0.88 0.91 0.99 0.9  0.88 0.99 0.85 1.   1.   0.85 0.99 0.93 0.98 0.97\n",
      " 0.9  1.   0.43 0.77 0.96 0.97 0.99 0.98 0.83 0.73 0.77 0.99 0.95 0.93\n",
      " 0.91 0.86 0.52 0.93 0.94 0.99 0.3  0.86 0.82 0.86 0.96 0.93 0.54 0.9\n",
      " 0.83 0.86 0.98 0.96 0.96 0.93 0.99 0.98 0.97 0.8  0.83 0.89]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   1.   0.48 0.71 0.93 0.75 0.89 0.87 0.91 0.93 0.87 0.97 1.   0.91\n",
      " 1.   1.   0.94 0.98 0.99 0.99 1.   0.85 1.   0.95 0.54 0.87 0.93 0.8\n",
      " 0.97 0.98 0.83 0.86 0.89 0.88 1.   1.   0.99 0.87 0.93 0.99 0.92 0.99\n",
      " 1.   0.97 0.72 0.96 0.9  1.   0.96 1.   0.54 0.95 0.92 0.93 0.96 0.95\n",
      " 0.96 0.88 0.59 0.94 0.97 1.   0.83 1.   0.94 0.99 0.96 1.   0.87 0.94\n",
      " 0.9  0.77 0.91 0.99 1.   0.95 0.98 0.86 0.99 1.   0.84 1.   0.93 0.83\n",
      " 0.83 0.87 0.98 0.98 1.   0.93 0.88 1.   1.   0.74 0.98 0.9  0.99 1.\n",
      " 0.98 0.93 0.66 1.   1.   0.96 0.9  0.91 0.94 1.   0.65 0.95 0.8  0.88\n",
      " 0.79 0.99 0.95 1.   1.   0.97 0.98 0.82 0.99 0.96 1.   0.78 0.99 1.\n",
      " 1.   0.95 0.95 0.94 0.9  0.75 0.81 0.91 0.98 0.91 1.   0.92 0.93 0.95\n",
      " 0.99 0.89 0.96 0.93 1.   0.98 0.91 0.98 1.   0.87 0.78 0.67 0.92 0.98\n",
      " 1.   0.77 1.   0.98 0.96 0.91 0.99 0.95 0.81 0.99 0.92]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.87 0.97 0.78 0.95 0.97 0.88 0.91 0.98 0.78 0.71 0.87 1.   0.97 0.98\n",
      " 0.99 0.93 0.71 0.97 1.   0.99 0.9  0.99 1.   0.78 0.87 0.9  1.   0.76\n",
      " 0.57 0.92 0.9  0.84 0.84 0.65 1.   0.96 0.81 1.   0.77 0.97 0.68 0.95\n",
      " 0.88 0.94 0.8  0.99 0.98 0.89 0.85 0.99 0.63 0.91 0.91 1.   0.92 0.89\n",
      " 1.   0.99 0.73 0.92 0.94 0.77 0.97 0.96 0.88 1.   0.92 0.96 0.77 0.95\n",
      " 0.88 0.96 0.97 0.78 0.92 0.89 1.   0.98 0.96 0.44 1.   0.88 0.96 0.69\n",
      " 0.99 0.89 0.93 0.94 0.8  0.95 0.9  1.   0.99 0.97 0.91 0.91 0.95 0.99\n",
      " 0.99 0.9  1.   0.95 0.91 0.96 0.99 0.76 0.96 0.93 0.67 0.84 0.97 0.9\n",
      " 1.   1.   0.77 0.9  0.34 0.94 0.91 0.8  1.   0.97 0.92 0.9  0.97 0.67\n",
      " 0.97 0.94 0.97 0.91 0.65 0.84 0.99 0.75 0.95 1.   0.92 0.67 0.89 0.88\n",
      " 0.96 0.94 1.   0.98 0.77 0.93 0.51 0.95 0.76 0.98 0.95 0.97 0.94 1.\n",
      " 0.95 0.99 0.81 0.88 1.   0.71 0.3  0.96 0.8  0.85 0.83]\n",
      "[  7  64   6 751]\n",
      "공통 \n",
      "AUC : 0.6677395947680801\n",
      "AUPRC : 0.9500979788093996\n",
      "Optimized precision : 0.020149633189773808\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9154589371980676\n",
      "Precision(pb) : 0.9327707369740114\n",
      "Recall(pb) : 0.8310709242472556\n",
      "F1 score(pb) : 0.8789889200723895\n",
      "MCC : 0.204206149678618\n",
      "G-mean : 0.3127460796424784\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  7  64]\n",
      " [  6 751]]\n",
      "Accuracy : 0.9154589371980676\n",
      "Precision : 0.9214723926380368\n",
      "Recall : 0.9920739762219286\n",
      "F1 score : 0.955470737913486\n",
      "MCC : 0.2178673860412754\n",
      "G-mean : 0.31274607964247836\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.   0.98 0.66 0.87 0.95 0.92 0.77 0.83 0.95 1.   0.99 0.86 0.87 0.86\n",
      " 0.95 1.   0.81 0.9  1.   0.94 1.   0.8  0.95 0.98 0.98 1.   0.97 1.\n",
      " 0.94 1.   0.77 0.99 0.89 0.83 0.82 1.   1.   0.95 0.93 0.86 0.96 0.7\n",
      " 0.98 0.9  0.88 1.   0.4  0.69 0.81 0.97 0.93 0.62 0.98 0.95 0.94 0.97\n",
      " 0.99 0.87 0.91 0.89 0.9  0.68 0.84 0.9  0.75 1.   0.91 0.97 0.87 0.98\n",
      " 0.98 0.76 0.79 0.95 0.99 0.86 0.96 0.94 0.94 0.92 1.   0.56 0.74 0.78\n",
      " 0.58 0.98 1.   0.99 0.99 0.79 0.85 0.96 0.99 1.   1.   0.9  0.93 1.\n",
      " 0.98 0.93 0.93 0.95 0.92 0.99 0.89 0.98 0.85 0.93 0.93 0.86 0.84 0.53\n",
      " 0.89 0.95 0.76 0.74 1.   0.94 0.82 0.8  0.87 0.66 1.   0.98 0.96 1.\n",
      " 0.92 0.84 0.93 0.99 1.   0.71 1.   1.   0.99 0.92 0.99 0.84 0.92 0.77\n",
      " 0.92 0.99 0.99 1.   0.94 0.99 0.99 0.94 0.93 0.95 0.18 0.8  0.78 0.96\n",
      " 0.94 0.82 0.99 0.99 0.97 0.87 1.   0.99 0.58 0.36 0.97 0.97]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.97 1.   0.85 0.77 0.9  0.99 0.83 1.   1.   0.93 1.   0.47 0.93 0.61\n",
      " 0.93 0.84 1.   0.9  1.   0.99 0.98 0.99 0.71 0.97 0.88 0.95 0.97 0.63\n",
      " 0.95 0.91 1.   0.97 0.96 0.72 0.93 0.91 0.75 0.61 0.92 1.   0.87 0.99\n",
      " 0.96 0.89 0.91 1.   0.87 1.   0.99 0.97 0.95 1.   0.74 1.   1.   0.98\n",
      " 1.   0.29 0.96 0.94 0.99 0.56 0.78 1.   0.88 0.96 0.9  0.8  0.47 1.\n",
      " 0.94 0.66 0.99 0.78 0.98 0.9  0.71 0.76 0.96 0.99 0.88 0.99 0.99 0.85\n",
      " 0.88 0.98 0.95 0.8  0.91 0.64 0.73 0.8  0.98 0.94 0.69 0.87 0.98 0.91\n",
      " 0.99 0.85 0.95 0.7  0.99 0.96 1.   1.   0.92 0.96 1.   0.63 0.84 0.96\n",
      " 0.85 1.   0.95 1.   0.99 0.99 0.99 0.97 1.   0.49 0.98 0.97 0.71 0.98\n",
      " 0.79 0.97 0.95 1.   0.99 0.94 0.99 1.   1.   1.   0.97 0.98 0.99 1.\n",
      " 0.69 0.96 1.   0.99 0.63 1.   0.97 0.76 1.   0.99 1.   0.59 0.83 0.99\n",
      " 0.92 1.   0.99 0.94 0.95 1.   0.94 1.   0.95 0.79 0.56 1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.51 0.94 0.96 0.78 0.99 0.97 0.88 0.99 0.97 0.98 0.99 0.78 1.   0.96\n",
      " 0.71 0.64 0.93 0.98 0.92 0.99 0.97 0.94 1.   0.89 1.   1.   0.97 0.95\n",
      " 0.99 0.98 0.77 0.94 0.97 0.95 0.54 0.86 0.99 0.85 0.94 0.93 0.99 0.95\n",
      " 0.84 0.97 0.99 0.91 0.89 0.8  0.96 1.   0.98 0.97 0.98 0.97 0.98 0.79\n",
      " 0.99 1.   1.   0.96 0.86 1.   1.   0.74 0.58 0.86 0.94 0.78 0.92 0.99\n",
      " 0.87 0.95 0.74 0.9  0.94 0.85 0.96 0.86 0.95 0.97 0.99 0.97 1.   0.98\n",
      " 1.   0.83 1.   0.98 0.97 0.61 0.87 0.87 0.95 1.   0.88 0.58 0.92 1.\n",
      " 0.99 0.88 0.98 0.98 0.99 0.99 0.99 0.96 0.78 0.99 0.89 0.81 0.87 0.76\n",
      " 0.89 0.97 1.   0.9  0.85 1.   0.87 1.   1.   0.86 0.97 0.95 0.98 1.\n",
      " 0.91 1.   0.45 0.85 0.95 0.97 0.94 1.   0.87 0.81 0.88 1.   0.89 0.94\n",
      " 0.92 0.82 0.62 0.94 0.85 0.99 0.36 0.91 0.86 0.86 0.99 0.95 0.47 0.93\n",
      " 0.86 0.81 0.97 0.98 0.96 0.96 1.   0.96 0.98 0.84 0.91 0.93]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   0.99 0.46 0.72 0.89 0.77 0.93 0.85 0.9  0.98 0.86 0.98 1.   0.9\n",
      " 1.   1.   0.92 0.98 1.   0.98 1.   0.85 1.   0.96 0.52 0.87 0.93 0.81\n",
      " 0.97 0.96 0.85 0.89 0.86 0.84 1.   0.97 0.95 0.96 0.91 0.99 0.94 0.98\n",
      " 0.97 0.96 0.75 0.97 0.86 1.   0.99 1.   0.65 0.96 0.96 0.92 0.99 0.93\n",
      " 0.96 0.84 0.63 0.95 0.96 0.99 0.84 0.99 0.97 0.99 0.9  1.   0.95 0.94\n",
      " 0.88 0.84 0.93 0.96 1.   0.92 0.99 0.79 0.96 0.98 0.9  0.99 0.93 0.85\n",
      " 0.85 0.89 0.95 0.98 0.99 0.94 0.87 0.99 1.   0.73 0.98 0.94 0.99 0.99\n",
      " 0.99 0.97 0.71 1.   1.   0.9  0.89 0.92 0.91 0.99 0.67 1.   0.88 0.92\n",
      " 0.8  1.   0.95 1.   1.   0.97 0.96 0.82 0.94 0.96 1.   0.71 0.95 1.\n",
      " 1.   0.95 0.95 0.91 0.94 0.79 0.8  0.85 0.97 0.86 1.   0.95 0.94 0.96\n",
      " 1.   0.94 0.95 0.89 1.   0.96 0.95 1.   1.   0.92 0.78 0.72 0.97 0.97\n",
      " 1.   0.75 1.   0.98 1.   0.82 0.99 0.96 0.74 0.97 0.92]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.89 0.95 0.82 0.96 0.94 0.9  0.88 0.97 0.75 0.76 0.84 1.   0.94 1.\n",
      " 1.   0.86 0.6  0.95 1.   0.97 0.77 0.98 1.   0.8  0.82 0.88 1.   0.82\n",
      " 0.61 0.99 0.86 0.84 0.86 0.72 1.   0.97 0.84 1.   0.77 0.97 0.82 0.96\n",
      " 0.86 0.95 0.86 0.97 0.98 0.85 0.77 1.   0.69 0.84 0.84 1.   0.95 0.8\n",
      " 0.99 0.98 0.7  0.93 0.9  0.8  0.97 0.91 0.93 1.   0.92 0.97 0.81 0.99\n",
      " 0.9  0.98 0.95 0.78 0.88 0.94 0.99 0.98 0.97 0.54 1.   0.91 0.98 0.71\n",
      " 0.99 0.82 0.89 0.93 0.81 0.9  0.94 1.   1.   0.94 0.94 0.87 0.91 0.98\n",
      " 0.99 0.87 0.96 0.91 0.85 0.96 0.98 0.77 0.93 0.92 0.68 0.83 0.96 0.9\n",
      " 0.94 0.97 0.76 0.87 0.39 0.94 0.94 0.77 1.   1.   0.88 0.93 0.95 0.68\n",
      " 0.98 0.96 0.97 0.86 0.67 0.86 0.99 0.83 0.93 0.98 0.9  0.67 0.91 0.86\n",
      " 0.99 0.92 0.99 0.97 0.81 0.85 0.47 0.98 0.68 0.97 1.   0.95 0.92 1.\n",
      " 0.97 0.99 0.88 0.84 1.   0.69 0.29 0.95 0.75 0.89 0.83]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6  65   8 749]\n",
      "공통 \n",
      "AUC : 0.659106554784453\n",
      "AUPRC : 0.9476817056407604\n",
      "Optimized precision : 0.006435023480850452\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9118357487922706\n",
      "Precision(pb) : 0.9319562327137783\n",
      "Recall(pb) : 0.8251639667199704\n",
      "F1 score(pb) : 0.8753148498814426\n",
      "MCC : 0.16057374204460284\n",
      "G-mean : 0.28916080154780077\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  6  65]\n",
      " [  8 749]]\n",
      "Accuracy : 0.9118357487922706\n",
      "Precision : 0.9201474201474201\n",
      "Recall : 0.9894319682959049\n",
      "F1 score : 0.9535327816677276\n",
      "MCC : 0.179281754768974\n",
      "G-mean : 0.28916080154780077\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n",
      "[1.   0.98 0.7  0.88 0.95 0.88 0.73 0.86 0.94 1.   0.99 0.88 0.89 0.86\n",
      " 0.95 1.   0.9  0.82 0.99 0.99 1.   0.8  0.96 0.96 0.99 0.99 0.98 1.\n",
      " 0.99 1.   0.76 0.99 0.95 0.82 0.83 1.   1.   0.95 0.95 0.91 0.87 0.75\n",
      " 0.99 0.88 0.88 1.   0.53 0.76 0.77 0.97 0.95 0.59 0.98 0.94 0.91 0.99\n",
      " 0.98 0.8  0.92 0.87 0.89 0.72 0.85 0.91 0.76 0.94 0.83 1.   0.79 0.99\n",
      " 0.95 0.74 0.8  0.97 0.95 0.89 0.95 0.93 0.91 0.87 1.   0.59 0.75 0.79\n",
      " 0.67 0.93 1.   0.94 1.   0.73 0.88 0.97 0.96 1.   0.99 0.86 0.91 1.\n",
      " 0.98 0.93 0.9  0.88 0.87 0.99 0.88 1.   0.94 0.91 0.9  0.76 0.89 0.42\n",
      " 0.91 0.95 0.8  0.75 0.97 0.94 0.79 0.87 0.86 0.59 0.99 0.97 0.94 1.\n",
      " 0.9  0.82 0.94 1.   1.   0.76 1.   1.   0.99 0.98 0.99 0.8  0.92 0.77\n",
      " 0.88 0.96 1.   1.   0.9  0.96 0.99 0.88 0.92 0.96 0.24 0.92 0.8  0.98\n",
      " 0.97 0.83 1.   0.99 0.95 0.87 1.   1.   0.62 0.27 0.95 0.88]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.99 1.   0.8  0.77 0.91 1.   0.72 1.   0.98 0.91 0.99 0.49 0.93 0.65\n",
      " 0.95 0.86 1.   0.86 1.   1.   0.95 1.   0.82 0.97 0.87 0.99 0.96 0.69\n",
      " 0.96 0.93 1.   0.99 0.95 0.72 0.91 0.87 0.72 0.62 0.93 1.   0.85 0.99\n",
      " 0.95 0.9  0.95 1.   0.95 1.   0.95 0.97 0.96 0.99 0.8  0.99 1.   0.98\n",
      " 0.97 0.31 0.99 0.99 0.99 0.61 0.86 0.97 0.89 0.93 0.91 0.7  0.51 1.\n",
      " 0.95 0.77 0.95 0.76 0.98 0.91 0.72 0.7  0.94 0.98 0.95 0.97 0.99 0.85\n",
      " 0.89 0.99 0.98 0.7  0.91 0.62 0.69 0.78 0.89 0.94 0.65 0.84 0.99 0.91\n",
      " 1.   0.81 0.98 0.71 0.99 0.99 0.96 0.99 0.93 0.88 1.   0.67 0.86 0.92\n",
      " 0.84 1.   0.94 1.   0.99 0.99 0.98 0.99 0.97 0.58 0.99 0.95 0.77 0.95\n",
      " 0.76 0.98 0.98 0.99 1.   0.89 0.94 1.   0.99 0.97 0.94 0.94 0.99 0.97\n",
      " 0.71 0.97 0.99 0.99 0.61 0.99 0.96 0.76 1.   0.98 0.97 0.61 0.85 0.99\n",
      " 0.94 0.98 1.   0.97 0.97 1.   0.96 0.99 0.95 0.83 0.56 1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.42 0.97 0.97 0.74 0.97 0.96 0.8  0.99 0.99 0.94 0.97 0.8  0.99 0.96\n",
      " 0.7  0.78 0.86 0.98 0.93 0.97 0.99 0.97 0.99 0.86 0.98 0.97 0.99 0.97\n",
      " 0.98 0.96 0.69 0.97 0.98 0.99 0.61 0.83 0.99 0.85 0.89 0.9  0.98 0.97\n",
      " 0.72 0.99 1.   0.92 0.96 0.74 0.9  1.   0.98 0.98 1.   0.98 1.   0.74\n",
      " 1.   1.   0.98 0.92 0.85 0.98 1.   0.75 0.65 0.8  0.89 0.82 0.91 1.\n",
      " 0.92 0.99 0.74 0.91 0.97 0.85 0.99 0.86 0.95 0.96 0.99 0.97 0.98 0.98\n",
      " 1.   0.72 0.98 0.97 0.96 0.62 0.83 0.78 0.91 1.   0.86 0.6  0.94 1.\n",
      " 0.98 0.91 0.91 1.   0.99 0.99 1.   0.95 0.78 0.99 0.88 0.83 0.86 0.64\n",
      " 0.92 0.94 0.99 0.9  0.86 0.99 0.86 1.   1.   0.77 0.99 0.96 0.97 0.97\n",
      " 0.87 0.98 0.54 0.74 0.89 0.91 0.93 0.97 0.82 0.71 0.86 1.   0.94 0.93\n",
      " 0.9  0.76 0.5  0.92 0.88 1.   0.42 0.9  0.92 0.85 0.99 0.98 0.46 0.91\n",
      " 0.85 0.78 0.98 0.94 0.9  0.9  1.   0.98 0.98 0.82 0.88 0.94]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   0.99 0.51 0.65 0.87 0.81 0.91 0.89 0.89 0.91 0.92 0.93 1.   0.93\n",
      " 1.   1.   0.88 0.99 0.99 0.99 1.   0.86 1.   0.96 0.47 0.92 0.92 0.79\n",
      " 0.95 0.97 0.86 0.91 0.91 0.87 1.   0.99 0.98 0.98 0.9  0.97 0.97 0.95\n",
      " 0.99 0.97 0.66 0.95 0.9  1.   0.98 1.   0.57 0.96 0.95 0.97 0.93 0.98\n",
      " 0.99 0.85 0.67 0.94 0.99 1.   0.77 1.   0.93 0.99 0.94 1.   0.93 0.93\n",
      " 0.89 0.77 0.89 0.96 1.   0.92 0.99 0.79 0.97 0.99 0.84 1.   0.91 0.87\n",
      " 0.87 0.9  0.97 1.   0.98 0.94 0.93 1.   0.99 0.71 0.98 0.93 0.97 0.99\n",
      " 0.98 1.   0.78 0.99 1.   0.95 0.9  0.81 0.95 0.98 0.69 0.98 0.81 0.88\n",
      " 0.74 1.   0.97 1.   1.   0.99 0.98 0.85 0.99 0.97 1.   0.71 0.95 1.\n",
      " 0.98 0.92 0.96 0.79 0.96 0.75 0.68 0.89 0.92 0.78 1.   0.94 0.92 0.92\n",
      " 1.   0.89 0.96 0.9  1.   0.98 0.9  1.   1.   0.96 0.81 0.7  0.96 0.98\n",
      " 1.   0.78 1.   0.97 0.98 0.86 0.98 0.95 0.78 0.96 0.92]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.91 0.95 0.72 0.95 0.97 0.92 0.88 0.97 0.76 0.68 0.87 0.98 0.92 0.99\n",
      " 0.99 0.86 0.62 0.92 1.   0.95 0.76 0.97 1.   0.82 0.89 0.83 0.99 0.84\n",
      " 0.49 0.92 0.89 0.85 0.88 0.72 1.   0.99 0.74 1.   0.74 0.92 0.78 0.93\n",
      " 0.86 0.97 0.85 0.99 0.96 0.86 0.82 1.   0.57 0.92 0.89 0.99 0.94 0.79\n",
      " 1.   0.99 0.67 0.96 0.89 0.77 0.98 0.9  0.84 1.   0.87 0.95 0.75 0.97\n",
      " 0.84 0.97 0.95 0.76 0.87 0.92 0.99 0.98 0.94 0.53 0.99 0.86 0.98 0.6\n",
      " 0.98 0.85 0.89 0.94 0.81 0.91 0.9  0.99 1.   0.96 0.95 0.92 0.96 0.98\n",
      " 0.97 0.97 0.99 0.96 0.91 0.94 0.97 0.81 0.88 0.94 0.61 0.92 0.99 0.91\n",
      " 0.97 1.   0.81 0.9  0.36 0.91 0.94 0.82 1.   0.98 0.89 0.94 0.97 0.62\n",
      " 0.91 0.9  0.98 0.91 0.67 0.89 0.97 0.84 0.96 1.   0.96 0.64 0.93 0.88\n",
      " 0.94 0.91 0.99 0.99 0.8  0.89 0.51 0.96 0.79 0.95 0.95 0.98 0.92 1.\n",
      " 0.98 0.99 0.87 0.84 1.   0.82 0.37 0.98 0.77 0.89 0.87]\n",
      "[  5  66   7 750]\n",
      "공통 \n",
      "AUC : 0.6557389249632537\n",
      "AUPRC : 0.9472447296624772\n",
      "Optimized precision : -0.008004979568016702\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9118357487922706\n",
      "Precision(pb) : 0.9319807775747224\n",
      "Recall(pb) : 0.8197640733273076\n",
      "F1 score(pb) : 0.8722781266849396\n",
      "MCC : 0.14332402378067838\n",
      "G-mean : 0.26414264342315424\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  5  66]\n",
      " [  7 750]]\n",
      "Accuracy : 0.9118357487922706\n",
      "Precision : 0.9191176470588235\n",
      "Recall : 0.9907529722589168\n",
      "F1 score : 0.9535918626827716\n",
      "MCC : 0.16089080650075543\n",
      "G-mean : 0.26414264342315424\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.   0.97 0.65 0.91 0.96 0.9  0.7  0.85 0.9  1.   0.98 0.87 0.9  0.8\n",
      " 0.99 1.   0.91 0.87 1.   0.92 1.   0.88 0.95 0.98 0.95 1.   0.96 1.\n",
      " 0.96 1.   0.79 0.99 0.94 0.9  0.85 0.99 1.   0.95 0.93 0.86 0.91 0.76\n",
      " 0.97 0.97 0.94 1.   0.47 0.79 0.83 0.98 0.99 0.6  0.96 0.93 0.94 0.98\n",
      " 0.98 0.83 0.91 0.89 0.91 0.64 0.88 0.95 0.77 0.96 0.91 0.99 0.83 0.95\n",
      " 0.97 0.82 0.83 0.98 0.95 0.92 0.93 0.89 0.94 0.86 1.   0.57 0.75 0.75\n",
      " 0.66 0.92 1.   0.94 1.   0.75 0.78 0.97 0.97 0.97 1.   0.84 0.97 1.\n",
      " 0.97 0.91 0.95 0.96 0.94 0.99 0.91 0.98 0.91 0.94 0.83 0.83 0.86 0.44\n",
      " 0.93 0.96 0.77 0.76 0.99 0.93 0.77 0.87 0.87 0.55 0.99 1.   0.97 1.\n",
      " 0.88 0.84 0.93 1.   1.   0.62 1.   0.95 1.   0.93 1.   0.86 0.98 0.75\n",
      " 0.96 0.98 1.   0.99 0.93 0.98 0.96 0.88 0.93 0.95 0.26 0.94 0.85 0.96\n",
      " 0.96 0.88 1.   0.99 1.   0.88 1.   1.   0.57 0.31 0.9  0.94]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.98 1.   0.86 0.79 0.87 1.   0.78 1.   1.   0.92 1.   0.55 0.93 0.62\n",
      " 0.97 0.83 1.   0.91 1.   0.98 0.95 0.99 0.68 1.   0.8  0.98 0.96 0.63\n",
      " 0.92 0.91 0.98 0.93 0.95 0.81 0.89 0.92 0.72 0.56 0.87 0.99 0.89 0.96\n",
      " 0.96 0.86 0.94 1.   0.92 0.99 0.97 0.99 0.97 0.99 0.8  0.99 1.   0.96\n",
      " 0.98 0.32 1.   0.96 0.98 0.7  0.86 0.99 0.85 0.98 0.95 0.82 0.38 1.\n",
      " 0.94 0.79 0.94 0.76 0.96 0.9  0.69 0.77 0.97 0.95 0.91 0.98 0.96 0.89\n",
      " 0.85 0.95 0.94 0.7  0.9  0.64 0.7  0.78 0.87 0.95 0.63 0.76 0.98 0.94\n",
      " 1.   0.8  0.94 0.7  0.98 0.97 0.98 1.   0.91 0.94 0.99 0.71 0.88 0.97\n",
      " 0.8  1.   0.94 1.   0.99 1.   0.96 1.   0.98 0.65 0.97 0.97 0.76 0.99\n",
      " 0.7  1.   0.98 1.   0.99 0.96 0.97 1.   1.   0.99 0.94 0.91 0.98 0.98\n",
      " 0.69 0.9  0.98 1.   0.68 1.   0.96 0.7  1.   0.98 0.99 0.6  0.92 1.\n",
      " 0.91 0.98 1.   0.95 0.95 1.   0.93 0.99 0.96 0.8  0.55 1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.51 0.98 0.94 0.72 0.99 0.92 0.81 0.99 0.96 0.98 0.99 0.87 1.   0.94\n",
      " 0.73 0.73 0.92 0.98 0.92 0.99 0.97 0.98 1.   0.87 1.   1.   0.98 0.92\n",
      " 1.   0.94 0.77 0.94 0.99 0.93 0.47 0.84 0.98 0.83 0.94 0.94 0.98 0.94\n",
      " 0.76 0.97 1.   0.95 0.97 0.69 0.94 1.   1.   0.94 1.   0.99 1.   0.81\n",
      " 0.99 0.98 1.   0.97 0.76 1.   1.   0.72 0.62 0.84 0.95 0.79 0.92 1.\n",
      " 0.86 0.99 0.7  0.93 0.98 0.8  0.98 0.76 0.96 0.98 1.   0.99 1.   1.\n",
      " 1.   0.8  0.99 0.99 1.   0.6  0.88 0.84 0.89 1.   0.91 0.61 0.93 1.\n",
      " 0.98 0.95 0.93 1.   1.   0.98 0.99 0.96 0.86 0.97 0.83 0.81 0.85 0.73\n",
      " 0.85 0.95 1.   0.92 0.76 0.99 0.9  1.   1.   0.84 0.99 1.   0.99 1.\n",
      " 0.91 0.99 0.45 0.87 0.93 0.97 0.91 0.99 0.89 0.84 0.8  0.98 0.96 0.97\n",
      " 0.93 0.78 0.59 0.85 0.78 1.   0.35 0.88 0.86 0.86 0.99 0.99 0.37 0.95\n",
      " 0.88 0.91 0.97 0.94 0.98 0.97 0.99 0.99 0.96 0.9  0.86 0.86]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   1.   0.5  0.67 0.93 0.78 0.89 0.92 0.89 0.9  0.9  0.94 1.   0.89\n",
      " 0.99 1.   0.93 0.98 1.   0.99 1.   0.82 1.   0.98 0.49 0.94 0.99 0.72\n",
      " 0.99 0.98 0.87 0.9  0.89 0.85 1.   1.   0.97 0.95 0.94 0.97 0.94 0.99\n",
      " 1.   0.98 0.8  0.93 0.91 1.   0.98 1.   0.5  0.93 0.96 0.92 0.99 0.94\n",
      " 0.95 0.85 0.67 0.94 0.94 1.   0.8  1.   0.94 0.96 0.93 1.   0.89 0.94\n",
      " 0.88 0.84 0.89 0.98 0.99 0.89 0.97 0.86 0.98 0.99 0.74 0.99 0.96 0.86\n",
      " 0.91 0.8  0.93 0.99 0.97 0.94 0.9  0.96 1.   0.7  0.95 0.98 0.99 0.98\n",
      " 0.98 0.97 0.73 0.99 0.99 0.96 0.9  0.86 0.9  1.   0.61 0.99 0.82 0.89\n",
      " 0.71 0.97 0.91 0.99 0.99 0.99 1.   0.78 0.95 0.96 1.   0.73 0.98 1.\n",
      " 1.   0.93 0.89 0.89 0.98 0.76 0.72 0.89 0.98 0.88 0.98 0.94 0.98 0.95\n",
      " 0.99 0.93 0.98 0.96 1.   0.98 0.93 1.   0.99 0.89 0.81 0.75 0.96 1.\n",
      " 1.   0.81 1.   0.94 0.98 0.83 1.   0.98 0.87 0.97 0.89]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.86 0.94 0.74 0.93 0.91 0.87 0.91 0.97 0.82 0.79 0.92 0.98 0.94 1.\n",
      " 1.   0.92 0.67 0.93 1.   0.91 0.69 0.96 0.99 0.83 0.87 0.92 1.   0.74\n",
      " 0.52 0.97 0.91 0.84 0.89 0.7  1.   0.99 0.8  1.   0.66 0.99 0.76 0.97\n",
      " 0.9  0.97 0.87 0.99 0.96 0.88 0.88 0.98 0.6  0.87 0.89 1.   0.97 0.69\n",
      " 1.   1.   0.71 0.97 0.92 0.84 0.97 0.92 0.83 0.99 0.9  0.97 0.8  0.96\n",
      " 0.91 0.98 0.96 0.76 0.86 0.88 1.   0.97 0.95 0.41 0.99 0.86 0.97 0.78\n",
      " 0.98 0.88 0.94 0.95 0.87 0.93 0.86 0.98 1.   0.96 0.91 0.92 0.91 0.98\n",
      " 0.96 0.9  0.98 0.97 0.94 0.92 0.99 0.77 0.89 0.89 0.74 0.86 0.96 0.9\n",
      " 0.93 0.99 0.84 0.92 0.42 0.93 0.94 0.79 1.   1.   0.9  0.96 0.94 0.66\n",
      " 1.   0.94 0.97 0.89 0.66 0.9  1.   0.86 0.91 1.   0.89 0.67 0.91 0.88\n",
      " 0.98 0.95 1.   0.97 0.84 0.91 0.44 0.99 0.83 0.97 0.94 0.93 0.92 0.99\n",
      " 0.98 0.97 0.87 0.88 1.   0.73 0.36 0.96 0.75 0.91 0.81]\n",
      "[  8  63   7 750]\n",
      "공통 \n",
      "AUC : 0.6444917855880329\n",
      "AUPRC : 0.9465334480901351\n",
      "Optimized precision : 0.03387804771160652\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9154589371980676\n",
      "Precision(pb) : 0.9307187435542682\n",
      "Recall(pb) : 0.8357840483357515\n",
      "F1 score(pb) : 0.8807004245008634\n",
      "MCC : 0.21713441231436417\n",
      "G-mean : 0.3341169521579452\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  8  63]\n",
      " [  7 750]]\n",
      "Accuracy : 0.9154589371980676\n",
      "Precision : 0.922509225092251\n",
      "Recall : 0.9907529722589168\n",
      "F1 score : 0.9554140127388534\n",
      "MCC : 0.23217250347123233\n",
      "G-mean : 0.3341169521579451\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99 0.96 0.68 0.9  0.98 0.86 0.7  0.9  0.93 1.   0.97 0.8  0.91 0.88\n",
      " 0.94 1.   0.88 0.83 1.   0.95 1.   0.84 0.95 0.98 0.99 1.   0.94 1.\n",
      " 0.99 1.   0.8  0.98 0.93 0.78 0.81 1.   1.   0.99 0.9  0.9  0.89 0.73\n",
      " 0.99 0.93 0.89 1.   0.42 0.8  0.85 0.95 0.99 0.63 0.97 0.96 0.97 0.99\n",
      " 0.96 0.8  0.96 0.91 0.91 0.73 0.9  0.92 0.72 0.96 0.87 0.96 0.83 0.99\n",
      " 0.96 0.81 0.81 0.98 0.96 0.92 0.93 0.88 0.94 0.8  0.99 0.48 0.74 0.76\n",
      " 0.65 0.95 0.99 0.94 1.   0.8  0.88 0.96 0.96 0.99 0.99 0.86 0.95 1.\n",
      " 0.96 0.91 0.9  0.89 0.87 0.98 0.91 0.99 0.9  0.9  0.94 0.76 0.89 0.47\n",
      " 0.92 0.89 0.86 0.81 1.   0.97 0.82 0.8  0.83 0.57 0.97 0.97 0.98 0.98\n",
      " 0.94 0.76 0.89 1.   1.   0.67 1.   0.99 0.98 0.97 0.98 0.81 0.92 0.75\n",
      " 0.91 0.97 1.   0.97 0.95 0.99 0.97 0.88 0.93 0.97 0.22 0.9  0.77 0.95\n",
      " 0.93 0.8  0.99 0.99 1.   0.89 1.   1.   0.56 0.3  0.89 0.92]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.97 1.   0.9  0.77 0.92 1.   0.76 1.   0.97 0.94 0.99 0.54 0.95 0.51\n",
      " 0.99 0.89 1.   0.82 1.   0.97 0.97 1.   0.74 0.98 0.89 0.98 0.98 0.68\n",
      " 0.88 0.95 1.   1.   0.94 0.68 0.91 0.9  0.7  0.58 0.92 0.99 0.91 0.94\n",
      " 1.   0.94 0.9  1.   0.85 0.98 0.96 0.99 0.98 1.   0.79 0.98 1.   0.98\n",
      " 0.99 0.29 0.99 0.96 0.98 0.62 0.84 0.97 0.88 0.94 0.9  0.81 0.43 1.\n",
      " 1.   0.74 0.96 0.71 1.   0.94 0.74 0.82 0.96 1.   0.94 0.97 0.99 0.82\n",
      " 0.91 0.95 0.97 0.64 0.88 0.55 0.68 0.82 0.96 0.92 0.68 0.83 0.98 0.92\n",
      " 1.   0.83 0.92 0.69 0.94 1.   1.   1.   0.95 0.87 1.   0.6  0.91 0.94\n",
      " 0.83 1.   0.96 1.   0.99 1.   0.99 1.   0.99 0.48 1.   0.97 0.76 0.96\n",
      " 0.79 0.98 0.98 1.   1.   0.94 0.99 1.   1.   0.98 0.95 0.95 1.   0.97\n",
      " 0.69 1.   1.   0.98 0.67 1.   0.97 0.79 1.   0.98 0.95 0.62 0.89 1.\n",
      " 0.95 0.99 0.99 0.94 0.97 1.   0.94 1.   0.97 0.8  0.61 1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.48 0.95 0.97 0.78 0.98 0.96 0.85 1.   1.   0.95 1.   0.79 0.97 0.92\n",
      " 0.75 0.77 0.93 0.99 0.9  0.98 0.93 0.98 1.   0.89 1.   0.99 0.96 0.95\n",
      " 1.   0.98 0.68 0.94 0.98 0.96 0.63 0.82 1.   0.83 0.87 0.93 0.96 0.93\n",
      " 0.84 0.98 0.98 0.94 0.96 0.77 0.94 1.   0.99 1.   1.   1.   1.   0.74\n",
      " 1.   0.99 0.99 0.96 0.81 0.99 1.   0.67 0.56 0.83 0.97 0.75 0.94 0.99\n",
      " 0.9  0.97 0.7  0.92 0.93 0.85 0.98 0.89 0.94 0.95 0.98 1.   0.98 0.98\n",
      " 1.   0.74 0.97 0.98 0.97 0.64 0.86 0.83 0.97 0.99 0.88 0.67 0.94 1.\n",
      " 0.98 0.94 0.9  1.   0.97 0.99 1.   0.96 0.74 1.   0.87 0.8  0.8  0.75\n",
      " 0.88 0.96 0.98 0.91 0.88 0.99 0.84 1.   0.99 0.85 0.97 0.95 0.98 0.98\n",
      " 0.93 1.   0.46 0.83 0.94 0.99 0.88 0.98 0.81 0.76 0.82 0.99 0.89 0.95\n",
      " 0.91 0.82 0.55 0.95 0.84 0.98 0.38 0.9  0.77 0.86 0.96 0.97 0.58 0.96\n",
      " 0.76 0.85 0.99 0.9  0.95 0.92 1.   0.98 0.98 0.87 0.86 0.92]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   1.   0.53 0.69 0.9  0.74 0.88 0.93 0.95 0.94 0.89 0.98 1.   0.91\n",
      " 1.   1.   0.92 0.94 1.   0.99 1.   0.87 1.   0.94 0.51 0.84 0.94 0.79\n",
      " 0.96 0.99 0.84 0.91 0.9  0.88 1.   0.99 0.97 0.95 0.95 0.97 0.92 1.\n",
      " 1.   0.95 0.77 0.97 0.92 0.99 0.99 1.   0.57 0.93 0.97 0.96 0.94 0.93\n",
      " 0.95 0.93 0.65 0.97 0.97 1.   0.78 1.   0.94 0.99 0.91 0.98 0.86 0.97\n",
      " 0.88 0.79 0.91 0.93 0.99 0.92 0.98 0.87 0.98 0.99 0.76 1.   0.93 0.85\n",
      " 0.93 0.87 0.94 1.   0.98 0.95 0.94 0.95 1.   0.67 0.98 0.92 0.99 0.98\n",
      " 0.98 0.93 0.64 1.   1.   0.92 0.91 0.88 0.96 1.   0.62 0.96 0.84 0.92\n",
      " 0.8  0.98 0.95 1.   1.   1.   0.98 0.8  0.97 0.89 1.   0.71 0.91 1.\n",
      " 1.   0.98 0.93 0.86 0.9  0.73 0.78 0.85 0.96 0.87 1.   0.95 0.93 0.99\n",
      " 1.   0.93 0.97 0.92 1.   0.98 0.9  0.98 1.   0.97 0.81 0.74 0.94 0.98\n",
      " 1.   0.83 0.99 0.94 0.98 0.81 0.97 0.92 0.75 0.97 0.93]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.84 0.92 0.73 0.96 0.92 0.85 0.93 0.93 0.82 0.69 0.93 1.   0.94 1.\n",
      " 1.   0.9  0.64 0.94 1.   0.96 0.82 1.   1.   0.88 0.85 0.89 1.   0.83\n",
      " 0.56 0.94 0.87 0.84 0.89 0.67 1.   0.97 0.72 1.   0.72 0.95 0.76 0.94\n",
      " 0.93 0.95 0.87 1.   0.93 0.84 0.83 1.   0.56 0.88 0.86 1.   0.96 0.71\n",
      " 1.   0.98 0.76 0.97 0.87 0.84 0.97 0.88 0.89 1.   0.92 0.91 0.83 0.95\n",
      " 0.92 0.97 0.96 0.77 0.91 0.93 0.99 0.99 0.98 0.61 1.   0.84 0.98 0.69\n",
      " 0.99 0.89 0.92 0.9  0.89 0.96 0.85 1.   1.   0.96 0.94 0.85 0.94 0.99\n",
      " 0.98 0.9  0.98 0.94 0.86 0.97 0.97 0.66 0.93 0.94 0.65 0.82 0.96 0.84\n",
      " 0.96 1.   0.81 0.91 0.35 0.93 0.83 0.82 1.   0.97 0.92 0.93 0.94 0.65\n",
      " 0.99 0.91 0.99 0.87 0.64 0.94 0.97 0.72 0.9  1.   0.86 0.67 0.95 0.86\n",
      " 1.   0.93 0.99 1.   0.76 0.92 0.52 0.98 0.78 0.96 0.94 0.95 0.91 1.\n",
      " 1.   0.97 0.89 0.89 1.   0.6  0.36 0.97 0.84 0.82 0.88]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6  65   7 750]\n",
      "공통 \n",
      "AUC : 0.6509200513517033\n",
      "AUPRC : 0.9462396908440116\n",
      "Optimized precision : 0.006321771630795837\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9130434782608695\n",
      "Precision(pb) : 0.931655097212988\n",
      "Recall(pb) : 0.8277992969572129\n",
      "F1 score(pb) : 0.8766620345885601\n",
      "MCC : 0.1695082855427888\n",
      "G-mean : 0.2893537684038104\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  6  65]\n",
      " [  7 750]]\n",
      "Accuracy : 0.9130434782608695\n",
      "Precision : 0.9202453987730062\n",
      "Recall : 0.9907529722589168\n",
      "F1 score : 0.9541984732824428\n",
      "MCC : 0.18614482184946052\n",
      "G-mean : 0.2893537684038104\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n",
      "[1.   0.98 0.59 0.91 0.97 0.91 0.7  0.84 0.96 1.   0.97 0.86 0.92 0.88\n",
      " 0.95 1.   0.88 0.82 1.   0.95 1.   0.75 0.96 0.98 0.98 0.99 0.94 1.\n",
      " 0.97 1.   0.89 1.   0.96 0.84 0.78 1.   1.   0.96 0.95 0.93 0.93 0.8\n",
      " 1.   0.86 0.93 1.   0.42 0.74 0.79 0.97 0.96 0.62 0.97 0.93 0.95 0.97\n",
      " 1.   0.85 0.97 0.93 0.89 0.64 0.9  0.91 0.85 0.96 0.86 0.99 0.82 0.95\n",
      " 0.97 0.73 0.82 0.93 0.93 0.88 0.93 0.9  0.91 0.83 1.   0.66 0.72 0.69\n",
      " 0.56 0.94 0.98 0.93 0.99 0.74 0.89 0.97 0.98 0.98 1.   0.93 0.95 1.\n",
      " 0.97 0.95 0.92 0.88 0.96 0.98 0.91 0.99 0.91 0.85 0.92 0.78 0.88 0.53\n",
      " 0.94 0.95 0.78 0.78 0.97 0.92 0.74 0.89 0.91 0.73 0.99 0.95 0.97 1.\n",
      " 0.93 0.86 0.94 1.   1.   0.68 1.   0.97 0.99 0.95 0.99 0.85 0.95 0.73\n",
      " 0.94 0.99 1.   0.99 0.95 0.98 0.98 0.91 0.91 0.92 0.26 0.94 0.82 0.99\n",
      " 0.95 0.84 1.   0.99 1.   0.85 1.   1.   0.61 0.32 0.92 0.93]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.99 1.   0.83 0.88 0.9  1.   0.85 1.   0.98 0.88 1.   0.45 0.97 0.58\n",
      " 0.94 0.88 1.   0.85 1.   0.98 1.   0.99 0.71 0.97 0.89 0.95 0.95 0.64\n",
      " 0.89 0.93 1.   0.96 0.94 0.76 0.87 0.86 0.68 0.56 0.92 1.   0.95 0.99\n",
      " 0.98 0.85 0.97 1.   0.94 1.   1.   0.95 0.96 0.99 0.77 0.97 1.   0.96\n",
      " 0.98 0.31 0.99 0.96 0.99 0.65 0.79 0.96 0.9  0.98 0.91 0.76 0.37 1.\n",
      " 0.97 0.8  0.94 0.69 0.98 0.89 0.78 0.86 0.98 0.95 0.89 0.94 0.97 0.92\n",
      " 0.89 0.94 0.95 0.76 0.89 0.65 0.7  0.85 0.95 0.96 0.65 0.84 0.96 0.92\n",
      " 1.   0.97 0.96 0.8  1.   0.98 1.   0.99 0.98 0.9  0.98 0.66 0.89 0.93\n",
      " 0.83 1.   0.98 1.   1.   1.   1.   0.96 0.95 0.62 0.99 0.99 0.79 0.94\n",
      " 0.73 0.99 0.97 1.   0.99 0.96 0.99 0.99 0.99 0.98 0.98 0.92 0.98 0.97\n",
      " 0.58 0.97 0.98 0.98 0.64 1.   0.95 0.74 1.   0.96 0.96 0.58 0.91 0.99\n",
      " 0.9  0.98 1.   0.95 0.95 1.   0.96 0.99 0.96 0.81 0.6  1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.51 0.96 0.96 0.77 1.   0.97 0.86 1.   1.   0.99 1.   0.79 0.99 0.93\n",
      " 0.76 0.71 0.89 1.   0.91 0.96 0.98 0.94 1.   0.85 1.   1.   0.99 0.93\n",
      " 0.97 0.97 0.68 0.96 0.99 0.9  0.63 0.84 0.99 0.81 0.92 0.95 0.97 0.98\n",
      " 0.87 1.   0.99 0.94 0.97 0.82 0.94 1.   1.   0.95 1.   0.98 1.   0.81\n",
      " 1.   1.   0.98 0.96 0.85 0.98 1.   0.7  0.6  0.84 0.96 0.85 0.93 1.\n",
      " 0.95 0.98 0.77 0.83 0.96 0.83 0.98 0.84 0.94 0.94 1.   0.98 0.99 0.98\n",
      " 1.   0.8  0.97 0.99 0.98 0.59 0.9  0.8  0.94 1.   0.84 0.66 0.93 1.\n",
      " 1.   0.95 0.91 0.99 1.   0.97 1.   0.96 0.82 0.98 0.87 0.78 0.9  0.72\n",
      " 0.85 0.97 1.   0.93 0.89 0.98 0.81 1.   1.   0.88 0.98 0.94 0.98 0.98\n",
      " 0.94 1.   0.46 0.83 0.93 0.92 0.93 0.96 0.84 0.81 0.81 0.99 0.95 0.96\n",
      " 0.91 0.84 0.53 0.99 0.81 1.   0.35 0.88 0.77 0.84 0.98 0.96 0.56 0.94\n",
      " 0.83 0.86 0.98 0.95 0.96 0.94 0.99 0.98 0.99 0.91 0.82 0.94]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   1.   0.42 0.69 0.91 0.86 0.85 0.88 0.91 0.92 0.87 0.98 1.   0.93\n",
      " 0.99 1.   0.89 0.98 0.99 1.   0.99 0.82 1.   0.93 0.48 0.85 0.95 0.81\n",
      " 0.96 1.   0.75 0.87 0.92 0.87 1.   0.99 0.95 0.93 0.93 0.97 0.92 0.96\n",
      " 1.   0.98 0.82 0.93 0.97 0.99 0.99 1.   0.55 0.9  0.9  0.92 0.98 0.92\n",
      " 0.95 0.84 0.72 0.93 0.95 0.98 0.73 1.   0.94 0.97 0.91 0.99 0.82 0.94\n",
      " 0.88 0.75 0.96 0.98 1.   0.91 0.99 0.9  0.99 0.97 0.84 1.   0.94 0.83\n",
      " 0.84 0.87 0.93 0.98 0.98 0.97 0.92 1.   1.   0.67 0.99 0.96 0.98 1.\n",
      " 1.   0.96 0.68 1.   1.   0.93 0.93 0.9  0.94 0.99 0.64 0.95 0.83 0.88\n",
      " 0.75 0.99 0.95 0.99 0.98 0.98 0.99 0.75 0.96 0.97 1.   0.71 0.96 1.\n",
      " 0.99 0.96 0.93 0.87 0.93 0.78 0.81 0.86 0.93 0.92 1.   0.88 0.93 0.96\n",
      " 1.   0.9  0.93 0.92 0.99 0.98 0.95 1.   1.   0.94 0.77 0.81 0.95 0.98\n",
      " 1.   0.75 1.   0.97 0.99 0.81 0.99 0.94 0.75 0.97 0.95]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.85 0.95 0.8  0.96 0.96 0.89 0.88 0.93 0.82 0.73 0.89 0.98 0.93 1.\n",
      " 1.   0.9  0.76 0.92 1.   0.96 0.76 1.   1.   0.81 0.86 0.94 1.   0.82\n",
      " 0.52 0.95 0.93 0.9  0.86 0.77 0.99 0.95 0.79 1.   0.75 0.97 0.81 0.95\n",
      " 0.9  0.97 0.82 0.99 0.96 0.83 0.87 0.99 0.58 0.85 0.91 1.   1.   0.8\n",
      " 1.   0.98 0.78 0.93 0.85 0.76 0.98 0.94 0.87 0.99 0.89 0.95 0.75 0.95\n",
      " 0.92 0.96 0.94 0.88 0.85 0.92 0.99 0.99 0.95 0.45 1.   0.86 0.98 0.72\n",
      " 0.97 0.86 0.9  0.95 0.72 0.93 0.94 1.   1.   0.96 0.91 0.92 0.94 0.97\n",
      " 0.96 0.93 0.97 0.94 0.93 0.91 0.97 0.8  0.92 0.92 0.7  0.88 0.97 0.88\n",
      " 0.96 1.   0.74 0.94 0.39 0.95 0.91 0.87 1.   0.96 0.9  0.95 0.96 0.64\n",
      " 0.98 0.91 0.96 0.87 0.61 0.86 0.97 0.85 0.93 0.98 0.87 0.74 0.94 0.89\n",
      " 0.96 0.92 0.99 0.97 0.84 0.88 0.57 0.97 0.69 0.92 0.97 0.98 0.91 0.99\n",
      " 0.96 0.95 0.92 0.84 1.   0.71 0.4  0.97 0.75 0.91 0.79]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7  64   6 751]\n",
      "공통 \n",
      "AUC : 0.6619625281411057\n",
      "AUPRC : 0.9493125772634221\n",
      "Optimized precision : 0.020149633189773808\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9154589371980676\n",
      "Precision(pb) : 0.9326305944210043\n",
      "Recall(pb) : 0.8321877169021337\n",
      "F1 score(pb) : 0.8795508524641404\n",
      "MCC : 0.204206149678618\n",
      "G-mean : 0.3127460796424784\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  7  64]\n",
      " [  6 751]]\n",
      "Accuracy : 0.9154589371980676\n",
      "Precision : 0.9214723926380368\n",
      "Recall : 0.9920739762219286\n",
      "F1 score : 0.955470737913486\n",
      "MCC : 0.2178673860412754\n",
      "G-mean : 0.31274607964247836\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n",
      "[1.   0.94 0.74 0.93 0.98 0.94 0.74 0.86 0.95 1.   0.98 0.81 0.92 0.83\n",
      " 0.99 0.98 0.93 0.81 1.   0.97 1.   0.85 0.98 0.93 0.98 1.   0.95 1.\n",
      " 0.97 1.   0.82 0.98 0.93 0.87 0.84 0.99 0.99 0.99 0.93 0.91 0.93 0.68\n",
      " 0.99 0.9  0.9  1.   0.45 0.78 0.87 0.95 0.94 0.64 0.97 0.91 0.93 0.97\n",
      " 1.   0.78 0.92 0.91 0.92 0.7  0.83 0.96 0.83 1.   0.89 0.99 0.83 0.94\n",
      " 0.96 0.81 0.84 0.97 0.98 0.89 0.98 0.91 0.96 0.81 1.   0.53 0.75 0.79\n",
      " 0.7  0.95 0.98 0.95 1.   0.76 0.82 0.94 0.97 1.   0.99 0.88 0.97 1.\n",
      " 0.95 0.93 0.92 0.91 0.91 0.97 0.85 0.98 0.86 0.93 0.9  0.81 0.91 0.56\n",
      " 0.89 0.93 0.83 0.81 0.98 0.94 0.84 0.87 0.83 0.71 1.   0.97 0.98 0.99\n",
      " 0.89 0.83 0.95 1.   1.   0.69 1.   0.96 0.98 0.96 0.98 0.82 0.99 0.69\n",
      " 0.94 0.96 1.   0.99 0.92 1.   1.   0.81 0.89 0.98 0.24 0.96 0.77 0.99\n",
      " 0.94 0.79 0.98 0.95 0.99 0.88 1.   1.   0.61 0.29 0.94 0.92]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.99 1.   0.79 0.86 0.88 1.   0.8  1.   0.97 0.92 1.   0.62 0.92 0.56\n",
      " 0.95 0.92 1.   0.88 1.   0.99 0.98 0.99 0.69 0.98 0.87 0.92 1.   0.61\n",
      " 0.89 0.93 0.99 0.98 0.94 0.71 0.9  0.9  0.67 0.59 0.93 1.   0.86 1.\n",
      " 0.96 0.91 0.95 1.   0.91 1.   0.99 0.98 0.95 0.99 0.82 0.99 0.99 0.96\n",
      " 0.97 0.26 0.98 0.97 0.98 0.54 0.85 0.96 0.86 0.97 0.9  0.86 0.38 1.\n",
      " 0.95 0.75 0.98 0.69 0.98 0.92 0.66 0.7  0.97 0.97 0.87 0.99 0.97 0.82\n",
      " 0.86 0.99 0.98 0.76 0.95 0.66 0.57 0.82 0.95 0.95 0.66 0.84 0.98 0.88\n",
      " 0.99 0.88 0.97 0.7  0.97 0.94 1.   0.99 0.92 0.89 0.96 0.57 0.8  0.95\n",
      " 0.85 1.   0.94 0.99 1.   1.   0.99 0.96 1.   0.61 0.99 0.97 0.74 0.95\n",
      " 0.79 0.98 0.95 1.   1.   0.91 0.99 0.99 0.99 0.99 0.95 0.94 1.   0.97\n",
      " 0.57 0.96 0.99 0.98 0.57 1.   0.98 0.74 1.   0.98 0.99 0.51 0.9  0.99\n",
      " 0.95 0.97 0.97 0.99 0.94 1.   0.95 1.   0.97 0.93 0.61 1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.52 0.96 0.98 0.68 0.99 0.92 0.83 0.99 0.96 0.97 0.98 0.85 0.98 0.92\n",
      " 0.72 0.68 0.95 1.   0.94 0.99 0.95 0.99 0.99 0.88 1.   0.99 0.97 0.95\n",
      " 0.99 0.98 0.71 0.97 0.98 0.96 0.62 0.84 1.   0.82 0.89 0.93 0.98 0.92\n",
      " 0.88 1.   0.99 0.93 0.95 0.76 0.96 1.   0.99 0.97 0.99 0.98 0.97 0.85\n",
      " 1.   1.   0.99 0.98 0.87 0.99 1.   0.67 0.66 0.88 0.96 0.91 0.94 0.99\n",
      " 0.86 0.99 0.75 0.89 0.95 0.81 0.98 0.86 0.95 0.95 0.99 0.99 0.99 0.97\n",
      " 1.   0.8  0.97 0.98 0.98 0.65 0.93 0.82 0.87 1.   0.8  0.61 0.91 1.\n",
      " 1.   0.93 0.93 1.   0.98 1.   0.99 0.98 0.79 0.98 0.88 0.81 0.87 0.79\n",
      " 0.9  0.99 1.   0.84 0.89 0.97 0.86 0.99 0.99 0.85 0.97 0.94 0.95 1.\n",
      " 0.88 1.   0.47 0.8  0.97 0.95 0.93 1.   0.82 0.85 0.89 0.99 0.94 0.95\n",
      " 0.94 0.79 0.47 0.95 0.86 0.99 0.32 0.91 0.81 0.9  1.   0.96 0.44 0.95\n",
      " 0.81 0.89 0.98 0.91 0.94 0.88 1.   0.94 0.98 0.82 0.89 0.96]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   0.99 0.56 0.7  0.86 0.76 0.88 0.89 0.88 0.98 0.86 0.98 1.   0.93\n",
      " 0.99 1.   0.91 0.96 1.   0.99 1.   0.88 1.   0.92 0.4  0.9  0.93 0.74\n",
      " 0.96 0.97 0.92 0.9  0.93 0.84 1.   1.   0.95 0.97 0.92 0.99 0.95 0.96\n",
      " 0.99 0.92 0.71 0.97 0.92 1.   0.97 1.   0.64 0.91 0.94 0.94 0.95 0.94\n",
      " 0.94 0.9  0.66 0.96 0.93 0.99 0.83 1.   0.99 0.99 0.91 0.99 0.86 0.93\n",
      " 0.9  0.78 0.88 0.95 0.98 0.91 0.99 0.82 0.99 0.99 0.74 1.   0.89 0.82\n",
      " 0.84 0.94 0.94 0.97 0.98 0.93 0.9  0.95 1.   0.7  0.96 0.97 0.97 0.99\n",
      " 0.95 0.97 0.72 1.   1.   0.95 0.91 0.91 0.94 1.   0.69 0.99 0.81 0.85\n",
      " 0.71 0.98 0.95 1.   1.   0.98 0.97 0.76 0.95 0.96 1.   0.75 0.94 1.\n",
      " 1.   0.93 0.84 0.89 0.91 0.68 0.74 0.84 0.97 0.93 1.   0.93 0.94 0.93\n",
      " 0.99 0.9  0.97 0.87 1.   0.98 0.93 1.   1.   0.96 0.77 0.72 0.92 0.99\n",
      " 1.   0.78 1.   0.97 1.   0.81 0.99 0.98 0.77 0.97 0.91]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.88 0.94 0.77 0.94 0.92 0.87 0.89 0.94 0.79 0.72 0.86 0.98 0.98 1.\n",
      " 1.   0.95 0.66 0.94 0.98 0.99 0.83 0.98 0.99 0.8  0.82 0.86 1.   0.83\n",
      " 0.5  0.97 0.9  0.9  0.78 0.7  1.   0.94 0.81 1.   0.77 0.96 0.8  0.97\n",
      " 0.88 0.94 0.87 0.99 0.98 0.89 0.85 1.   0.61 0.85 0.93 0.99 0.98 0.83\n",
      " 1.   0.99 0.77 0.92 0.79 0.81 0.98 0.92 0.92 0.99 0.89 0.97 0.82 0.94\n",
      " 0.94 0.95 0.97 0.73 0.92 0.96 0.99 0.98 0.95 0.56 1.   0.88 1.   0.75\n",
      " 0.96 0.89 0.93 0.94 0.82 0.92 0.9  1.   1.   0.95 0.96 0.9  0.93 1.\n",
      " 0.96 0.93 0.99 0.92 0.89 0.95 0.99 0.75 0.88 0.92 0.7  0.92 0.98 0.87\n",
      " 0.99 0.99 0.78 0.9  0.41 0.9  0.91 0.84 1.   0.97 0.91 0.94 0.97 0.66\n",
      " 0.99 0.93 0.98 0.88 0.58 0.89 0.99 0.76 0.88 1.   0.9  0.68 0.9  0.84\n",
      " 0.96 0.91 1.   0.95 0.82 0.91 0.54 0.97 0.72 0.92 0.94 0.94 0.94 1.\n",
      " 0.97 0.97 0.93 0.88 1.   0.64 0.32 1.   0.81 0.85 0.81]\n",
      "[  6  65   6 751]\n",
      "공통 \n",
      "AUC : 0.6634230747762665\n",
      "AUPRC : 0.9455051021738019\n",
      "Optimized precision : 0.006208797708997964\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9142512077294686\n",
      "Precision(pb) : 0.9327933439892464\n",
      "Recall(pb) : 0.8253261952768315\n",
      "F1 score(pb) : 0.8757752409663485\n",
      "MCC : 0.17941656991522875\n",
      "G-mean : 0.2895466066580737\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  6  65]\n",
      " [  6 751]]\n",
      "Accuracy : 0.9142512077294686\n",
      "Precision : 0.9203431372549019\n",
      "Recall : 0.9920739762219286\n",
      "F1 score : 0.9548633184996821\n",
      "MCC : 0.19384487036273623\n",
      "G-mean : 0.2895466066580737\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.   0.99 0.71 0.88 0.97 0.82 0.7  0.88 0.93 1.   1.   0.87 0.88 0.9\n",
      " 0.98 1.   0.91 0.91 1.   0.94 1.   0.86 0.98 0.98 1.   1.   0.95 0.99\n",
      " 0.99 0.99 0.81 0.99 0.92 0.88 0.86 1.   1.   0.92 0.93 0.97 0.97 0.8\n",
      " 0.99 0.91 0.92 1.   0.39 0.73 0.83 0.95 0.95 0.71 0.99 0.94 0.92 0.96\n",
      " 0.99 0.86 0.91 0.93 0.94 0.69 0.89 0.95 0.75 0.98 0.87 0.98 0.78 1.\n",
      " 0.99 0.77 0.89 0.95 0.96 0.94 0.96 0.93 0.96 0.9  1.   0.46 0.79 0.73\n",
      " 0.64 0.96 1.   0.92 0.96 0.77 0.89 0.97 0.97 0.98 1.   0.87 0.95 1.\n",
      " 0.99 0.88 0.89 0.9  0.93 0.97 0.92 0.96 0.89 0.9  0.93 0.7  0.93 0.43\n",
      " 0.91 0.92 0.81 0.78 0.99 0.93 0.74 0.74 0.81 0.59 0.99 1.   0.95 1.\n",
      " 0.89 0.86 0.93 1.   1.   0.63 1.   0.97 0.98 0.93 1.   0.9  0.93 0.65\n",
      " 0.89 0.97 1.   0.99 0.89 0.98 0.98 0.89 0.88 0.99 0.19 0.95 0.85 0.97\n",
      " 0.98 0.87 1.   0.98 1.   0.85 1.   1.   0.67 0.25 0.9  0.97]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.98 1.   0.88 0.82 0.9  1.   0.77 1.   0.99 0.91 1.   0.57 0.94 0.67\n",
      " 0.97 0.93 1.   0.81 1.   1.   0.97 1.   0.76 0.98 0.84 0.96 0.99 0.57\n",
      " 0.91 0.91 1.   0.95 0.96 0.81 0.94 0.84 0.69 0.66 0.98 1.   0.84 0.98\n",
      " 0.99 0.92 0.98 1.   0.98 1.   0.98 0.99 0.99 0.99 0.75 0.98 1.   0.98\n",
      " 1.   0.26 0.99 0.93 1.   0.63 0.83 0.98 0.88 0.97 0.9  0.86 0.5  1.\n",
      " 0.96 0.76 0.96 0.8  0.96 0.9  0.67 0.78 0.97 0.98 0.9  0.99 0.97 0.85\n",
      " 0.86 0.97 0.96 0.64 0.93 0.67 0.74 0.82 0.97 0.97 0.68 0.89 0.99 0.96\n",
      " 0.99 0.89 0.93 0.74 0.95 0.97 1.   1.   0.96 0.9  0.99 0.74 0.86 0.95\n",
      " 0.95 1.   0.95 1.   1.   0.99 0.99 0.97 1.   0.54 0.96 0.98 0.77 0.96\n",
      " 0.84 0.99 0.97 1.   1.   0.94 0.99 0.97 0.99 0.98 0.96 0.95 1.   1.\n",
      " 0.67 0.97 1.   1.   0.69 1.   0.97 0.83 1.   0.99 0.97 0.62 0.92 0.99\n",
      " 0.92 0.99 1.   0.96 1.   1.   0.94 0.99 0.97 0.82 0.6  1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.44 0.91 0.98 0.74 0.98 0.96 0.75 1.   1.   0.96 0.98 0.84 0.98 0.96\n",
      " 0.7  0.66 0.94 0.99 0.92 1.   0.98 0.99 0.97 0.87 0.99 1.   1.   0.99\n",
      " 1.   0.96 0.72 0.97 0.98 0.96 0.55 0.87 0.99 0.76 0.9  0.9  0.99 0.96\n",
      " 0.84 0.98 0.99 0.92 0.99 0.72 0.94 1.   1.   0.96 0.97 0.99 1.   0.76\n",
      " 1.   0.99 1.   0.98 0.83 0.99 0.99 0.73 0.59 0.83 0.94 0.81 0.89 0.99\n",
      " 0.89 1.   0.73 0.9  0.96 0.78 0.99 0.8  0.99 0.99 1.   0.98 0.98 1.\n",
      " 0.99 0.76 0.98 0.99 0.97 0.56 0.91 0.79 0.94 1.   0.85 0.69 0.9  1.\n",
      " 0.99 0.95 0.89 1.   0.99 1.   1.   0.97 0.89 0.96 0.89 0.75 0.82 0.75\n",
      " 0.88 0.96 1.   0.88 0.84 0.98 0.89 1.   1.   0.84 0.98 0.98 0.98 0.97\n",
      " 0.91 1.   0.44 0.8  0.96 0.95 0.96 0.96 0.88 0.77 0.86 0.99 0.93 0.91\n",
      " 0.95 0.84 0.57 0.95 0.85 1.   0.35 0.87 0.82 0.86 1.   0.97 0.6  0.97\n",
      " 0.87 0.81 1.   0.93 0.95 0.99 0.99 0.99 0.97 0.86 0.86 0.94]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   1.   0.56 0.61 0.88 0.8  0.89 0.93 0.94 0.94 0.85 0.97 1.   0.9\n",
      " 1.   1.   0.92 1.   1.   0.98 1.   0.82 0.99 0.96 0.47 0.89 0.95 0.8\n",
      " 0.95 0.98 0.79 0.87 0.84 0.84 1.   0.99 0.98 0.97 0.95 1.   0.9  0.97\n",
      " 0.98 0.95 0.72 1.   0.92 1.   0.98 1.   0.67 0.9  0.92 0.96 0.93 0.94\n",
      " 0.96 0.82 0.68 0.92 0.96 1.   0.86 0.99 0.98 0.99 0.96 0.99 0.9  0.96\n",
      " 0.88 0.77 0.95 1.   0.98 0.93 0.98 0.86 0.97 0.99 0.81 1.   0.97 0.87\n",
      " 0.91 0.86 0.96 1.   0.97 0.94 0.88 0.99 0.99 0.77 0.99 0.98 0.98 0.99\n",
      " 0.98 0.98 0.71 0.99 1.   0.89 0.89 0.89 0.93 0.98 0.68 0.94 0.82 0.9\n",
      " 0.68 0.99 0.93 0.98 1.   0.99 0.99 0.86 0.99 0.95 1.   0.75 0.99 1.\n",
      " 0.98 0.91 0.94 0.87 0.9  0.7  0.73 0.87 0.99 0.83 0.99 0.93 0.89 0.94\n",
      " 1.   0.88 0.96 0.89 1.   0.97 0.95 1.   1.   0.93 0.79 0.76 0.96 0.98\n",
      " 1.   0.77 1.   0.96 0.98 0.83 0.95 0.94 0.7  0.96 0.92]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.92 0.94 0.8  0.91 0.97 0.91 0.91 0.95 0.72 0.72 0.9  1.   0.96 0.99\n",
      " 1.   0.86 0.61 0.94 0.99 0.98 0.75 0.99 1.   0.78 0.8  0.87 1.   0.81\n",
      " 0.52 0.96 0.89 0.9  0.9  0.68 1.   0.99 0.81 1.   0.67 0.98 0.7  0.95\n",
      " 0.9  0.95 0.82 0.99 0.98 0.82 0.84 0.96 0.71 0.84 0.89 1.   0.95 0.76\n",
      " 1.   0.98 0.73 0.9  0.93 0.79 0.94 0.94 0.85 1.   0.88 0.93 0.79 0.96\n",
      " 0.9  1.   0.93 0.7  0.8  0.9  1.   0.99 0.94 0.51 1.   0.85 0.97 0.7\n",
      " 1.   0.89 0.92 0.98 0.82 0.95 0.92 1.   1.   0.96 0.89 0.93 0.96 1.\n",
      " 0.98 0.89 1.   0.98 0.92 0.95 0.96 0.78 0.91 0.95 0.58 0.85 0.94 0.93\n",
      " 0.97 0.99 0.77 0.92 0.38 0.97 0.93 0.88 1.   0.99 0.92 0.92 0.93 0.75\n",
      " 0.96 0.92 0.96 0.83 0.69 0.93 0.98 0.85 0.91 1.   0.9  0.61 0.93 0.8\n",
      " 0.99 0.96 0.98 0.94 0.83 0.86 0.52 0.98 0.83 0.96 0.98 1.   0.96 1.\n",
      " 0.97 0.97 0.85 0.88 1.   0.68 0.44 0.98 0.81 0.9  0.93]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6  65   6 751]\n",
      "공통 \n",
      "AUC : 0.6655255177033136\n",
      "AUPRC : 0.94889439538073\n",
      "Optimized precision : 0.006208797708997964\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9142512077294686\n",
      "Precision(pb) : 0.9318647795068122\n",
      "Recall(pb) : 0.8365918097754292\n",
      "F1 score(pb) : 0.8816619498361512\n",
      "MCC : 0.17941656991522875\n",
      "G-mean : 0.2895466066580737\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  6  65]\n",
      " [  6 751]]\n",
      "Accuracy : 0.9142512077294686\n",
      "Precision : 0.9203431372549019\n",
      "Recall : 0.9920739762219286\n",
      "F1 score : 0.9548633184996821\n",
      "MCC : 0.19384487036273623\n",
      "G-mean : 0.2895466066580737\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n",
      "[1.   0.97 0.66 0.94 0.98 0.88 0.72 0.86 0.93 1.   0.99 0.82 0.85 0.89\n",
      " 0.98 1.   0.9  0.88 1.   0.92 1.   0.85 0.96 0.97 0.98 1.   0.95 1.\n",
      " 0.99 1.   0.86 0.99 0.95 0.84 0.83 0.99 0.99 0.97 0.96 0.93 0.9  0.79\n",
      " 0.98 0.91 0.95 1.   0.4  0.75 0.78 0.98 0.93 0.61 0.98 0.94 0.96 0.99\n",
      " 0.99 0.82 0.95 0.89 0.9  0.74 0.9  0.93 0.72 0.92 0.9  0.98 0.85 0.98\n",
      " 0.97 0.81 0.85 0.97 0.95 0.87 0.95 0.89 0.95 0.84 0.99 0.57 0.71 0.75\n",
      " 0.62 0.89 1.   0.95 0.99 0.83 0.78 0.95 0.95 0.98 0.99 0.86 0.88 1.\n",
      " 0.99 0.92 0.92 0.89 0.98 0.94 0.85 0.95 0.88 0.9  0.93 0.82 0.9  0.48\n",
      " 0.93 0.94 0.82 0.78 0.99 0.93 0.81 0.87 0.89 0.73 0.98 0.99 0.96 1.\n",
      " 0.96 0.76 0.92 1.   1.   0.78 1.   1.   1.   0.92 1.   0.84 0.93 0.75\n",
      " 0.88 0.95 1.   1.   0.97 0.98 1.   0.92 0.92 0.97 0.16 0.91 0.81 0.99\n",
      " 0.91 0.75 0.99 1.   0.98 0.89 1.   1.   0.51 0.39 0.98 0.98]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.98 1.   0.78 0.79 0.93 1.   0.78 1.   1.   0.95 0.99 0.64 0.93 0.62\n",
      " 0.98 0.89 1.   0.9  1.   0.99 0.96 1.   0.75 1.   0.8  1.   0.99 0.71\n",
      " 0.93 0.95 1.   0.96 0.93 0.74 0.87 0.91 0.74 0.63 0.87 1.   0.94 0.99\n",
      " 0.95 0.92 0.98 1.   0.99 1.   0.99 0.97 0.97 1.   0.79 0.99 1.   0.96\n",
      " 0.98 0.22 0.99 0.95 0.98 0.65 0.84 0.96 0.9  0.97 0.96 0.77 0.52 1.\n",
      " 0.96 0.75 0.98 0.79 0.98 0.94 0.73 0.82 0.99 0.99 0.9  0.97 0.99 0.81\n",
      " 0.89 0.95 0.95 0.65 0.88 0.63 0.68 0.79 0.89 0.91 0.69 0.84 0.98 0.91\n",
      " 1.   0.82 0.92 0.69 0.98 0.98 0.99 1.   0.92 0.89 1.   0.63 0.88 0.96\n",
      " 0.84 1.   0.95 1.   1.   0.98 1.   0.98 0.99 0.59 0.98 0.98 0.82 0.92\n",
      " 0.76 0.99 0.96 1.   1.   0.95 0.96 0.99 0.95 0.99 0.98 0.96 0.99 0.98\n",
      " 0.57 0.98 1.   0.99 0.63 0.99 1.   0.69 1.   0.96 0.98 0.48 0.92 0.99\n",
      " 0.93 0.98 0.99 0.96 0.94 1.   0.92 0.99 0.95 0.87 0.62 1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.49 0.94 0.96 0.73 0.99 0.95 0.85 1.   0.99 0.95 0.99 0.86 1.   0.96\n",
      " 0.76 0.7  0.9  0.98 0.87 1.   0.96 0.97 1.   0.83 1.   1.   0.99 0.96\n",
      " 1.   0.97 0.74 0.96 0.99 0.95 0.53 0.85 0.99 0.85 0.91 0.91 0.95 0.91\n",
      " 0.75 1.   0.98 0.89 0.95 0.72 0.96 1.   0.99 0.95 0.99 0.98 1.   0.7\n",
      " 0.98 0.99 1.   0.96 0.81 1.   1.   0.68 0.74 0.86 0.91 0.8  0.89 0.99\n",
      " 0.91 0.96 0.79 0.91 0.97 0.8  0.99 0.91 0.98 0.98 1.   0.98 1.   0.97\n",
      " 1.   0.75 0.98 0.96 0.98 0.54 0.84 0.79 0.88 1.   0.89 0.63 0.9  1.\n",
      " 0.98 0.94 0.92 1.   1.   0.99 0.99 0.97 0.81 0.99 0.9  0.84 0.89 0.79\n",
      " 0.95 0.91 1.   0.89 0.91 0.99 0.81 1.   0.99 0.88 0.99 0.97 0.95 0.98\n",
      " 0.94 1.   0.48 0.86 0.96 0.94 0.96 0.97 0.77 0.79 0.91 1.   0.92 0.88\n",
      " 0.92 0.88 0.55 0.94 0.86 0.96 0.4  0.9  0.91 0.84 0.99 0.98 0.52 0.95\n",
      " 0.83 0.9  0.98 0.96 0.94 0.93 0.99 0.96 0.99 0.89 0.88 0.94]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   1.   0.49 0.68 0.88 0.83 0.82 0.8  0.95 0.94 0.87 0.98 1.   0.92\n",
      " 1.   1.   0.96 0.97 1.   0.99 1.   0.9  1.   0.96 0.5  0.87 0.95 0.76\n",
      " 0.91 0.96 0.82 0.86 0.92 0.83 1.   0.99 0.98 0.93 0.98 0.97 0.95 0.98\n",
      " 0.99 0.94 0.73 0.98 0.91 1.   0.98 1.   0.57 0.91 0.93 0.96 0.98 0.95\n",
      " 0.99 0.86 0.65 0.98 0.96 1.   0.78 1.   0.95 1.   0.93 1.   0.83 0.98\n",
      " 0.86 0.74 0.92 0.96 1.   0.92 0.99 0.84 0.97 0.98 0.82 1.   0.93 0.85\n",
      " 0.89 0.85 0.92 0.98 1.   0.95 0.89 0.96 1.   0.69 0.98 0.91 0.96 0.99\n",
      " 0.98 0.96 0.68 1.   1.   0.91 0.89 0.88 0.91 0.99 0.63 0.98 0.76 0.89\n",
      " 0.76 0.99 0.93 1.   1.   0.98 0.99 0.82 0.96 0.95 1.   0.67 0.97 1.\n",
      " 1.   0.95 0.91 0.92 0.88 0.79 0.77 0.9  0.98 0.85 1.   0.96 0.92 0.95\n",
      " 1.   0.93 0.91 0.92 1.   0.98 0.89 1.   1.   0.95 0.8  0.67 0.96 0.95\n",
      " 1.   0.8  1.   0.97 1.   0.88 0.98 0.94 0.8  1.   0.9 ]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.89 0.97 0.81 0.98 0.95 0.83 0.85 0.96 0.78 0.78 0.87 0.99 0.92 0.99\n",
      " 1.   0.86 0.65 0.94 1.   0.95 0.86 0.99 1.   0.85 0.81 0.91 1.   0.8\n",
      " 0.56 0.97 0.83 0.89 0.86 0.73 1.   0.97 0.74 1.   0.79 0.97 0.79 0.97\n",
      " 0.88 0.89 0.84 1.   1.   0.9  0.87 0.99 0.66 0.85 0.86 1.   0.94 0.76\n",
      " 1.   0.98 0.77 0.94 0.89 0.8  0.99 0.9  0.88 0.98 0.82 0.98 0.78 0.99\n",
      " 0.91 0.96 0.92 0.75 0.85 0.93 1.   0.99 0.97 0.52 1.   0.88 0.97 0.67\n",
      " 0.97 0.86 0.92 0.96 0.85 0.87 0.94 1.   1.   0.9  0.94 0.93 0.92 0.98\n",
      " 0.99 0.89 0.98 0.95 0.87 0.92 0.97 0.81 0.93 0.91 0.78 0.79 0.96 0.94\n",
      " 0.93 0.97 0.87 0.93 0.45 0.91 0.96 0.82 1.   0.99 0.85 0.95 0.96 0.8\n",
      " 0.97 0.92 0.95 0.88 0.59 0.81 0.97 0.73 0.9  1.   0.88 0.68 0.91 0.89\n",
      " 0.97 0.95 0.98 0.98 0.77 0.91 0.6  0.98 0.75 0.96 1.   0.98 0.92 0.98\n",
      " 0.94 0.96 0.92 0.88 0.99 0.67 0.35 0.97 0.77 0.84 0.85]\n",
      "[  6  65   6 751]\n",
      "공통 \n",
      "AUC : 0.656845963495637\n",
      "AUPRC : 0.9472123587659149\n",
      "Optimized precision : 0.006208797708997964\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9142512077294686\n",
      "Precision(pb) : 0.9324747002768509\n",
      "Recall(pb) : 0.8257201789149227\n",
      "F1 score(pb) : 0.8758564655815462\n",
      "MCC : 0.17941656991522875\n",
      "G-mean : 0.2895466066580737\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  6  65]\n",
      " [  6 751]]\n",
      "Accuracy : 0.9142512077294686\n",
      "Precision : 0.9203431372549019\n",
      "Recall : 0.9920739762219286\n",
      "F1 score : 0.9548633184996821\n",
      "MCC : 0.19384487036273623\n",
      "G-mean : 0.2895466066580737\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.   0.98 0.68 0.9  0.96 0.92 0.76 0.87 0.89 1.   0.99 0.87 0.89 0.84\n",
      " 0.98 0.99 0.85 0.87 0.99 0.97 1.   0.82 0.94 0.98 0.99 0.99 0.95 1.\n",
      " 0.96 0.99 0.85 1.   0.92 0.89 0.8  0.99 0.98 0.98 0.92 0.94 0.9  0.8\n",
      " 0.99 0.92 0.9  1.   0.41 0.71 0.88 0.98 0.99 0.63 0.99 0.94 0.98 0.97\n",
      " 0.97 0.85 0.96 0.9  0.91 0.67 0.87 0.92 0.78 0.97 0.94 1.   0.91 0.99\n",
      " 0.97 0.74 0.84 0.93 0.94 0.88 0.94 0.91 0.94 0.82 1.   0.62 0.75 0.79\n",
      " 0.67 0.98 0.99 0.97 0.99 0.83 0.87 0.97 0.99 1.   1.   0.91 0.95 1.\n",
      " 0.95 0.89 0.9  0.89 0.95 0.96 0.91 0.98 0.88 0.92 0.95 0.77 0.89 0.44\n",
      " 0.92 0.94 0.82 0.84 1.   0.93 0.71 0.89 0.88 0.61 0.99 0.98 0.94 0.98\n",
      " 0.91 0.85 0.92 1.   0.99 0.73 1.   0.99 0.99 0.97 0.99 0.8  0.95 0.77\n",
      " 0.92 1.   1.   0.98 0.91 0.97 0.99 0.85 0.91 0.97 0.23 0.95 0.8  0.98\n",
      " 0.96 0.88 0.99 0.97 0.99 0.84 0.99 1.   0.52 0.32 0.91 0.96]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.97 1.   0.8  0.81 0.94 1.   0.8  1.   0.98 0.85 0.97 0.47 0.94 0.51\n",
      " 0.98 0.87 0.99 0.91 1.   0.98 1.   1.   0.68 0.96 0.86 0.97 0.99 0.67\n",
      " 0.9  0.9  1.   0.93 0.94 0.75 0.92 0.9  0.75 0.64 0.9  1.   0.88 0.96\n",
      " 0.97 0.88 0.95 1.   0.96 1.   0.99 0.98 0.97 0.97 0.71 0.99 0.99 0.97\n",
      " 0.99 0.32 1.   1.   0.99 0.61 0.88 0.96 0.82 0.96 0.9  0.7  0.52 1.\n",
      " 0.95 0.67 0.97 0.75 0.98 0.89 0.71 0.74 0.98 0.98 0.88 0.98 1.   0.88\n",
      " 0.9  0.97 0.95 0.79 0.98 0.63 0.67 0.79 0.96 0.94 0.67 0.84 0.99 0.88\n",
      " 1.   0.92 0.98 0.74 0.99 0.96 0.96 0.99 0.96 0.88 1.   0.62 0.9  0.98\n",
      " 0.79 0.99 0.97 1.   0.97 1.   1.   0.96 1.   0.57 1.   0.96 0.77 0.96\n",
      " 0.75 1.   0.94 0.99 1.   0.94 0.96 1.   0.99 0.97 0.94 0.89 0.99 0.98\n",
      " 0.7  0.98 0.99 1.   0.65 1.   0.97 0.8  1.   0.97 0.97 0.57 0.85 0.98\n",
      " 0.94 0.99 1.   0.97 0.98 1.   0.91 0.99 0.98 0.85 0.64 1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.56 0.94 0.96 0.72 1.   0.91 0.83 1.   0.98 0.97 0.98 0.78 0.98 0.93\n",
      " 0.68 0.65 0.89 0.97 0.89 0.96 0.96 0.94 1.   0.93 1.   1.   0.98 0.96\n",
      " 0.98 1.   0.75 0.95 0.98 1.   0.47 0.87 0.99 0.82 0.91 0.94 0.96 0.92\n",
      " 0.85 0.97 0.99 0.9  0.96 0.77 0.96 1.   0.98 0.95 0.97 1.   1.   0.76\n",
      " 1.   1.   0.99 0.95 0.88 0.99 1.   0.7  0.59 0.86 0.95 0.79 0.92 1.\n",
      " 0.86 0.93 0.83 0.87 0.98 0.78 0.98 0.84 0.98 0.97 0.99 0.99 0.98 0.95\n",
      " 1.   0.73 0.95 0.98 0.99 0.53 0.85 0.75 0.9  0.99 0.85 0.56 0.96 1.\n",
      " 0.97 0.95 0.94 1.   0.99 0.99 0.99 0.97 0.82 0.98 0.87 0.82 0.89 0.73\n",
      " 0.89 0.95 0.99 0.88 0.85 0.99 0.79 0.98 0.99 0.86 0.98 0.98 0.99 0.95\n",
      " 0.9  0.99 0.4  0.79 0.97 0.94 0.97 0.96 0.78 0.76 0.85 1.   0.97 0.86\n",
      " 0.95 0.77 0.54 0.89 0.82 0.99 0.33 0.9  0.86 0.86 1.   0.98 0.51 0.97\n",
      " 0.9  0.82 0.93 0.94 0.92 0.95 0.98 0.97 0.98 0.89 0.91 0.88]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   1.   0.48 0.62 0.87 0.74 0.89 0.87 0.95 0.95 0.86 0.95 1.   0.93\n",
      " 1.   1.   0.94 0.95 1.   0.99 1.   0.95 0.99 0.96 0.46 0.86 0.9  0.75\n",
      " 0.98 0.96 0.84 0.89 0.89 0.78 1.   0.97 0.97 0.97 0.92 0.94 0.92 0.97\n",
      " 1.   0.92 0.71 0.93 0.91 0.98 1.   1.   0.64 0.87 0.94 0.93 0.98 0.93\n",
      " 0.98 0.86 0.7  0.96 0.94 0.99 0.83 1.   0.96 0.99 0.94 0.98 0.85 0.96\n",
      " 0.92 0.83 0.85 0.97 1.   0.91 1.   0.84 0.98 0.98 0.84 1.   0.96 0.88\n",
      " 0.87 0.88 0.93 0.98 1.   0.95 0.88 0.98 1.   0.7  0.98 0.95 1.   1.\n",
      " 0.98 0.98 0.69 1.   0.99 0.87 0.92 0.9  0.92 0.98 0.64 0.98 0.77 0.93\n",
      " 0.8  1.   0.94 1.   1.   1.   1.   0.91 0.97 0.96 1.   0.81 0.99 0.99\n",
      " 0.99 0.94 0.95 0.9  0.85 0.8  0.79 0.9  0.95 0.84 1.   0.95 0.96 0.91\n",
      " 1.   0.94 0.97 0.96 1.   1.   0.94 1.   0.99 0.92 0.86 0.69 0.92 0.98\n",
      " 1.   0.81 0.99 0.97 0.95 0.81 0.98 0.94 0.71 0.94 0.94]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.85 0.91 0.8  0.91 0.97 0.86 0.91 0.96 0.81 0.68 0.87 1.   0.91 1.\n",
      " 0.97 0.94 0.72 0.94 0.99 0.95 0.83 0.99 1.   0.78 0.89 0.88 1.   0.8\n",
      " 0.58 0.97 0.85 0.87 0.85 0.65 1.   0.97 0.82 1.   0.73 0.97 0.82 0.91\n",
      " 0.88 0.95 0.89 0.99 1.   0.87 0.87 0.99 0.58 0.87 0.92 1.   0.95 0.82\n",
      " 1.   0.97 0.7  0.93 0.92 0.81 0.97 0.94 0.92 1.   0.87 0.98 0.77 0.95\n",
      " 0.84 0.96 0.95 0.79 0.88 0.89 0.99 0.98 0.95 0.44 1.   0.9  0.99 0.71\n",
      " 0.98 0.85 0.92 0.94 0.82 0.93 0.92 1.   1.   0.94 0.91 0.9  0.93 0.99\n",
      " 0.94 0.93 0.99 0.98 0.89 0.97 0.97 0.79 0.94 0.95 0.7  0.85 0.98 0.91\n",
      " 0.94 1.   0.81 0.93 0.33 0.91 0.93 0.83 0.99 0.99 0.85 0.9  0.96 0.66\n",
      " 0.99 0.91 0.94 0.88 0.64 0.87 0.98 0.77 0.95 1.   0.9  0.75 0.95 0.9\n",
      " 0.97 0.92 1.   0.96 0.8  0.92 0.5  0.97 0.71 0.93 1.   0.98 0.93 1.\n",
      " 0.98 0.99 0.88 0.87 1.   0.71 0.34 0.96 0.71 0.89 0.83]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8  63   6 751]\n",
      "공통 \n",
      "AUC : 0.6582506930619385\n",
      "AUPRC : 0.9474247746829547\n",
      "Optimized precision : 0.033735004013899866\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9166666666666666\n",
      "Precision(pb) : 0.9319419477780918\n",
      "Recall(pb) : 0.8304860846785931\n",
      "F1 score(pb) : 0.8782938141072618\n",
      "MCC : 0.22748620224235377\n",
      "G-mean : 0.3343396225939631\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  8  63]\n",
      " [  6 751]]\n",
      "Accuracy : 0.9166666666666666\n",
      "Precision : 0.9226044226044227\n",
      "Recall : 0.9920739762219286\n",
      "F1 score : 0.9560789306174411\n",
      "MCC : 0.24045655231691782\n",
      "G-mean : 0.33433962259396305\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n",
      "[1.   0.97 0.72 0.91 0.97 0.95 0.69 0.88 0.92 1.   0.99 0.84 0.94 0.82\n",
      " 0.96 1.   0.89 0.84 0.99 0.95 1.   0.88 0.97 0.96 1.   0.98 0.96 0.99\n",
      " 0.98 1.   0.8  0.99 0.94 0.89 0.85 1.   1.   0.95 0.94 0.93 0.85 0.71\n",
      " 0.98 0.82 0.93 1.   0.41 0.75 0.86 0.9  0.94 0.62 0.99 0.91 0.92 0.95\n",
      " 0.99 0.87 0.94 0.93 0.91 0.71 0.89 0.95 0.72 0.95 0.92 0.97 0.85 0.98\n",
      " 0.98 0.7  0.83 0.9  0.98 0.89 0.92 0.92 0.98 0.85 1.   0.49 0.77 0.71\n",
      " 0.69 0.95 0.97 0.95 0.96 0.7  0.84 1.   0.99 0.99 1.   0.89 0.93 1.\n",
      " 0.96 0.93 0.89 0.88 0.96 0.94 0.9  0.96 0.88 0.92 0.87 0.78 0.88 0.45\n",
      " 0.96 0.89 0.81 0.76 0.98 0.96 0.77 0.85 0.88 0.63 0.99 1.   0.96 1.\n",
      " 0.91 0.84 0.95 1.   1.   0.68 1.   0.99 0.99 0.96 1.   0.85 0.93 0.78\n",
      " 0.91 0.97 1.   0.97 0.95 0.97 0.99 0.91 0.9  1.   0.25 0.93 0.8  0.99\n",
      " 0.96 0.78 1.   1.   0.99 0.89 1.   1.   0.53 0.37 0.95 0.89]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.97 1.   0.82 0.82 0.97 1.   0.78 1.   0.99 0.89 0.98 0.48 0.91 0.49\n",
      " 0.93 0.89 1.   0.88 1.   0.99 0.97 0.99 0.73 0.99 0.86 1.   0.97 0.61\n",
      " 0.92 0.87 0.99 0.98 0.97 0.75 0.92 0.9  0.66 0.65 0.91 0.99 0.86 0.96\n",
      " 0.94 0.89 0.95 1.   0.96 1.   1.   0.99 0.96 0.98 0.76 0.99 1.   0.98\n",
      " 0.99 0.36 0.97 0.91 0.97 0.69 0.88 0.96 0.83 0.97 0.91 0.72 0.41 1.\n",
      " 0.93 0.77 0.97 0.77 1.   0.97 0.69 0.75 0.93 0.92 0.91 0.95 0.99 0.9\n",
      " 0.89 0.95 0.97 0.74 0.91 0.64 0.69 0.81 0.96 0.92 0.68 0.8  1.   0.89\n",
      " 1.   0.84 0.91 0.73 0.99 0.99 1.   0.99 0.91 0.93 0.99 0.65 0.89 0.96\n",
      " 0.8  1.   0.95 1.   1.   1.   1.   0.99 1.   0.59 0.99 0.97 0.73 0.99\n",
      " 0.74 0.98 0.97 1.   1.   0.94 0.97 0.98 0.96 0.94 0.96 0.89 0.99 0.99\n",
      " 0.67 0.95 1.   1.   0.66 1.   0.98 0.72 1.   0.99 0.97 0.6  0.92 1.\n",
      " 0.97 0.99 0.99 0.99 0.98 1.   0.93 1.   0.97 0.83 0.63 1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.57 0.95 0.87 0.75 0.96 0.97 0.77 1.   1.   0.98 0.98 0.84 1.   0.93\n",
      " 0.78 0.72 0.9  0.98 0.87 1.   0.93 0.94 1.   0.94 1.   1.   0.96 0.95\n",
      " 0.98 0.97 0.7  0.96 0.98 0.94 0.57 0.88 0.99 0.82 0.93 0.93 0.96 0.9\n",
      " 0.81 0.99 1.   0.93 0.99 0.77 0.93 1.   1.   0.95 0.99 0.96 1.   0.76\n",
      " 1.   0.99 0.97 0.96 0.83 0.99 0.99 0.68 0.57 0.84 0.9  0.84 0.87 0.99\n",
      " 0.87 0.99 0.76 0.85 0.94 0.84 0.98 0.78 0.95 0.96 1.   0.96 1.   0.99\n",
      " 1.   0.71 0.93 0.97 0.98 0.69 0.92 0.78 0.91 1.   0.84 0.59 0.93 1.\n",
      " 1.   0.9  0.89 0.99 0.99 0.99 1.   0.95 0.77 0.99 0.86 0.86 0.89 0.82\n",
      " 0.9  0.94 0.99 0.9  0.84 0.97 0.82 0.99 1.   0.91 0.98 0.99 0.98 0.98\n",
      " 0.92 1.   0.45 0.71 0.92 0.94 0.94 0.98 0.8  0.69 0.82 0.96 0.95 0.91\n",
      " 0.96 0.8  0.65 0.92 0.86 0.99 0.37 0.88 0.79 0.82 0.96 0.92 0.47 0.89\n",
      " 0.85 0.89 0.96 0.98 0.97 0.95 1.   0.97 0.96 0.89 0.84 0.89]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   0.98 0.44 0.73 0.93 0.83 0.91 0.91 0.94 0.9  0.88 0.94 1.   0.9\n",
      " 1.   1.   0.9  0.98 1.   0.96 1.   0.91 0.99 0.93 0.54 0.83 0.93 0.77\n",
      " 0.98 0.97 0.82 0.88 0.94 0.87 1.   0.99 1.   0.95 0.94 0.98 0.93 0.96\n",
      " 1.   0.93 0.72 0.98 0.91 0.99 0.95 1.   0.55 0.93 0.91 0.93 0.96 0.96\n",
      " 0.97 0.87 0.65 0.95 0.96 1.   0.77 1.   0.96 1.   0.91 0.98 0.88 0.94\n",
      " 0.88 0.83 0.92 0.97 0.98 0.94 1.   0.82 0.99 1.   0.83 1.   0.98 0.91\n",
      " 0.92 0.87 0.92 0.99 0.98 0.91 0.9  0.97 0.97 0.71 0.99 0.93 0.98 0.99\n",
      " 0.97 0.94 0.78 0.98 1.   0.95 0.87 0.95 0.93 1.   0.66 0.95 0.8  0.91\n",
      " 0.78 0.97 0.97 0.98 1.   0.97 0.97 0.83 0.94 0.97 0.99 0.75 0.96 1.\n",
      " 1.   0.95 0.93 0.88 0.88 0.76 0.87 0.84 0.95 0.89 1.   0.93 0.96 0.94\n",
      " 1.   0.94 0.97 0.95 1.   0.95 0.93 0.98 0.99 0.93 0.8  0.71 0.94 0.99\n",
      " 0.99 0.84 1.   0.99 0.96 0.79 0.96 0.94 0.7  0.97 0.89]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.89 0.92 0.74 0.97 0.93 0.89 0.93 0.97 0.77 0.74 0.9  0.99 0.93 1.\n",
      " 1.   0.93 0.66 0.97 1.   0.93 0.83 0.99 1.   0.71 0.81 0.91 1.   0.75\n",
      " 0.56 0.93 0.85 0.9  0.84 0.64 1.   0.98 0.84 1.   0.81 0.93 0.82 0.91\n",
      " 0.79 1.   0.83 0.99 0.99 0.86 0.85 0.97 0.58 0.84 0.96 1.   0.97 0.87\n",
      " 1.   0.99 0.67 0.93 0.86 0.8  0.98 0.95 0.86 1.   0.93 0.93 0.83 0.99\n",
      " 0.91 0.97 0.94 0.78 0.82 0.92 1.   0.99 0.97 0.49 1.   0.84 0.97 0.72\n",
      " 0.99 0.91 0.91 0.94 0.77 0.96 0.92 1.   1.   0.92 0.9  0.87 0.92 1.\n",
      " 1.   0.94 1.   0.95 0.9  0.95 0.99 0.86 0.93 0.92 0.65 0.92 0.97 0.9\n",
      " 0.95 0.99 0.78 0.97 0.46 0.9  0.95 0.83 1.   0.99 0.9  0.95 0.95 0.61\n",
      " 0.96 0.94 0.97 0.83 0.67 0.9  0.99 0.79 0.89 1.   0.85 0.76 0.95 0.88\n",
      " 0.96 0.88 1.   0.98 0.81 0.89 0.53 0.94 0.76 0.95 0.98 0.96 0.93 1.\n",
      " 0.95 0.97 0.95 0.85 1.   0.57 0.39 0.96 0.77 0.84 0.84]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8  63   8 749]\n",
      "공통 \n",
      "AUC : 0.6636835544309451\n",
      "AUPRC : 0.9486608031066219\n",
      "Optimized precision : 0.03402143431815221\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9142512077294686\n",
      "Precision(pb) : 0.9331557603667706\n",
      "Recall(pb) : 0.8261740992119527\n",
      "F1 score(pb) : 0.8764122492982271\n",
      "MCC : 0.20768205987017752\n",
      "G-mean : 0.3338941332254215\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  8  63]\n",
      " [  8 749]]\n",
      "Accuracy : 0.9142512077294686\n",
      "Precision : 0.9224137931034483\n",
      "Recall : 0.9894319682959049\n",
      "F1 score : 0.9547482472912683\n",
      "MCC : 0.2246735403515386\n",
      "G-mean : 0.33389413322542144\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n",
      "[1.   0.98 0.63 0.9  0.96 0.92 0.72 0.79 0.96 1.   0.99 0.92 0.94 0.81\n",
      " 0.94 0.99 0.84 0.87 1.   0.99 1.   0.84 0.92 1.   0.99 1.   0.99 0.99\n",
      " 0.97 0.99 0.78 0.99 0.94 0.83 0.85 1.   1.   0.97 0.91 0.91 0.93 0.76\n",
      " 1.   0.96 0.94 1.   0.47 0.77 0.88 0.94 0.97 0.62 1.   0.93 0.97 0.96\n",
      " 0.98 0.84 0.91 0.92 0.98 0.74 0.86 0.93 0.79 0.99 0.92 1.   0.81 0.97\n",
      " 0.99 0.8  0.79 0.98 0.95 0.89 0.99 0.91 0.98 0.85 1.   0.55 0.74 0.74\n",
      " 0.7  0.94 0.97 0.98 0.97 0.74 0.9  0.99 0.96 1.   1.   0.84 0.92 1.\n",
      " 0.96 0.93 0.89 0.94 0.93 0.94 0.87 0.96 0.86 0.93 0.91 0.8  0.87 0.48\n",
      " 0.92 0.93 0.83 0.83 1.   0.95 0.75 0.85 0.91 0.68 0.99 0.99 0.96 1.\n",
      " 0.92 0.84 0.88 1.   1.   0.67 1.   1.   0.98 0.97 0.99 0.88 0.96 0.76\n",
      " 0.96 0.98 1.   0.99 0.92 0.97 0.97 0.93 0.97 0.99 0.23 0.91 0.81 1.\n",
      " 0.92 0.83 0.97 1.   1.   0.87 1.   1.   0.55 0.34 0.95 0.95]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.96 1.   0.83 0.83 0.95 1.   0.82 1.   0.97 0.88 1.   0.57 0.92 0.58\n",
      " 0.97 0.85 1.   0.89 0.99 1.   0.99 1.   0.72 0.95 0.8  0.97 0.99 0.68\n",
      " 0.97 0.86 0.99 0.97 0.96 0.73 0.89 0.95 0.69 0.65 0.86 1.   0.85 0.98\n",
      " 0.89 0.93 0.97 1.   0.97 1.   0.99 0.98 0.95 1.   0.78 1.   0.99 0.99\n",
      " 0.98 0.38 0.99 0.94 0.98 0.59 0.87 0.96 0.87 0.96 0.89 0.8  0.5  1.\n",
      " 0.95 0.65 0.94 0.75 1.   0.94 0.63 0.78 0.96 0.96 0.87 0.98 0.98 0.86\n",
      " 0.86 0.97 0.96 0.71 0.92 0.55 0.72 0.82 0.9  0.91 0.67 0.85 0.96 0.92\n",
      " 1.   0.89 0.94 0.72 0.96 0.98 1.   0.99 0.9  0.93 0.99 0.64 0.83 0.92\n",
      " 0.89 0.99 0.94 1.   0.99 0.99 0.99 0.98 0.98 0.62 1.   0.99 0.8  1.\n",
      " 0.79 0.99 0.99 1.   0.98 0.92 0.97 0.99 0.98 1.   0.96 0.96 0.98 0.94\n",
      " 0.62 0.99 0.98 1.   0.65 1.   0.96 0.67 1.   0.97 0.99 0.64 0.95 1.\n",
      " 0.93 0.98 1.   0.95 0.93 1.   0.96 1.   0.97 0.88 0.59 1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.47 0.93 0.95 0.72 1.   0.95 0.87 1.   0.98 0.99 0.99 0.87 1.   0.94\n",
      " 0.69 0.63 0.95 0.97 0.88 0.94 0.98 0.95 1.   0.83 1.   0.98 0.98 0.97\n",
      " 1.   0.96 0.63 0.97 1.   0.93 0.55 0.89 1.   0.88 0.89 0.96 0.97 0.91\n",
      " 0.79 0.97 1.   0.92 0.97 0.73 0.92 1.   0.97 0.96 0.99 0.99 1.   0.75\n",
      " 1.   0.99 0.96 0.93 0.85 0.99 0.99 0.63 0.63 0.88 0.92 0.92 0.87 1.\n",
      " 0.87 0.98 0.75 0.91 0.96 0.8  0.97 0.76 0.93 0.94 1.   0.99 1.   0.98\n",
      " 1.   0.78 0.95 0.95 0.98 0.65 0.83 0.75 0.89 1.   0.86 0.56 0.94 1.\n",
      " 0.97 0.98 0.92 1.   0.97 1.   0.99 0.95 0.82 0.99 0.91 0.77 0.86 0.74\n",
      " 0.91 0.97 1.   0.95 0.84 1.   0.86 1.   1.   0.85 0.99 0.99 0.98 0.97\n",
      " 0.9  1.   0.44 0.81 0.93 0.94 0.91 0.99 0.85 0.77 0.82 1.   0.92 0.91\n",
      " 0.87 0.8  0.51 0.89 0.83 1.   0.35 0.83 0.82 0.89 0.97 0.98 0.54 0.94\n",
      " 0.81 0.87 0.95 0.94 0.97 0.93 1.   1.   0.96 0.79 0.89 0.91]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   1.   0.48 0.7  0.92 0.83 0.81 0.87 0.93 0.95 0.87 0.94 1.   0.89\n",
      " 1.   1.   0.94 0.96 0.99 0.98 1.   0.93 1.   0.94 0.52 0.86 0.92 0.82\n",
      " 0.94 0.97 0.82 0.87 0.9  0.82 1.   1.   0.96 0.95 0.92 0.99 0.9  1.\n",
      " 0.99 0.94 0.82 0.95 0.92 1.   0.99 1.   0.59 0.94 0.95 0.95 0.98 0.97\n",
      " 0.99 0.87 0.63 0.94 0.94 1.   0.77 0.98 0.98 0.96 0.94 1.   0.88 0.92\n",
      " 0.91 0.8  0.87 0.99 1.   0.89 0.99 0.82 0.99 1.   0.8  1.   0.92 0.84\n",
      " 0.88 0.91 0.99 1.   0.99 0.96 0.92 0.98 1.   0.75 0.93 0.92 0.98 0.99\n",
      " 0.95 0.98 0.7  1.   1.   0.9  0.86 0.88 0.95 1.   0.65 0.99 0.79 0.88\n",
      " 0.8  0.97 0.93 1.   1.   1.   0.99 0.74 0.96 0.96 1.   0.81 0.95 1.\n",
      " 1.   0.95 0.93 0.89 0.93 0.7  0.78 0.88 0.98 0.86 1.   0.89 0.93 0.92\n",
      " 0.98 0.86 0.97 0.97 1.   0.97 0.94 1.   0.99 0.92 0.79 0.71 0.96 0.99\n",
      " 1.   0.75 1.   1.   0.97 0.84 0.97 0.95 0.78 0.97 0.93]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.86 0.95 0.79 0.94 0.95 0.96 0.92 0.98 0.8  0.7  0.92 1.   0.92 1.\n",
      " 0.99 0.93 0.72 0.91 0.98 0.96 0.8  0.94 0.99 0.78 0.87 0.92 0.99 0.76\n",
      " 0.59 0.95 0.87 0.85 0.83 0.75 0.99 0.96 0.89 1.   0.7  0.97 0.69 0.98\n",
      " 0.94 0.96 0.87 0.98 0.96 0.87 0.87 0.99 0.63 0.85 0.95 1.   0.97 0.84\n",
      " 0.99 0.96 0.68 0.96 0.92 0.8  0.93 0.91 0.91 1.   0.92 0.99 0.75 0.97\n",
      " 0.86 0.96 0.98 0.73 0.9  0.9  0.97 0.98 0.95 0.44 0.98 0.87 0.97 0.69\n",
      " 0.99 0.89 0.9  0.9  0.78 0.96 0.92 1.   1.   0.97 0.9  0.9  0.93 0.96\n",
      " 0.98 0.93 0.98 0.95 0.84 0.94 1.   0.85 0.89 0.88 0.58 0.9  0.92 0.82\n",
      " 0.99 1.   0.77 0.96 0.37 0.94 0.88 0.83 0.98 0.97 0.88 0.93 0.97 0.68\n",
      " 0.93 0.96 0.99 0.89 0.63 0.97 0.97 0.71 0.89 1.   0.86 0.73 0.91 0.9\n",
      " 0.98 0.86 1.   0.96 0.75 0.9  0.57 0.97 0.69 0.97 0.95 0.99 0.94 0.99\n",
      " 0.97 0.97 0.92 0.92 0.97 0.73 0.38 0.96 0.84 0.89 0.85]\n",
      "[  7  64   5 752]\n",
      "공통 \n",
      "AUC : 0.6591716746981227\n",
      "AUPRC : 0.9465526152884073\n",
      "Optimized precision : 0.020021525162858544\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9166666666666666\n",
      "Precision(pb) : 0.9327912447631173\n",
      "Recall(pb) : 0.8196949681758136\n",
      "F1 score(pb) : 0.8725937859545663\n",
      "MCC : 0.21550911604977915\n",
      "G-mean : 0.31295423013450174\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  7  64]\n",
      " [  5 752]]\n",
      "Accuracy : 0.9166666666666666\n",
      "Precision : 0.9215686274509803\n",
      "Recall : 0.9933949801849405\n",
      "F1 score : 0.9561347743165924\n",
      "MCC : 0.22688611428784394\n",
      "G-mean : 0.3129542301345017\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.   0.93 0.69 0.91 0.98 0.92 0.71 0.85 0.93 0.98 1.   0.87 0.94 0.83\n",
      " 0.99 1.   0.89 0.83 1.   0.96 1.   0.8  0.97 0.98 0.96 1.   0.99 0.99\n",
      " 0.98 1.   0.85 0.98 0.94 0.88 0.8  1.   0.99 0.94 0.91 0.94 0.91 0.73\n",
      " 1.   0.91 0.92 1.   0.47 0.7  0.83 0.96 0.94 0.67 0.96 0.93 0.95 0.95\n",
      " 0.97 0.84 0.94 0.92 0.88 0.72 0.88 0.94 0.77 0.98 0.89 0.99 0.81 0.95\n",
      " 0.98 0.73 0.81 0.97 0.97 0.94 0.92 0.91 0.92 0.83 1.   0.47 0.72 0.74\n",
      " 0.6  0.94 0.99 0.96 0.99 0.77 0.9  0.98 0.94 1.   1.   0.86 0.95 1.\n",
      " 0.95 0.91 0.87 0.93 0.94 0.96 0.89 0.97 0.94 0.91 0.91 0.82 0.91 0.47\n",
      " 0.91 0.95 0.74 0.8  0.96 0.86 0.79 0.87 0.89 0.67 1.   0.98 0.96 1.\n",
      " 0.98 0.82 0.91 1.   1.   0.71 1.   0.96 1.   0.96 0.99 0.77 0.92 0.66\n",
      " 0.93 0.97 1.   0.96 0.89 0.97 0.99 0.93 0.97 0.95 0.27 0.91 0.89 0.99\n",
      " 0.94 0.79 1.   0.97 0.99 0.77 1.   1.   0.6  0.34 0.95 0.94]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.98 1.   0.86 0.82 0.87 1.   0.8  1.   0.98 0.91 0.99 0.48 0.92 0.6\n",
      " 0.98 0.91 1.   0.86 1.   0.99 0.97 0.99 0.73 0.94 0.87 0.97 0.98 0.6\n",
      " 0.88 0.95 0.99 0.97 0.89 0.7  0.87 0.82 0.58 0.61 0.89 0.99 0.93 0.98\n",
      " 0.95 0.91 0.93 1.   0.9  0.98 0.99 0.97 0.92 1.   0.78 0.99 0.99 0.99\n",
      " 1.   0.29 0.98 0.98 0.98 0.67 0.8  0.99 0.89 1.   0.89 0.74 0.48 1.\n",
      " 0.97 0.78 0.95 0.74 1.   0.93 0.78 0.83 0.94 1.   0.89 0.97 0.97 0.88\n",
      " 0.75 1.   0.98 0.75 0.92 0.73 0.66 0.85 0.94 0.97 0.71 0.82 0.99 0.91\n",
      " 1.   0.9  1.   0.72 0.96 0.98 0.99 1.   0.98 0.88 0.99 0.68 0.86 0.92\n",
      " 0.88 1.   0.94 1.   0.98 1.   0.97 0.99 0.98 0.53 1.   0.97 0.79 0.95\n",
      " 0.82 0.99 0.97 1.   1.   0.97 0.98 0.99 1.   0.99 1.   0.95 0.98 0.96\n",
      " 0.63 0.99 1.   0.99 0.54 1.   0.98 0.7  1.   0.94 0.95 0.62 0.88 0.98\n",
      " 0.89 1.   1.   0.98 0.97 1.   0.92 1.   0.96 0.79 0.66 1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.44 0.95 0.96 0.78 1.   0.94 0.81 1.   1.   0.94 1.   0.84 0.98 0.95\n",
      " 0.7  0.67 0.93 0.99 0.95 0.98 0.97 0.97 1.   0.88 1.   0.99 0.97 0.95\n",
      " 0.97 0.98 0.72 0.96 0.98 0.97 0.55 0.88 1.   0.87 0.93 0.92 0.98 0.87\n",
      " 0.82 0.96 1.   0.96 0.95 0.75 0.97 1.   1.   0.93 0.96 0.98 1.   0.76\n",
      " 1.   0.99 0.98 0.95 0.81 0.98 0.99 0.73 0.66 0.88 0.98 0.87 0.93 0.99\n",
      " 0.87 1.   0.75 0.86 0.95 0.85 0.99 0.82 0.9  0.9  0.98 0.98 1.   1.\n",
      " 1.   0.83 0.97 0.95 0.98 0.67 0.95 0.79 0.93 0.99 0.85 0.57 0.9  1.\n",
      " 0.99 0.98 0.94 1.   0.97 0.99 0.98 0.99 0.76 1.   0.86 0.86 0.79 0.75\n",
      " 0.89 0.97 1.   0.9  0.87 0.96 0.83 0.99 0.99 0.85 0.96 0.93 0.97 0.97\n",
      " 0.92 0.99 0.48 0.77 0.95 0.98 0.95 1.   0.81 0.77 0.83 0.99 0.93 0.94\n",
      " 0.91 0.86 0.55 0.94 0.84 1.   0.31 0.88 0.8  0.82 0.98 0.95 0.5  0.94\n",
      " 0.84 0.94 0.96 0.92 0.97 0.93 1.   1.   0.94 0.8  0.82 0.95]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   0.98 0.46 0.63 0.94 0.77 0.89 0.89 0.91 0.94 0.87 0.94 1.   0.95\n",
      " 1.   1.   0.96 0.97 1.   0.99 1.   0.86 1.   0.96 0.49 0.9  0.95 0.65\n",
      " 0.94 0.98 0.85 0.93 0.94 0.95 1.   0.98 0.96 0.95 0.93 0.98 0.94 0.99\n",
      " 1.   0.93 0.77 0.95 0.92 1.   0.98 1.   0.71 0.91 0.93 0.96 0.98 0.93\n",
      " 0.98 0.84 0.65 0.96 0.93 1.   0.77 0.99 0.97 0.98 0.93 1.   0.82 0.93\n",
      " 0.87 0.78 0.94 0.99 0.99 0.89 0.97 0.78 0.99 0.99 0.88 0.99 0.93 0.87\n",
      " 0.94 0.87 0.94 0.97 0.99 0.93 0.96 0.97 0.99 0.66 0.99 0.96 0.96 1.\n",
      " 0.94 0.94 0.72 0.99 1.   0.9  0.9  0.9  0.92 1.   0.68 0.97 0.79 0.91\n",
      " 0.7  1.   0.94 0.99 0.98 1.   0.97 0.83 0.96 0.95 1.   0.79 0.96 1.\n",
      " 0.98 0.96 0.93 0.91 0.91 0.74 0.73 0.83 0.94 0.8  1.   0.94 0.93 0.92\n",
      " 1.   0.95 0.98 0.89 1.   0.99 0.88 1.   0.99 0.96 0.86 0.6  0.93 1.\n",
      " 1.   0.74 1.   0.98 0.96 0.84 0.97 0.96 0.83 0.97 0.88]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.9  0.94 0.76 0.94 0.96 0.8  0.89 0.97 0.87 0.77 0.93 1.   0.93 0.99\n",
      " 1.   0.89 0.69 0.95 0.99 0.97 0.77 0.99 1.   0.73 0.88 0.84 1.   0.81\n",
      " 0.5  0.95 0.9  0.9  0.84 0.69 1.   0.96 0.82 1.   0.75 0.94 0.69 0.94\n",
      " 0.92 0.97 0.86 0.99 0.97 0.87 0.95 0.99 0.66 0.9  0.9  1.   0.91 0.74\n",
      " 1.   0.99 0.72 0.91 0.92 0.76 0.97 0.91 0.92 1.   0.94 0.97 0.81 0.96\n",
      " 0.91 0.97 0.96 0.81 0.88 0.94 1.   0.98 0.94 0.58 0.99 0.89 0.97 0.66\n",
      " 0.99 0.84 0.92 0.93 0.76 0.94 0.9  0.99 1.   0.98 0.92 0.87 0.89 0.98\n",
      " 0.99 0.94 0.96 0.95 0.91 0.93 0.98 0.83 0.89 0.92 0.71 0.84 0.97 0.9\n",
      " 0.96 0.99 0.78 0.81 0.33 0.97 0.93 0.86 1.   0.99 0.83 0.96 0.97 0.63\n",
      " 0.96 0.92 0.96 0.88 0.68 0.95 0.99 0.83 0.94 1.   0.91 0.68 0.91 0.86\n",
      " 1.   0.92 0.99 0.98 0.89 0.94 0.58 0.96 0.76 0.95 0.95 0.99 0.92 1.\n",
      " 0.97 0.98 0.93 0.92 1.   0.64 0.43 1.   0.72 0.85 0.78]\n",
      "[  6  65   9 748]\n",
      "공통 \n",
      "AUC : 0.6611810891770703\n",
      "AUPRC : 0.9478834270572565\n",
      "Optimized precision : 0.006548554286025549\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9106280193236715\n",
      "Precision(pb) : 0.9333428250449363\n",
      "Recall(pb) : 0.8187822745286417\n",
      "F1 score(pb) : 0.8723173492478005\n",
      "MCC : 0.15245109035131557\n",
      "G-mean : 0.2889677058324118\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  6  65]\n",
      " [  9 748]]\n",
      "Accuracy : 0.9106280193236715\n",
      "Precision : 0.9200492004920049\n",
      "Recall : 0.988110964332893\n",
      "F1 score : 0.9528662420382167\n",
      "MCC : 0.17311381820062277\n",
      "G-mean : 0.2889677058324118\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.   0.99 0.67 0.92 0.95 0.91 0.69 0.93 0.95 1.   0.99 0.86 0.89 0.86\n",
      " 0.97 0.99 0.85 0.85 1.   0.93 0.99 0.82 0.97 0.98 0.98 0.99 0.95 1.\n",
      " 0.96 1.   0.84 0.98 0.89 0.91 0.85 0.99 1.   0.98 0.94 0.91 0.93 0.7\n",
      " 0.99 0.91 0.88 0.99 0.42 0.74 0.87 0.96 0.96 0.58 0.98 0.9  0.95 0.98\n",
      " 0.97 0.84 0.95 0.9  0.88 0.74 0.89 0.96 0.77 0.98 0.94 1.   0.84 1.\n",
      " 0.99 0.8  0.81 0.9  0.95 0.91 0.91 0.94 0.94 0.89 1.   0.6  0.76 0.77\n",
      " 0.66 0.93 1.   0.97 0.96 0.77 0.87 0.96 0.97 0.98 1.   0.83 0.94 1.\n",
      " 0.98 0.94 0.95 0.88 0.9  0.98 0.88 0.96 0.9  0.95 0.91 0.79 0.88 0.46\n",
      " 0.9  0.94 0.79 0.73 0.99 0.94 0.78 0.85 0.84 0.67 1.   0.98 0.93 1.\n",
      " 0.89 0.9  0.95 1.   1.   0.74 1.   0.99 1.   0.92 0.96 0.87 0.93 0.71\n",
      " 0.89 0.97 1.   1.   0.94 0.99 1.   0.92 0.92 0.98 0.21 0.88 0.87 0.98\n",
      " 0.93 0.84 1.   0.99 0.99 0.88 1.   1.   0.61 0.38 0.92 0.87]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.99 1.   0.84 0.81 0.97 1.   0.77 1.   0.98 0.96 0.99 0.48 0.96 0.66\n",
      " 0.93 0.92 1.   0.94 1.   0.99 0.98 0.99 0.75 0.98 0.83 0.96 0.93 0.73\n",
      " 0.96 0.93 1.   0.96 0.95 0.69 0.94 0.9  0.62 0.64 0.93 0.98 0.85 0.98\n",
      " 0.94 0.9  0.94 1.   0.97 0.99 0.98 0.99 0.97 0.99 0.77 0.99 0.98 0.99\n",
      " 0.99 0.29 1.   0.96 0.99 0.61 0.76 0.98 0.84 0.98 0.88 0.81 0.42 1.\n",
      " 0.96 0.68 0.99 0.79 0.99 0.93 0.61 0.79 0.98 0.96 0.88 0.98 0.97 0.79\n",
      " 0.88 0.95 0.95 0.74 0.9  0.69 0.65 0.85 0.97 0.94 0.66 0.82 0.98 0.93\n",
      " 1.   0.85 0.93 0.71 1.   0.99 0.99 1.   0.88 0.88 0.99 0.67 0.83 0.97\n",
      " 0.82 0.99 0.97 1.   1.   0.98 0.99 0.98 1.   0.58 0.99 1.   0.7  0.97\n",
      " 0.76 0.98 0.97 1.   1.   0.94 0.97 1.   0.96 0.98 0.94 0.97 1.   0.97\n",
      " 0.61 0.98 1.   0.97 0.66 1.   0.99 0.76 1.   0.95 0.99 0.66 0.92 0.99\n",
      " 0.89 0.99 1.   0.92 0.91 1.   0.94 1.   0.91 0.8  0.47 1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.51 0.93 0.97 0.73 0.97 0.93 0.87 1.   0.98 0.96 0.97 0.78 0.97 0.93\n",
      " 0.72 0.6  0.89 1.   0.91 0.94 0.97 0.97 1.   0.86 1.   0.98 0.97 0.94\n",
      " 0.99 0.95 0.6  0.94 1.   0.97 0.5  0.83 0.97 0.83 0.86 0.92 0.96 0.9\n",
      " 0.8  0.98 0.97 0.9  0.94 0.76 0.92 1.   0.98 0.93 0.99 0.97 1.   0.74\n",
      " 0.98 1.   0.95 0.92 0.79 1.   0.99 0.58 0.6  0.85 0.93 0.87 0.92 0.99\n",
      " 0.84 0.96 0.74 0.91 0.94 0.81 0.95 0.81 0.95 0.95 1.   0.99 0.99 0.96\n",
      " 1.   0.75 0.96 0.93 0.99 0.68 0.92 0.8  0.93 1.   0.85 0.64 0.91 1.\n",
      " 0.99 0.93 0.94 0.99 1.   0.98 1.   1.   0.8  0.99 0.87 0.78 0.88 0.77\n",
      " 0.79 0.91 1.   0.91 0.9  0.96 0.85 1.   1.   0.89 0.98 0.96 0.99 0.98\n",
      " 0.9  1.   0.36 0.78 0.94 0.94 0.95 0.98 0.8  0.77 0.7  0.99 0.96 0.92\n",
      " 0.93 0.82 0.53 0.91 0.86 0.99 0.32 0.93 0.8  0.84 1.   0.96 0.51 0.91\n",
      " 0.84 0.91 0.99 0.93 0.93 0.93 1.   0.95 0.97 0.75 0.85 0.94]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   1.   0.45 0.68 0.92 0.76 0.86 0.89 0.92 0.96 0.92 0.98 1.   0.94\n",
      " 1.   1.   0.89 0.96 1.   1.   1.   0.87 1.   0.96 0.45 0.93 0.93 0.76\n",
      " 0.93 0.95 0.9  0.91 0.9  0.9  1.   0.99 0.95 0.97 0.92 0.96 0.95 1.\n",
      " 0.99 0.98 0.8  0.92 0.95 1.   0.96 1.   0.57 0.96 0.94 0.91 0.95 0.93\n",
      " 0.95 0.85 0.71 1.   0.93 1.   0.8  1.   0.98 0.99 0.92 1.   0.9  0.96\n",
      " 0.87 0.83 0.93 0.97 0.97 0.91 0.94 0.8  0.99 0.98 0.83 1.   0.97 0.88\n",
      " 0.92 0.83 0.95 1.   0.98 0.96 0.91 1.   1.   0.78 0.99 0.95 0.95 0.99\n",
      " 0.98 0.97 0.72 0.99 1.   0.91 0.88 0.91 0.97 1.   0.7  0.96 0.83 0.91\n",
      " 0.75 0.99 0.95 0.99 1.   1.   1.   0.87 1.   0.97 0.99 0.75 0.95 0.98\n",
      " 0.99 0.89 0.91 0.86 0.89 0.79 0.74 0.88 0.97 0.86 1.   0.92 0.98 0.97\n",
      " 1.   0.93 0.98 0.93 1.   0.99 0.9  1.   0.99 0.98 0.77 0.72 0.92 0.97\n",
      " 1.   0.75 1.   0.97 0.97 0.8  0.94 0.95 0.75 0.96 0.93]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.9  0.98 0.71 0.96 0.93 0.88 0.89 0.88 0.77 0.78 0.91 1.   0.93 1.\n",
      " 1.   0.84 0.59 0.92 1.   0.97 0.78 1.   1.   0.79 0.86 0.86 1.   0.84\n",
      " 0.53 0.93 0.93 0.87 0.85 0.68 1.   0.99 0.76 1.   0.75 0.94 0.8  0.95\n",
      " 0.94 0.94 0.86 0.99 0.99 0.85 0.81 1.   0.71 0.8  0.93 1.   0.97 0.78\n",
      " 0.98 0.99 0.7  0.92 0.85 0.83 0.95 0.91 0.9  1.   0.85 0.94 0.85 0.96\n",
      " 0.88 0.97 0.98 0.75 0.84 0.92 0.97 0.99 0.96 0.46 0.98 0.89 0.98 0.67\n",
      " 0.99 0.86 0.88 0.94 0.84 0.94 0.94 0.99 0.99 0.94 0.94 0.91 0.95 0.98\n",
      " 0.98 0.93 0.99 0.96 0.89 0.89 0.95 0.84 0.89 0.96 0.73 0.91 0.95 0.9\n",
      " 0.98 0.97 0.77 0.9  0.31 0.91 0.92 0.88 0.98 0.99 0.85 0.95 0.96 0.63\n",
      " 0.96 0.95 0.99 0.85 0.6  0.89 0.98 0.78 0.89 1.   0.88 0.76 0.92 0.89\n",
      " 0.95 0.92 1.   0.96 0.82 0.92 0.56 0.96 0.76 0.98 0.98 0.99 0.86 1.\n",
      " 0.96 0.96 0.82 0.91 1.   0.68 0.35 0.99 0.78 0.84 0.8 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7  64   8 749]\n",
      "공통 \n",
      "AUC : 0.6661022940815302\n",
      "AUPRC : 0.9478316874532787\n",
      "Optimized precision : 0.020406782483787998\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9130434782608695\n",
      "Precision(pb) : 0.9339205023738395\n",
      "Recall(pb) : 0.8233099260701291\n",
      "F1 score(pb) : 0.8751339691353417\n",
      "MCC : 0.18479275133283987\n",
      "G-mean : 0.3123293624958452\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  7  64]\n",
      " [  8 749]]\n",
      "Accuracy : 0.9130434782608695\n",
      "Precision : 0.9212792127921279\n",
      "Recall : 0.9894319682959049\n",
      "F1 score : 0.954140127388535\n",
      "MCC : 0.2026041008588967\n",
      "G-mean : 0.31232936249584514\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n",
      "[0.99 0.98 0.76 0.88 0.94 0.95 0.71 0.84 0.91 1.   0.99 0.84 0.92 0.85\n",
      " 0.94 1.   0.85 0.84 1.   0.95 1.   0.85 0.96 0.98 0.99 0.99 0.96 1.\n",
      " 0.96 1.   0.83 1.   0.89 0.89 0.82 0.99 0.99 0.97 0.91 0.87 0.91 0.74\n",
      " 0.96 0.9  0.88 1.   0.44 0.82 0.75 0.91 0.96 0.55 0.97 0.93 0.88 0.98\n",
      " 1.   0.86 0.92 0.92 0.89 0.74 0.84 0.92 0.81 0.98 0.93 0.98 0.79 0.94\n",
      " 0.99 0.79 0.79 0.95 0.96 0.95 0.96 0.87 0.9  0.91 1.   0.58 0.75 0.74\n",
      " 0.69 0.93 0.98 0.94 0.97 0.8  0.86 0.97 0.98 1.   1.   0.87 0.97 1.\n",
      " 0.95 0.89 0.86 0.9  0.98 0.97 0.9  0.96 0.89 0.9  0.9  0.82 0.87 0.5\n",
      " 0.93 0.93 0.75 0.77 1.   0.93 0.79 0.85 0.92 0.58 1.   0.98 0.98 1.\n",
      " 0.94 0.81 0.93 1.   1.   0.79 1.   0.98 1.   0.94 0.99 0.87 0.94 0.79\n",
      " 0.96 0.95 1.   0.99 0.94 0.99 0.98 0.88 0.94 0.93 0.24 0.88 0.86 0.99\n",
      " 0.97 0.85 0.99 1.   1.   0.84 1.   1.   0.6  0.38 0.92 0.97]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.97 0.99 0.91 0.8  0.87 1.   0.82 1.   1.   0.93 0.99 0.49 0.94 0.62\n",
      " 0.96 0.82 1.   0.93 1.   0.99 0.98 0.98 0.78 1.   0.84 0.98 0.99 0.71\n",
      " 0.92 0.91 0.99 0.97 0.96 0.73 0.88 0.89 0.78 0.62 0.96 1.   0.91 0.97\n",
      " 0.95 0.85 0.93 1.   0.93 0.99 0.98 0.97 0.94 0.99 0.81 0.99 0.98 0.98\n",
      " 0.98 0.24 0.98 0.97 0.99 0.68 0.85 0.96 0.81 0.99 0.88 0.78 0.47 1.\n",
      " 0.95 0.78 0.97 0.73 1.   0.94 0.69 0.78 0.95 0.98 0.9  0.98 0.97 0.9\n",
      " 0.89 0.99 0.95 0.66 0.92 0.61 0.76 0.79 0.94 0.92 0.71 0.85 0.98 0.95\n",
      " 1.   0.85 0.94 0.76 0.97 0.94 0.99 0.99 0.94 0.91 1.   0.65 0.95 0.91\n",
      " 0.89 1.   0.95 0.98 1.   1.   1.   1.   0.99 0.61 1.   0.98 0.82 0.95\n",
      " 0.77 0.99 0.99 1.   1.   0.9  0.99 1.   0.98 0.97 0.95 0.93 0.98 0.96\n",
      " 0.72 0.97 0.99 0.99 0.65 0.99 0.99 0.83 1.   0.97 1.   0.59 0.95 1.\n",
      " 0.96 0.98 0.98 0.96 0.98 1.   0.89 1.   0.96 0.84 0.62 1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.55 0.93 0.96 0.67 0.99 0.98 0.85 1.   0.98 0.99 1.   0.84 1.   0.93\n",
      " 0.72 0.64 0.89 1.   0.86 0.99 0.93 0.95 1.   0.86 1.   1.   0.97 0.97\n",
      " 0.97 0.96 0.68 0.98 0.97 1.   0.55 0.87 0.99 0.9  0.92 0.92 0.97 0.95\n",
      " 0.73 0.99 1.   0.88 0.95 0.69 0.94 1.   0.99 0.97 0.99 0.98 0.99 0.74\n",
      " 1.   0.98 0.97 0.92 0.77 0.99 1.   0.7  0.63 0.84 0.94 0.84 0.87 0.99\n",
      " 0.87 0.99 0.78 0.88 0.96 0.83 1.   0.87 0.97 0.96 0.98 0.98 1.   0.98\n",
      " 1.   0.75 0.96 1.   0.97 0.65 0.87 0.71 0.93 1.   0.93 0.63 0.93 1.\n",
      " 0.99 0.93 0.9  0.99 0.96 0.99 1.   0.96 0.8  0.99 0.89 0.81 0.84 0.63\n",
      " 0.9  0.92 1.   0.86 0.86 0.96 0.86 0.99 1.   0.84 0.98 0.96 0.97 0.97\n",
      " 0.89 0.99 0.42 0.8  0.94 0.92 0.9  0.97 0.81 0.73 0.89 0.98 0.93 0.86\n",
      " 0.92 0.79 0.47 0.96 0.83 0.98 0.34 0.87 0.81 0.82 0.95 0.97 0.53 0.88\n",
      " 0.82 0.87 0.98 0.94 0.96 0.93 0.98 0.96 0.98 0.79 0.87 0.89]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   0.97 0.55 0.68 0.92 0.86 0.85 0.86 0.92 0.96 0.85 0.97 0.99 0.96\n",
      " 0.99 1.   0.93 1.   0.99 0.99 0.99 0.86 1.   0.92 0.54 0.89 0.95 0.76\n",
      " 0.97 0.99 0.87 0.86 0.86 0.87 1.   1.   0.96 0.96 0.93 0.98 0.95 0.97\n",
      " 0.99 0.97 0.75 0.97 0.97 0.99 0.97 1.   0.65 0.91 0.97 0.93 0.95 0.95\n",
      " 0.96 0.81 0.68 0.95 0.94 0.99 0.75 0.99 0.94 0.99 0.94 1.   0.82 0.96\n",
      " 0.91 0.82 0.88 0.98 0.98 0.97 0.97 0.83 0.99 0.99 0.73 0.98 0.95 0.84\n",
      " 0.81 0.86 0.97 0.99 0.97 0.98 0.91 0.96 0.98 0.65 0.96 0.95 1.   0.99\n",
      " 0.99 0.96 0.73 1.   0.99 0.95 0.88 0.86 0.93 0.99 0.67 0.98 0.83 0.88\n",
      " 0.76 0.99 0.97 0.97 1.   0.97 0.98 0.84 0.96 0.92 1.   0.68 0.98 1.\n",
      " 1.   0.93 0.89 0.93 0.91 0.82 0.77 0.89 0.97 0.88 1.   0.97 0.96 0.92\n",
      " 1.   0.89 0.95 0.87 1.   0.99 0.91 1.   1.   0.92 0.7  0.7  0.91 0.99\n",
      " 1.   0.72 1.   0.97 0.96 0.89 0.95 0.98 0.74 0.97 0.93]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.87 0.94 0.74 0.96 0.92 0.88 0.91 0.98 0.79 0.69 0.91 1.   0.95 1.\n",
      " 0.99 0.88 0.73 0.97 0.99 0.95 0.81 0.99 0.99 0.83 0.77 0.93 1.   0.86\n",
      " 0.52 0.95 0.88 0.86 0.94 0.62 1.   0.97 0.76 1.   0.8  0.91 0.75 0.97\n",
      " 0.86 0.95 0.83 0.99 1.   0.93 0.89 0.99 0.62 0.87 0.91 1.   0.98 0.81\n",
      " 1.   0.99 0.67 0.92 0.93 0.82 0.97 0.86 0.87 1.   0.88 0.94 0.84 0.92\n",
      " 0.89 0.96 0.98 0.75 0.92 0.92 0.98 0.99 0.93 0.48 1.   0.83 0.97 0.69\n",
      " 1.   0.86 0.9  0.97 0.78 0.92 0.92 1.   1.   0.95 0.92 0.94 0.97 0.98\n",
      " 0.97 0.92 0.98 0.99 0.88 0.94 0.97 0.78 0.91 0.9  0.67 0.87 0.96 0.98\n",
      " 0.99 1.   0.85 0.93 0.44 0.95 0.99 0.89 0.99 0.98 0.83 0.94 0.98 0.64\n",
      " 0.96 0.88 0.95 0.92 0.62 0.91 0.98 0.8  0.94 1.   0.88 0.66 0.93 0.86\n",
      " 0.98 0.96 0.99 0.96 0.82 0.93 0.51 0.94 0.7  0.96 0.97 0.98 0.91 1.\n",
      " 0.98 0.94 0.86 0.9  1.   0.71 0.32 0.96 0.84 0.9  0.85]\n",
      "[  7  64   5 752]\n",
      "공통 \n",
      "AUC : 0.6710793160548496\n",
      "AUPRC : 0.9508470564782245\n",
      "Optimized precision : 0.020021525162858544\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9166666666666666\n",
      "Precision(pb) : 0.9331304686341729\n",
      "Recall(pb) : 0.8279050699221063\n",
      "F1 score(pb) : 0.8773740551702487\n",
      "MCC : 0.21550911604977915\n",
      "G-mean : 0.31295423013450174\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  7  64]\n",
      " [  5 752]]\n",
      "Accuracy : 0.9166666666666666\n",
      "Precision : 0.9215686274509803\n",
      "Recall : 0.9933949801849405\n",
      "F1 score : 0.9561347743165924\n",
      "MCC : 0.22688611428784394\n",
      "G-mean : 0.3129542301345017\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.   0.98 0.67 0.9  0.94 0.92 0.71 0.84 0.93 1.   1.   0.79 0.93 0.81\n",
      " 0.96 0.99 0.94 0.88 0.99 0.98 0.99 0.84 0.92 0.99 0.97 0.98 0.98 1.\n",
      " 0.98 1.   0.8  0.99 0.92 0.83 0.79 0.99 1.   0.94 0.89 0.89 0.87 0.75\n",
      " 0.99 0.87 0.91 1.   0.4  0.76 0.8  0.95 0.93 0.62 0.96 0.95 0.94 0.96\n",
      " 0.96 0.81 0.94 0.92 0.91 0.66 0.82 0.93 0.74 0.92 0.91 0.98 0.81 0.99\n",
      " 0.96 0.78 0.75 0.94 0.93 0.85 0.95 0.95 0.96 0.87 1.   0.64 0.74 0.73\n",
      " 0.62 0.94 0.98 0.96 1.   0.77 0.87 0.98 0.97 1.   1.   0.9  0.97 0.99\n",
      " 0.98 0.93 0.93 0.96 0.87 0.98 0.85 0.97 0.96 0.87 0.9  0.77 0.89 0.54\n",
      " 0.92 0.9  0.74 0.81 0.99 0.97 0.76 0.82 0.87 0.68 0.99 0.97 0.97 1.\n",
      " 0.92 0.8  0.9  1.   0.99 0.76 1.   0.98 1.   0.98 1.   0.87 0.94 0.7\n",
      " 0.94 0.97 0.99 0.98 0.91 0.99 0.98 0.92 0.85 0.92 0.2  0.89 0.82 0.97\n",
      " 0.96 0.87 0.99 0.97 1.   0.86 1.   1.   0.61 0.31 0.93 0.91]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.99 1.   0.81 0.82 0.91 1.   0.75 1.   1.   0.9  1.   0.52 0.95 0.57\n",
      " 0.93 0.87 1.   0.88 1.   0.98 0.98 1.   0.77 0.94 0.83 0.99 0.96 0.65\n",
      " 0.97 0.92 1.   0.96 0.94 0.73 0.92 0.88 0.75 0.57 0.89 0.99 0.88 0.97\n",
      " 0.99 0.93 0.92 1.   0.95 0.99 1.   0.97 0.96 1.   0.8  0.98 0.99 0.95\n",
      " 0.98 0.25 0.98 0.98 0.99 0.69 0.84 0.97 0.87 0.97 0.95 0.82 0.33 1.\n",
      " 0.95 0.8  0.98 0.78 0.99 0.92 0.75 0.72 0.95 0.96 0.9  0.96 0.99 0.8\n",
      " 0.89 0.99 0.93 0.71 0.94 0.6  0.7  0.79 0.96 0.96 0.6  0.79 1.   0.94\n",
      " 1.   0.96 0.96 0.73 0.99 1.   1.   1.   0.97 0.87 0.99 0.57 0.86 0.9\n",
      " 0.81 1.   0.95 1.   1.   0.98 1.   1.   0.99 0.57 0.99 0.96 0.83 0.96\n",
      " 0.72 0.96 0.96 1.   1.   0.96 0.95 1.   0.97 0.99 0.94 0.92 0.98 0.98\n",
      " 0.61 0.95 1.   0.99 0.65 1.   0.95 0.75 1.   0.99 0.95 0.55 0.87 1.\n",
      " 0.93 0.98 0.97 0.92 0.93 1.   0.94 1.   0.97 0.78 0.59 1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.41 0.95 0.96 0.71 1.   0.96 0.86 1.   0.97 1.   0.99 0.83 0.98 0.98\n",
      " 0.75 0.73 0.93 1.   0.91 0.95 0.97 0.97 0.98 0.93 1.   0.99 0.97 0.96\n",
      " 0.99 0.97 0.69 0.97 0.98 0.94 0.5  0.89 0.99 0.78 0.87 0.91 0.96 0.96\n",
      " 0.77 0.99 0.98 0.91 0.97 0.77 0.96 1.   1.   0.96 0.99 0.98 1.   0.7\n",
      " 0.99 1.   0.98 0.93 0.88 0.99 1.   0.66 0.46 0.87 0.94 0.83 0.88 1.\n",
      " 0.85 0.98 0.84 0.9  0.96 0.81 0.92 0.78 0.96 0.97 1.   0.99 0.98 0.96\n",
      " 1.   0.75 0.97 0.94 0.97 0.63 0.9  0.88 0.92 0.99 0.89 0.64 0.96 1.\n",
      " 1.   0.98 0.94 1.   1.   0.99 1.   0.98 0.8  1.   0.92 0.79 0.8  0.77\n",
      " 0.9  0.96 1.   0.84 0.9  0.97 0.86 1.   0.99 0.81 0.98 0.98 0.99 0.97\n",
      " 0.94 0.99 0.59 0.85 0.97 0.97 0.96 0.99 0.86 0.84 0.84 0.99 0.94 0.9\n",
      " 0.92 0.81 0.59 0.98 0.82 1.   0.35 0.9  0.86 0.93 0.98 0.96 0.53 0.91\n",
      " 0.79 0.85 0.97 0.97 0.95 0.94 1.   0.97 0.97 0.82 0.83 0.93]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   0.99 0.52 0.68 0.84 0.86 0.9  0.92 0.89 0.96 0.88 0.96 0.99 0.97\n",
      " 1.   1.   0.93 0.99 0.99 0.99 1.   0.85 0.99 0.95 0.57 0.88 0.95 0.71\n",
      " 0.97 0.98 0.85 0.86 0.91 0.81 1.   1.   0.98 0.93 0.92 0.98 0.94 0.97\n",
      " 0.99 0.95 0.76 0.97 0.88 1.   0.99 1.   0.59 0.93 0.97 0.96 0.96 0.88\n",
      " 0.97 0.83 0.68 0.97 0.95 1.   0.77 1.   0.94 0.99 0.98 1.   0.89 0.98\n",
      " 0.89 0.78 0.97 0.96 0.98 0.87 0.99 0.86 1.   0.98 0.83 1.   0.92 0.84\n",
      " 0.91 0.87 0.93 0.99 0.98 0.94 0.96 0.98 0.99 0.73 0.98 0.94 0.99 0.97\n",
      " 0.99 0.94 0.6  1.   0.99 0.94 0.88 0.89 0.93 0.96 0.59 1.   0.72 0.92\n",
      " 0.76 0.99 0.95 0.99 0.99 0.99 0.98 0.8  0.98 0.94 1.   0.76 0.94 0.99\n",
      " 1.   0.91 0.94 0.91 0.95 0.74 0.75 0.91 0.95 0.83 1.   0.92 0.95 0.96\n",
      " 0.99 0.89 0.99 0.88 1.   0.98 0.93 1.   1.   0.93 0.78 0.68 0.93 0.98\n",
      " 1.   0.78 1.   0.99 0.98 0.9  0.98 0.92 0.75 0.97 0.92]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.92 0.98 0.77 0.95 0.98 0.86 0.87 0.93 0.83 0.78 0.89 0.99 0.9  1.\n",
      " 0.99 0.88 0.65 0.94 0.98 0.92 0.76 1.   1.   0.79 0.86 0.89 0.99 0.87\n",
      " 0.49 0.96 0.92 0.83 0.86 0.71 1.   1.   0.73 1.   0.8  0.95 0.77 0.93\n",
      " 0.92 0.96 0.82 0.98 0.94 0.81 0.87 0.99 0.61 0.97 0.94 0.99 0.98 0.77\n",
      " 1.   0.99 0.73 0.97 0.93 0.76 0.97 0.9  0.9  1.   0.88 0.97 0.76 0.95\n",
      " 0.86 0.99 0.92 0.77 0.88 0.92 1.   0.98 0.99 0.55 1.   0.84 0.97 0.64\n",
      " 0.98 0.85 0.9  0.92 0.87 0.91 0.93 1.   1.   0.97 0.92 0.91 0.96 1.\n",
      " 0.98 0.91 0.99 0.96 0.92 0.91 0.98 0.87 0.95 0.88 0.67 0.88 0.98 0.89\n",
      " 0.98 1.   0.79 0.91 0.38 0.94 0.94 0.81 1.   0.99 0.88 0.98 0.93 0.69\n",
      " 0.94 0.96 0.96 0.85 0.63 0.93 0.98 0.83 0.96 1.   0.9  0.7  0.95 0.89\n",
      " 0.96 0.87 0.99 0.98 0.74 0.93 0.5  0.98 0.77 0.96 0.97 0.98 0.94 0.99\n",
      " 0.97 0.99 0.85 0.85 1.   0.51 0.34 0.95 0.84 0.89 0.82]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6  65   5 752]\n",
      "공통 \n",
      "AUC : 0.657646008149292\n",
      "AUPRC : 0.9469804894154108\n",
      "Optimized precision : 0.006096100693626181\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9154589371980676\n",
      "Precision(pb) : 0.9323027731308152\n",
      "Recall(pb) : 0.8302845755994895\n",
      "F1 score(pb) : 0.8783412780952489\n",
      "MCC : 0.19051030658901236\n",
      "G-mean : 0.2897393165673664\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  6  65]\n",
      " [  5 752]]\n",
      "Accuracy : 0.9154589371980676\n",
      "Precision : 0.9204406364749081\n",
      "Recall : 0.9933949801849405\n",
      "F1 score : 0.9555273189326556\n",
      "MCC : 0.20256792093008907\n",
      "G-mean : 0.2897393165673664\n",
      "TEST: [  1   5  11  12  34  36  39  46  49  52  56  68  71  93 102 104 112 119\n",
      " 122 125 126 140 145 152 157 162 164 166 167 183 190 194 198 205 209 210\n",
      " 211 214 223 228 230 233 234 237 249 257 258 264 272 277 280 281 291 298\n",
      " 303 323 326 328 331 332 338 340 342 354 366 368 373 374 375 380 381 383\n",
      " 389 399 401 403 409 412 413 421 422 423 431 432 434 443 444 447 456 460\n",
      " 461 464 465 468 471 474 476 481 491 493 496 498 506 507 516 518 520 523\n",
      " 526 528 532 537 538 547 551 553 556 572 575 581 586 589 604 606 611 613\n",
      " 624 629 632 645 651 652 653 658 664 665 666 673 694 700 701 702 706 707\n",
      " 708 709 718 723 735 744 745 754 757 766 767 769 774 780 783 789 799 800\n",
      " 806 816 822 825]\n",
      "[1.   0.98 0.67 0.9  0.97 0.85 0.64 0.91 0.92 1.   0.97 0.82 0.91 0.9\n",
      " 0.95 1.   0.84 0.84 0.99 0.97 1.   0.74 0.96 0.99 0.99 0.99 0.97 1.\n",
      " 0.96 0.99 0.77 0.99 0.95 0.89 0.82 0.99 1.   0.93 0.95 0.86 0.93 0.78\n",
      " 1.   0.94 0.95 1.   0.46 0.75 0.83 0.94 0.97 0.58 0.96 0.88 0.95 0.96\n",
      " 0.97 0.87 0.92 0.93 0.93 0.71 0.89 0.9  0.74 0.96 0.84 0.97 0.82 0.98\n",
      " 0.95 0.75 0.87 0.94 0.94 0.87 0.96 0.84 0.94 0.79 1.   0.57 0.77 0.79\n",
      " 0.61 0.91 0.99 0.95 0.99 0.8  0.84 0.96 0.99 0.99 1.   0.88 0.93 1.\n",
      " 0.98 0.98 0.91 0.91 0.93 0.97 0.84 0.95 0.9  0.91 0.85 0.82 0.86 0.49\n",
      " 0.94 0.95 0.78 0.79 0.96 0.93 0.74 0.84 0.89 0.69 1.   0.98 0.95 0.99\n",
      " 0.92 0.83 0.9  1.   1.   0.7  1.   0.97 1.   0.95 1.   0.83 0.95 0.69\n",
      " 0.93 0.96 1.   0.99 0.88 1.   1.   0.85 0.91 0.95 0.24 0.93 0.84 0.99\n",
      " 0.95 0.88 0.99 0.98 1.   0.86 0.99 1.   0.56 0.26 0.93 0.91]\n",
      "TEST: [  0   3  10  13  16  20  22  23  24  25  26  32  43  59  65  67  72  73\n",
      "  83  87  89  94  96  97 100 117 120 121 132 134 146 150 158 175 180 189\n",
      " 192 195 196 212 215 226 240 241 243 244 246 248 251 255 261 262 263 271\n",
      " 279 294 295 301 307 309 310 314 316 325 327 335 349 355 360 361 367 371\n",
      " 385 386 395 405 406 410 411 415 419 433 436 440 446 452 458 462 463 469\n",
      " 473 492 499 502 509 510 512 514 530 535 536 540 549 550 561 567 577 582\n",
      " 585 587 597 599 600 601 602 612 618 623 625 626 627 630 634 639 644 648\n",
      " 649 656 660 663 678 679 684 695 697 715 717 719 721 722 725 729 749 752\n",
      " 753 758 761 765 768 785 786 787 790 792 793 795 796 797 801 802 804 809\n",
      " 811 814 817 820]\n",
      "[0.98 1.   0.85 0.82 0.94 1.   0.83 1.   0.96 0.88 0.99 0.49 0.93 0.62\n",
      " 0.96 0.88 1.   0.9  1.   0.99 0.94 1.   0.62 0.98 0.87 0.98 0.98 0.62\n",
      " 0.89 0.91 0.96 0.96 0.96 0.77 0.88 0.91 0.65 0.56 0.93 1.   0.89 0.96\n",
      " 0.99 0.85 0.92 1.   0.89 0.99 0.98 0.99 0.99 1.   0.79 0.99 1.   0.99\n",
      " 0.98 0.24 0.96 0.97 0.99 0.57 0.84 0.93 0.89 0.99 0.88 0.66 0.38 1.\n",
      " 0.92 0.83 0.96 0.81 0.99 0.87 0.71 0.85 0.96 0.96 0.9  0.98 1.   0.92\n",
      " 0.92 0.95 1.   0.76 0.94 0.54 0.68 0.82 0.94 0.97 0.71 0.82 0.97 0.97\n",
      " 1.   0.87 0.94 0.76 0.98 0.95 0.99 1.   0.97 0.91 1.   0.67 0.85 0.97\n",
      " 0.91 0.99 0.93 1.   1.   0.99 0.99 0.97 0.97 0.57 0.98 0.97 0.83 0.94\n",
      " 0.79 0.97 0.99 1.   1.   0.94 0.96 0.98 0.99 0.99 0.95 0.96 0.99 0.98\n",
      " 0.61 0.97 1.   1.   0.69 0.99 0.98 0.83 1.   0.96 0.96 0.72 0.91 1.\n",
      " 0.91 0.98 1.   0.98 0.94 1.   0.95 1.   0.95 0.88 0.6  1.  ]\n",
      "TEST: [  2   6   9  15  18  19  27  30  53  57  58  63  76  77  85  99 101 106\n",
      " 107 111 115 130 131 133 143 147 149 153 154 165 169 172 176 182 184 185\n",
      " 186 188 193 199 201 202 204 208 218 221 224 225 227 238 242 250 252 256\n",
      " 269 278 282 285 300 306 308 311 312 318 319 320 321 329 330 336 337 339\n",
      " 341 346 347 351 353 357 358 359 370 384 388 397 402 407 408 414 416 420\n",
      " 425 438 439 441 442 450 451 459 467 472 484 487 489 495 497 503 517 524\n",
      " 527 534 552 554 557 558 559 564 565 573 576 579 583 584 593 594 596 605\n",
      " 614 621 633 637 642 654 655 659 661 668 671 672 674 676 682 687 688 691\n",
      " 696 711 720 730 734 736 741 746 750 751 755 756 760 763 777 784 798 807\n",
      " 812 813 823 826]\n",
      "[0.51 0.92 0.94 0.69 0.98 0.95 0.78 0.99 0.99 0.95 0.97 0.76 0.99 0.95\n",
      " 0.73 0.66 0.89 1.   0.91 0.97 0.98 0.97 1.   0.83 0.99 0.99 0.97 0.98\n",
      " 0.98 0.95 0.62 0.97 0.97 0.97 0.55 0.9  0.99 0.82 0.89 0.95 0.97 0.88\n",
      " 0.84 0.98 0.97 0.91 0.97 0.71 0.94 1.   1.   0.89 0.98 1.   1.   0.82\n",
      " 1.   1.   0.95 0.93 0.8  0.98 1.   0.7  0.64 0.83 0.92 0.84 0.91 0.99\n",
      " 0.87 0.95 0.77 0.9  0.92 0.79 0.95 0.8  0.95 0.94 1.   0.99 1.   0.97\n",
      " 0.98 0.77 1.   0.98 0.95 0.63 0.86 0.78 0.89 1.   0.86 0.62 0.9  1.\n",
      " 1.   0.95 0.91 0.99 0.98 1.   1.   0.97 0.76 0.98 0.93 0.84 0.83 0.79\n",
      " 0.92 0.93 0.99 0.88 0.86 0.97 0.81 1.   0.98 0.89 0.99 0.95 0.95 0.98\n",
      " 0.87 0.99 0.38 0.8  0.96 0.94 0.93 0.99 0.77 0.82 0.85 0.98 0.89 0.87\n",
      " 0.9  0.82 0.45 0.94 0.91 1.   0.3  0.91 0.85 0.85 0.99 0.99 0.6  0.95\n",
      " 0.91 0.86 0.97 0.95 0.93 0.91 1.   0.96 0.97 0.82 0.82 0.92]\n",
      "TEST: [  4  14  17  28  29  35  38  41  44  45  47  48  51  55  61  78  81  88\n",
      "  90  92  95 108 109 114 116 124 127 128 142 148 151 155 159 161 163 168\n",
      " 171 174 179 181 191 203 206 207 213 216 217 220 229 235 245 247 253 254\n",
      " 259 265 267 274 276 286 287 288 292 297 299 305 313 315 333 343 348 352\n",
      " 362 365 369 372 376 382 387 390 392 393 394 398 400 404 417 424 427 430\n",
      " 435 445 448 453 454 457 478 479 480 482 494 500 501 504 515 521 522 529\n",
      " 533 541 542 544 545 555 568 569 574 578 588 592 595 603 607 608 610 616\n",
      " 619 620 622 628 635 636 638 640 647 650 657 667 670 675 677 686 690 692\n",
      " 693 703 705 710 716 726 737 739 748 759 762 772 776 779 781 788 791 803\n",
      " 815 821 827]\n",
      "[1.   0.98 0.43 0.56 0.88 0.81 0.89 0.8  0.91 0.95 0.88 0.98 1.   0.97\n",
      " 0.99 1.   0.94 0.96 1.   0.98 1.   0.91 1.   0.88 0.44 0.89 0.94 0.78\n",
      " 0.98 0.97 0.82 0.85 0.89 0.83 1.   0.99 0.97 0.9  0.95 0.97 0.95 0.98\n",
      " 1.   0.93 0.75 0.94 0.86 1.   0.95 1.   0.66 0.98 0.89 0.94 0.99 0.93\n",
      " 0.95 0.8  0.68 0.97 0.91 1.   0.8  1.   0.98 0.97 0.95 1.   0.85 0.95\n",
      " 0.93 0.86 0.9  0.95 0.99 0.89 0.99 0.89 0.99 0.99 0.84 1.   0.94 0.84\n",
      " 0.87 0.87 0.97 0.99 0.97 0.91 0.94 0.97 1.   0.71 0.99 0.95 0.97 0.99\n",
      " 0.96 0.97 0.75 1.   0.99 0.86 0.89 0.91 0.94 1.   0.71 0.99 0.74 0.9\n",
      " 0.78 1.   0.93 1.   1.   0.99 0.95 0.88 0.95 0.97 1.   0.67 0.94 1.\n",
      " 1.   0.95 0.98 0.94 0.92 0.77 0.75 0.94 0.96 0.9  1.   0.92 0.96 0.91\n",
      " 1.   0.88 0.97 0.91 1.   0.99 0.91 1.   0.99 0.91 0.79 0.69 0.96 0.98\n",
      " 1.   0.79 1.   0.97 0.95 0.84 0.97 0.97 0.76 0.96 0.87]\n",
      "TEST: [  7   8  21  31  33  37  40  42  50  54  60  62  64  66  69  70  74  75\n",
      "  79  80  82  84  86  91  98 103 105 110 113 118 123 129 135 136 137 138\n",
      " 139 141 144 156 160 170 173 177 178 187 197 200 219 222 231 232 236 239\n",
      " 260 266 268 270 273 275 283 284 289 290 293 296 302 304 317 322 324 334\n",
      " 344 345 350 356 363 364 377 378 379 391 396 418 426 428 429 437 449 455\n",
      " 466 470 475 477 483 485 486 488 490 505 508 511 513 519 525 531 539 543\n",
      " 546 548 560 562 563 566 570 571 580 590 591 598 609 615 617 631 641 643\n",
      " 646 662 669 680 681 683 685 689 698 699 704 712 713 714 724 727 728 731\n",
      " 732 733 738 740 742 743 747 764 770 771 773 775 778 782 794 805 808 810\n",
      " 818 819 824]\n",
      "[0.9  0.93 0.77 0.93 0.98 0.89 0.87 0.96 0.8  0.72 0.87 0.99 0.95 1.\n",
      " 0.99 0.91 0.66 0.97 0.98 0.96 0.86 1.   1.   0.82 0.8  0.87 1.   0.74\n",
      " 0.54 0.95 0.91 0.85 0.86 0.7  0.99 0.99 0.74 1.   0.74 0.95 0.81 0.92\n",
      " 0.88 0.93 0.8  0.98 0.96 0.79 0.84 1.   0.57 0.82 0.87 1.   0.95 0.76\n",
      " 0.99 0.98 0.74 0.96 0.88 0.86 0.96 0.89 0.9  1.   0.86 0.96 0.81 0.94\n",
      " 0.91 0.97 0.89 0.89 0.89 0.92 0.99 0.99 0.95 0.48 1.   0.89 0.98 0.66\n",
      " 0.99 0.83 0.9  0.92 0.88 0.88 0.92 1.   1.   0.93 0.89 0.93 0.94 0.99\n",
      " 0.96 0.92 0.99 0.97 0.87 0.93 0.98 0.85 0.92 0.88 0.62 0.85 0.97 0.91\n",
      " 0.95 0.99 0.88 0.85 0.39 0.95 0.92 0.85 1.   1.   0.88 0.94 0.95 0.63\n",
      " 0.95 0.96 0.97 0.92 0.67 0.84 0.98 0.78 0.95 1.   0.93 0.64 0.91 0.9\n",
      " 0.97 0.89 1.   0.99 0.74 0.88 0.56 0.97 0.69 1.   0.98 0.98 0.95 1.\n",
      " 0.96 0.95 0.86 0.9  0.99 0.64 0.34 0.97 0.77 0.9  0.88]\n",
      "[  7  64   8 749]\n",
      "공통 \n",
      "AUC : 0.6585390812510465\n",
      "AUPRC : 0.9473747830005773\n",
      "Optimized precision : 0.020406782483787998\n",
      "\n",
      "function 사용\n",
      "Accuracy : 0.9130434782608695\n",
      "Precision(pb) : 0.9325390066469341\n",
      "Recall(pb) : 0.8275977878781093\n",
      "F1 score(pb) : 0.8769400439916448\n",
      "MCC : 0.18479275133283987\n",
      "G-mean : 0.3123293624958452\n",
      "\n",
      "Confusion_matrix 사용 \n",
      " [[  7  64]\n",
      " [  8 749]]\n",
      "Accuracy : 0.9130434782608695\n",
      "Precision : 0.9212792127921279\n",
      "Recall : 0.9894319682959049\n",
      "F1 score : 0.954140127388535\n",
      "MCC : 0.2026041008588967\n",
      "G-mean : 0.31232936249584514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20, 15)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_y_set = []\n",
    "\n",
    "for i in range(20):\n",
    "    rf_y, f_sm_y, rf_cw_y, rf_smcw_y = [], [], [], []\n",
    "    output_rf = train_model(X, y, dict_x, dict_y)\n",
    "\n",
    "    # sum model training\n",
    "    rf_y = output_rf.train_based()\n",
    "    #rf_cw_y = output_rf.train_weight()\n",
    "    #rf_smcw_y = output_rf.train_sm_weight()\n",
    "    \n",
    "    final_result = Evaluation(predict_change(rf_y), y)\n",
    "\n",
    "    rf_y_set = np.append(rf_y_set, final_result.matrix())\n",
    "    #print(len(rf_y_set))\n",
    "    \n",
    "rf_y_np = np.array(rf_y_set)\n",
    "\n",
    "rf_y_np = rf_y_np.reshape(-1,15)\n",
    "rf_y_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_result(data):\n",
    "    auc = data[:,0]\n",
    "    auprc = data[:,1]\n",
    "    op = data[:,2]\n",
    "    acc = data[:,3]\n",
    "    precision = data[:,4]\n",
    "    recall = data[:,5]\n",
    "    f1 = data[:,6]\n",
    "    mcc = data[:,7]\n",
    "    g_mean = data[:,8]\n",
    "    confu_acc = data[:,9]\n",
    "    confu_precision = data[:,10]\n",
    "    confu_recall = data[:,11]\n",
    "    confu_f1 = data[:,12]\n",
    "    confu_mcc = data[:,13]\n",
    "    confu_g_mean = data[:,14]\n",
    "\n",
    "    print('공통 \\nAUC :\\n',\"[평균 :\",format(auc.mean(), \".5f\"), \"] [표준편차 :\",format(auc.std(), \".5f\"),\"]\") # pb\n",
    "    print(\"AUPRC :\\n\", \"[평균 :\",format(auprc.mean(), \".5f\"), \"] [표준편차 :\",format(auprc.std(), \".5f\"),\"]\")\n",
    "    print(\"Optimized precision :\\n\",\"[평균 :\",format(op.mean(), \".5f\"), \"] [표준편차 :\",format(op.std(), \".5f\"),\"]\")\n",
    "\n",
    "    print(\"\\nfunction 사용\\nAccuracy :\\n\",\"[평균 :\",format(acc.mean(), \".5f\"), \"] [표준편차 :\",format(acc.std(), \".5f\"),\"]\")\n",
    "    print(\"Precision(pb) :\\n\", \"[평균 :\",format(precision.mean(), \".5f\"), \"] [표준편차 :\",format(precision.std(), \".5f\"),\"]\")\n",
    "    print(\"Recall(pb) :\\n\", \"[평균 :\",format(recall.mean(), \".5f\"), \"] [표준편차 :\",format(recall.std(), \".5f\"),\"]\")\n",
    "    print(\"F1 score(pb) :\\n\",\"[평균 :\",format(f1.mean(), \".5f\"), \"] [표준편차 :\",format(f1.std(), \".5f\"),\"]\")\n",
    "    print(\"MCC :\\n\", \"[평균 :\",format(mcc.mean(), \".5f\"), \"] [표준편차 :\",format(mcc.std(), \".5f\"),\"]\")\n",
    "    print(\"G-mean :\\n\", \"[평균 :\",format(g_mean.mean(), \".5f\"), \"] [표준편차 :\",format(g_mean.std(), \".5f\"),\"]\")\n",
    "    \n",
    "    print(\"\\nConfusion matrix 사용 \\nAccuracy :\\n\", \"[평균 :\",format(confu_acc.mean(), \".5f\"), \"] [표준편차 :\",format(confu_acc.std(), \".5f\"),\"]\")\n",
    "    print(\"Precision :\\n\", \"[평균 :\",format(confu_precision.mean(), \".5f\"), \"] [표준편차 :\",format(confu_precision.std(), \".5f\"),\"]\")\n",
    "    print(\"Recall :\\n\", \"[평균 :\",format(confu_recall.mean(), \".5f\"), \"] [표준편차 :\",format(confu_recall.std(), \".5f\"),\"]\")\n",
    "    print(\"F1 score :\\n\", \"[평균 :\",format(confu_f1.mean(), \".5f\"), \"] [표준편차 :\",format(confu_f1.std(), \".5f\"),\"]\")\n",
    "    print(\"MCC :\\n\", \"[평균 :\",format(confu_mcc.mean(), \".5f\"), \"] [표준편차 :\",format(confu_mcc.std(), \".5f\"),\"]\")\n",
    "    print(\"G-mean :\\n\", \"[평균 :\",format(confu_g_mean.mean(), \".5f\"), \"] [표준편차 :\",format(confu_g_mean.std(), \".5f\"),\"]\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "공통 \n",
      "AUC :\n",
      " [평균 : 0.66133 ] [표준편차 : 0.00712 ]\n",
      "AUPRC :\n",
      " [평균 : 0.94823 ] [표준편차 : 0.00175 ]\n",
      "Optimized precision :\n",
      " [평균 : 0.01663 ] [표준편차 : 0.01230 ]\n",
      "\n",
      "function 사용\n",
      "Accuracy :\n",
      " [평균 : 0.91449 ] [표준편차 : 0.00182 ]\n",
      "Precision(pb) :\n",
      " [평균 : 0.93263 ] [표준편차 : 0.00081 ]\n",
      "Recall(pb) :\n",
      " [평균 : 0.82600 ] [표준편차 : 0.00638 ]\n",
      "F1 score(pb) :\n",
      " [평균 : 0.87607 ] [표준편차 : 0.00336 ]\n",
      "MCC :\n",
      " [평균 : 0.19265 ] [표준편차 : 0.02448 ]\n",
      "G-mean :\n",
      " [평균 : 0.30633 ] [표준편차 : 0.02022 ]\n",
      "\n",
      "Confusion matrix 사용 \n",
      "Accuracy :\n",
      " [평균 : 0.91449 ] [표준편차 : 0.00182 ]\n",
      "Precision :\n",
      " [평균 : 0.92114 ] [표준편차 : 0.00101 ]\n",
      "Recall :\n",
      " [평균 : 0.99135 ] [표준편차 : 0.00164 ]\n",
      "F1 score :\n",
      " [평균 : 0.95495 ] [표준편차 : 0.00097 ]\n",
      "MCC :\n",
      " [평균 : 0.20762 ] [표준편차 : 0.02269 ]\n",
      "G-mean :\n",
      " [평균 : 0.30633 ] [표준편차 : 0.02022 ]\n"
     ]
    }
   ],
   "source": [
    "new_result(rf_y_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
